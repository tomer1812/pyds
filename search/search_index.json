{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Discrete Time Survival Analysis \u00a4 A Python package for discrete-time survival data analysis with competing risks. Tomer Meir , Rom Gutman , Malka Gorfine 2022 Installation \u00a4 pip install pydts Quick Start \u00a4 from pydts.fitters import TwoStagesFitter from pydts.examples_utils.generate_simulations_data import generate_quick_start_df patients_df = generate_quick_start_df ( n_patients = 10000 , n_cov = 5 , d_times = 14 , j_events = 2 , pid_col = 'pid' , seed = 0 ) fitter = TwoStagesFitter () fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Examples \u00a4 Usage Example Hospital Length of Stay Simulation Example Citation \u00a4 If you found PyDTS useful, please cite: @article { Meir_PyDTS_2022 , author = {Meir, Tomer and Gutman, Rom, and Gorfine, Malka} , doi = {10.48550/arXiv.2204.05731} , title = {{PyDTS: A Python Package for Discrete Time Survival Analysis with Competing Risks}} , url = {https://arxiv.org/abs/2204.05731} , year = {2022} } @article { Meir_Gorfine_DTSP_2023 , author = {Meir, Tomer and Gorfine, Malka} , doi = {10.48550/arXiv.2303.01186} , title = {{Discrete-time Competing-Risks Regression with or without Penalization}} , url = {https://arxiv.org/abs/2303.01186} , year = {2023} } and please consider starring the project on GitHub How to Contribute \u00a4 Open Github issues to suggest new features or to report bugs\\errors Contact Tomer or Rom if you want to add a usage example to the documentation If you want to become a developer (thank you, we appreciate it!) - please contact Tomer or Rom for developers' on-boarding Tomer Meir: tomer1812@gmail.com, Rom Gutman: rom.gutman1@gmail.com","title":"Home"},{"location":"#discrete-time-survival-analysis","text":"A Python package for discrete-time survival data analysis with competing risks. Tomer Meir , Rom Gutman , Malka Gorfine 2022","title":"Discrete Time Survival Analysis"},{"location":"#installation","text":"pip install pydts","title":"Installation"},{"location":"#quick-start","text":"from pydts.fitters import TwoStagesFitter from pydts.examples_utils.generate_simulations_data import generate_quick_start_df patients_df = generate_quick_start_df ( n_patients = 10000 , n_cov = 5 , d_times = 14 , j_events = 2 , pid_col = 'pid' , seed = 0 ) fitter = TwoStagesFitter () fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary ()","title":"Quick Start"},{"location":"#examples","text":"Usage Example Hospital Length of Stay Simulation Example","title":"Examples"},{"location":"#citation","text":"If you found PyDTS useful, please cite: @article { Meir_PyDTS_2022 , author = {Meir, Tomer and Gutman, Rom, and Gorfine, Malka} , doi = {10.48550/arXiv.2204.05731} , title = {{PyDTS: A Python Package for Discrete Time Survival Analysis with Competing Risks}} , url = {https://arxiv.org/abs/2204.05731} , year = {2022} } @article { Meir_Gorfine_DTSP_2023 , author = {Meir, Tomer and Gorfine, Malka} , doi = {10.48550/arXiv.2303.01186} , title = {{Discrete-time Competing-Risks Regression with or without Penalization}} , url = {https://arxiv.org/abs/2303.01186} , year = {2023} } and please consider starring the project on GitHub","title":"Citation"},{"location":"#how-to-contribute","text":"Open Github issues to suggest new features or to report bugs\\errors Contact Tomer or Rom if you want to add a usage example to the documentation If you want to become a developer (thank you, we appreciate it!) - please contact Tomer or Rom for developers' on-boarding Tomer Meir: tomer1812@gmail.com, Rom Gutman: rom.gutman1@gmail.com","title":"How to Contribute"},{"location":"EventTimesSampler/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Event Times Sampler \u00a4 PyDTS provides EventTimesSampler (ETS) class for sampling discrete-time survival data with competing risks and right censoring under the log-link model. In the following, we present an example of how to use the ETS to sample discrete-time with competing events and right censoring data. Covariates \u00a4 A user-supplied covariates should be passed to ETS. For example, consider a setting with \\(n=10,000\\) independent observations and the following covariates \\[ Z_1 \\sim \\mbox{Bernoulli(0.5)} \\] \\[ Z_2 | Z_1 \\sim \\mbox{Normal}(72 + 10 Z_1, 12) \\, , \\] and \\[ Z_3 \\sim 1+\\mbox{Poisson}(4) \\, . \\] Any sampling framework can be used for creating the covariates' dataframe. For example: import numpy as np import pandas as pd n_observations = 10000 observations_df = pd . DataFrame ( columns = [ 'Z1' , 'Z2' , 'Z3' ]) observations_df [ 'Z1' ] = np . random . binomial ( n = 1 , p = 0.5 , size = n_observations ) Z1_zero_index = observations_df . loc [ observations_df [ 'Z1' ] == 0 ] . index observations_df . loc [ Z1_zero_index , 'Z2' ] = np . random . normal ( loc = 72 , scale = 12 , size = n_observations - observations_df [ 'Z1' ] . sum ()) Z1_one_index = observations_df . loc [ observations_df [ 'Z1' ] == 1 ] . index observations_df . loc [ Z1_one_index , 'Z2' ] = np . random . normal ( loc = 82 , scale = 12 , size = observations_df [ 'Z1' ] . sum ()) observations_df [ 'Z3' ] = 1 + np . random . poisson ( lam = 4 , size = n_observations ) from pydts.examples_utils.plots import plot_sampled_covariates_figure plot_sampled_covariates_figure ( observations_df , fname = 'tmp.png' ) Event Times \u00a4 The ETS function assumes that the possible failure times are \\(1, \\ldots, d\\) , and the user should supply the value of \\(d\\) . Clearly, the time intervals can be irregularly spaced and variable in size. For instance, discrete-time categories 1, 2, and 3 could correspond to specific days like Tuesday, Thursday, and Friday-Sunday, respectively. In the current example, we chose \\(d=7\\) . For the competing-events setting the user should decide on the number of competing events, and the values of model parameters, \\(\\alpha_{jt}\\) , \\(\\beta_j\\) , \\(t=1,\\ldots,d\\) , \\(j=1,\\ldots,M\\) . For example, consider \\(M=2\\) competing events and \\[ \\alpha_{1t} = -1 - 0.3 \\log t \\, , \\, t =1, \\ldots, 7 \\] \\[ \\alpha_{2t} = -1.75 - 0.15 \\log t \\, , \\, t=1, \\ldots, 7 \\] \\[ \\beta^\\top_{1} = (-\\log 0.8, -\\log 1.4, -\\log 3) \\] \\[ \\beta^\\top_{2} = (-\\log 1, -\\log 0.95, -\\log 2) \\, . \\] All together, the ETS function is defined for sampling event-type and event-time, and adding it to the data frame, as follows: from pydts.data_generation import EventTimesSampler ets = EventTimesSampler ( d_times = 7 , j_event_types = 2 ) coefficients_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ), }, \"beta\" : { 1 : - 1 * np . log ([ 0.8 , 1.4 , 3 ]), 2 : - 1 * np . log ([ 1 , 0.95 , 2 ]), }} observations_df = ets . sample_event_times ( observations_df , coefficients_dict ) If the sampled covariates and parameters' values lead to impossible survival probabilities (i.e., negative or greater than one), the sampling process will be terminated with an error message. In such scenarios, it may be useful to adjust the coefficients or constrain extreme values of the covariates to ensure that the probabilities are appropriate and the sampling process is executed successfully. Censoring Time \u00a4 Two types of right censoring are implemented in PyDTS, administrative and random right censoring. For administrative censoring, \\(J_i=0\\) and \\(T_i = d + 1\\) . These are the default values of observations for which the sampled event type was observed to be greater than \\(d\\) . Random right censoring is optional and could be either dependent or independent of the covariates. For example, assume \\[ \\Pr (C_i = t) = 0.05 \\quad t=1, \\ldots, 7 \\, . \\] The censoring times can be sampled by prob_lof_at_t = [ 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 ] observations_df = ets . sample_independent_lof_censoring ( observations_df , prob_lof_at_t ) To generate right-censoring times that depend on the covariates, the user should supply to censoring hazard function, \\(\\lambda_c(t|Z)\\) in the form of the logit-link model. For example, censoring_coef_dict = { \"alpha\" : { 0 : lambda t : - 0.3 - 0.3 * np . log ( t ), }, \"beta\" : { 0 : - 1 * np . log ([ 8 , 0.95 , 6 ]), }} observations_df = ets . sample_hazard_lof_censoring ( observations_df , censoring_coef_dict ) Updating the Observations \u00a4 Finally, the observed data should be updated by \\(X_i = min(T_i, C_i)\\) and \\(J_i\\) as follows observations_df = ets . update_event_or_lof ( observations_df ) The first observations of the sampled data are observations_df . head ( 30 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 J T C X 0 1 77.707214 4 2 3 8 3 1 0 79.17863 6 2 5 8 5 2 0 66.023722 5 0 8 8 8 3 1 89.672794 5 2 2 8 2 4 0 76.386179 5 0 8 8 8 5 1 79.180418 3 2 4 8 4 6 1 79.500548 5 2 5 8 5 7 1 89.930194 3 2 1 8 1 8 0 68.328199 4 2 2 8 2 9 1 92.693606 8 2 4 8 4 10 1 93.037814 6 2 1 8 1 11 1 90.09132 3 2 1 8 1 12 0 93.967038 3 2 1 2 1 13 0 54.984284 1 2 1 1 1 14 1 92.6541 8 0 8 8 8 15 0 78.404324 4 2 1 8 1 16 1 64.595813 2 2 2 8 2 17 0 66.326022 7 0 8 8 8 18 1 102.786353 6 2 2 8 2 19 1 76.981113 2 2 1 8 1 20 0 65.093074 7 0 8 8 8 21 0 70.287022 2 2 1 1 1 22 1 87.354831 5 2 1 8 1 23 1 81.826717 4 2 2 8 2 24 1 72.835886 3 2 1 8 1 25 1 77.435862 10 0 8 8 8 26 0 84.643334 4 2 2 6 2 27 0 48.055462 9 0 8 8 8 28 1 76.259105 4 2 1 8 1 29 0 68.426955 3 0 3 1 1 References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022)","title":"Event Times Sampler"},{"location":"EventTimesSampler/#event-times-sampler","text":"PyDTS provides EventTimesSampler (ETS) class for sampling discrete-time survival data with competing risks and right censoring under the log-link model. In the following, we present an example of how to use the ETS to sample discrete-time with competing events and right censoring data.","title":"Event Times Sampler"},{"location":"EventTimesSampler/#covariates","text":"A user-supplied covariates should be passed to ETS. For example, consider a setting with \\(n=10,000\\) independent observations and the following covariates \\[ Z_1 \\sim \\mbox{Bernoulli(0.5)} \\] \\[ Z_2 | Z_1 \\sim \\mbox{Normal}(72 + 10 Z_1, 12) \\, , \\] and \\[ Z_3 \\sim 1+\\mbox{Poisson}(4) \\, . \\] Any sampling framework can be used for creating the covariates' dataframe. For example: import numpy as np import pandas as pd n_observations = 10000 observations_df = pd . DataFrame ( columns = [ 'Z1' , 'Z2' , 'Z3' ]) observations_df [ 'Z1' ] = np . random . binomial ( n = 1 , p = 0.5 , size = n_observations ) Z1_zero_index = observations_df . loc [ observations_df [ 'Z1' ] == 0 ] . index observations_df . loc [ Z1_zero_index , 'Z2' ] = np . random . normal ( loc = 72 , scale = 12 , size = n_observations - observations_df [ 'Z1' ] . sum ()) Z1_one_index = observations_df . loc [ observations_df [ 'Z1' ] == 1 ] . index observations_df . loc [ Z1_one_index , 'Z2' ] = np . random . normal ( loc = 82 , scale = 12 , size = observations_df [ 'Z1' ] . sum ()) observations_df [ 'Z3' ] = 1 + np . random . poisson ( lam = 4 , size = n_observations ) from pydts.examples_utils.plots import plot_sampled_covariates_figure plot_sampled_covariates_figure ( observations_df , fname = 'tmp.png' )","title":"Covariates"},{"location":"EventTimesSampler/#event-times","text":"The ETS function assumes that the possible failure times are \\(1, \\ldots, d\\) , and the user should supply the value of \\(d\\) . Clearly, the time intervals can be irregularly spaced and variable in size. For instance, discrete-time categories 1, 2, and 3 could correspond to specific days like Tuesday, Thursday, and Friday-Sunday, respectively. In the current example, we chose \\(d=7\\) . For the competing-events setting the user should decide on the number of competing events, and the values of model parameters, \\(\\alpha_{jt}\\) , \\(\\beta_j\\) , \\(t=1,\\ldots,d\\) , \\(j=1,\\ldots,M\\) . For example, consider \\(M=2\\) competing events and \\[ \\alpha_{1t} = -1 - 0.3 \\log t \\, , \\, t =1, \\ldots, 7 \\] \\[ \\alpha_{2t} = -1.75 - 0.15 \\log t \\, , \\, t=1, \\ldots, 7 \\] \\[ \\beta^\\top_{1} = (-\\log 0.8, -\\log 1.4, -\\log 3) \\] \\[ \\beta^\\top_{2} = (-\\log 1, -\\log 0.95, -\\log 2) \\, . \\] All together, the ETS function is defined for sampling event-type and event-time, and adding it to the data frame, as follows: from pydts.data_generation import EventTimesSampler ets = EventTimesSampler ( d_times = 7 , j_event_types = 2 ) coefficients_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ), }, \"beta\" : { 1 : - 1 * np . log ([ 0.8 , 1.4 , 3 ]), 2 : - 1 * np . log ([ 1 , 0.95 , 2 ]), }} observations_df = ets . sample_event_times ( observations_df , coefficients_dict ) If the sampled covariates and parameters' values lead to impossible survival probabilities (i.e., negative or greater than one), the sampling process will be terminated with an error message. In such scenarios, it may be useful to adjust the coefficients or constrain extreme values of the covariates to ensure that the probabilities are appropriate and the sampling process is executed successfully.","title":"Event Times"},{"location":"EventTimesSampler/#censoring-time","text":"Two types of right censoring are implemented in PyDTS, administrative and random right censoring. For administrative censoring, \\(J_i=0\\) and \\(T_i = d + 1\\) . These are the default values of observations for which the sampled event type was observed to be greater than \\(d\\) . Random right censoring is optional and could be either dependent or independent of the covariates. For example, assume \\[ \\Pr (C_i = t) = 0.05 \\quad t=1, \\ldots, 7 \\, . \\] The censoring times can be sampled by prob_lof_at_t = [ 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 ] observations_df = ets . sample_independent_lof_censoring ( observations_df , prob_lof_at_t ) To generate right-censoring times that depend on the covariates, the user should supply to censoring hazard function, \\(\\lambda_c(t|Z)\\) in the form of the logit-link model. For example, censoring_coef_dict = { \"alpha\" : { 0 : lambda t : - 0.3 - 0.3 * np . log ( t ), }, \"beta\" : { 0 : - 1 * np . log ([ 8 , 0.95 , 6 ]), }} observations_df = ets . sample_hazard_lof_censoring ( observations_df , censoring_coef_dict )","title":"Censoring Time"},{"location":"EventTimesSampler/#updating-the-observations","text":"Finally, the observed data should be updated by \\(X_i = min(T_i, C_i)\\) and \\(J_i\\) as follows observations_df = ets . update_event_or_lof ( observations_df ) The first observations of the sampled data are observations_df . head ( 30 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 J T C X 0 1 77.707214 4 2 3 8 3 1 0 79.17863 6 2 5 8 5 2 0 66.023722 5 0 8 8 8 3 1 89.672794 5 2 2 8 2 4 0 76.386179 5 0 8 8 8 5 1 79.180418 3 2 4 8 4 6 1 79.500548 5 2 5 8 5 7 1 89.930194 3 2 1 8 1 8 0 68.328199 4 2 2 8 2 9 1 92.693606 8 2 4 8 4 10 1 93.037814 6 2 1 8 1 11 1 90.09132 3 2 1 8 1 12 0 93.967038 3 2 1 2 1 13 0 54.984284 1 2 1 1 1 14 1 92.6541 8 0 8 8 8 15 0 78.404324 4 2 1 8 1 16 1 64.595813 2 2 2 8 2 17 0 66.326022 7 0 8 8 8 18 1 102.786353 6 2 2 8 2 19 1 76.981113 2 2 1 8 1 20 0 65.093074 7 0 8 8 8 21 0 70.287022 2 2 1 1 1 22 1 87.354831 5 2 1 8 1 23 1 81.826717 4 2 2 8 2 24 1 72.835886 3 2 1 8 1 25 1 77.435862 10 0 8 8 8 26 0 84.643334 4 2 2 6 2 27 0 48.055462 9 0 8 8 8 28 1 76.259105 4 2 1 8 1 29 0 68.426955 3 0 3 1 1","title":"Updating the Observations"},{"location":"EventTimesSampler/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022)","title":"References"},{"location":"ModelsComparison/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing the Estimation Methods \u00a4 Introduction \u00a4 We conducted a simulation study demonstrating the performances of Meir et al. (2022) [1] and comparing it with that of Lee et al. (2018) [2]. The data was generated in the same way as in Usage Example section, i.e. \\(M=2\\) competing events, \\(n=50,000\\) observations, Z with 5 covariates and right censoring. Failure times were generated based on \\[ \\lambda_{j}(t|Z) = \\frac{\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})} \\] with \\(\\alpha_{1t} = -1 -0.3 \\log(t)\\) , \\(\\alpha_{2t} = -1.75 -0.15\\log(t)\\) , \\(t=1,\\ldots,d\\) , \\(\\beta_1 = (-\\log 0.8, \\log 3, \\log 3, \\log 2.5, \\log 2)\\) , \\(\\beta_{2} = (-\\log 1, \\log 3, \\log 4, \\log 3, \\log 2)\\) . Censoring time for each observation was sampled from a discrete uniform distribution, i.e. \\(C_i \\sim \\mbox{Uniform}\\{1,...,d+1\\}\\) . We repeated this procedure for \\(d \\in (15, 30, 45, 60, 100)\\) and report the results in Meir et al. (2022) [1]. For each value of \\(d\\) , the results are based on 100 replications. We showed that both estimation methods perform very well in terms of bias and provide highly similar results in terms of point estimators and their standard errors. However, the computational running time of our approach is 1.5-3.5 times shorter depending on \\(d\\) , where the improvement factor increases as a function of \\(d\\) . Estimation Replications \u00a4 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 from pydts.fitters import repetitive_fitters rep_dict , times_dict , counts_df = repetitive_fitters ( rep = 100 , n_patients = n_patients , n_cov = n_cov , d_times = 60 , j_events = 2 , pid_col = 'pid' , test_size = 0.25 , verbose = 0 , real_coef_dict = real_coef_dict , censoring_prob = 0.8 ) 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 99/120 [2:29:29<31:42, 90.60s/it] final: 100 Comparing Standard Error of Lee et al. (2018) and Meir et al. (2022) \u00a4 from pydts.examples_utils.plots import plot_reps_coef_std new_res_dict = plot_reps_coef_std ( rep_dict , True ) Comparison of the Estimated Coefficients \u00a4 from pydts.examples_utils.plots import plot_models_coefficients a = new_res_dict [ 'alpha' ] b = new_res_dict [ 'beta' ] times = [ t + 1 for t in list ( a [ 1 ] . reset_index () . index )] n_cov = 5 plot_models_coefficients ( a , b , times , counts_df ) Computational Time Comparison \u00a4 from pydts.examples_utils.plots import plot_times plot_times ( times_dict ) <AxesSubplot:xlabel='Model type', ylabel='Fitting Time [seconds]'> References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881","title":"Comparing the Estimation Methods"},{"location":"ModelsComparison/#comparing-the-estimation-methods","text":"","title":"Comparing the Estimation Methods"},{"location":"ModelsComparison/#introduction","text":"We conducted a simulation study demonstrating the performances of Meir et al. (2022) [1] and comparing it with that of Lee et al. (2018) [2]. The data was generated in the same way as in Usage Example section, i.e. \\(M=2\\) competing events, \\(n=50,000\\) observations, Z with 5 covariates and right censoring. Failure times were generated based on \\[ \\lambda_{j}(t|Z) = \\frac{\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})} \\] with \\(\\alpha_{1t} = -1 -0.3 \\log(t)\\) , \\(\\alpha_{2t} = -1.75 -0.15\\log(t)\\) , \\(t=1,\\ldots,d\\) , \\(\\beta_1 = (-\\log 0.8, \\log 3, \\log 3, \\log 2.5, \\log 2)\\) , \\(\\beta_{2} = (-\\log 1, \\log 3, \\log 4, \\log 3, \\log 2)\\) . Censoring time for each observation was sampled from a discrete uniform distribution, i.e. \\(C_i \\sim \\mbox{Uniform}\\{1,...,d+1\\}\\) . We repeated this procedure for \\(d \\in (15, 30, 45, 60, 100)\\) and report the results in Meir et al. (2022) [1]. For each value of \\(d\\) , the results are based on 100 replications. We showed that both estimation methods perform very well in terms of bias and provide highly similar results in terms of point estimators and their standard errors. However, the computational running time of our approach is 1.5-3.5 times shorter depending on \\(d\\) , where the improvement factor increases as a function of \\(d\\) .","title":"Introduction"},{"location":"ModelsComparison/#estimation-replications","text":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 from pydts.fitters import repetitive_fitters rep_dict , times_dict , counts_df = repetitive_fitters ( rep = 100 , n_patients = n_patients , n_cov = n_cov , d_times = 60 , j_events = 2 , pid_col = 'pid' , test_size = 0.25 , verbose = 0 , real_coef_dict = real_coef_dict , censoring_prob = 0.8 ) 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 99/120 [2:29:29<31:42, 90.60s/it] final: 100","title":"Estimation Replications"},{"location":"ModelsComparison/#comparing-standard-error-of-lee-et-al-2018-and-meir-et-al-2022","text":"from pydts.examples_utils.plots import plot_reps_coef_std new_res_dict = plot_reps_coef_std ( rep_dict , True )","title":"Comparing Standard Error of Lee et al. (2018) and Meir et al. (2022)"},{"location":"ModelsComparison/#comparison-of-the-estimated-coefficients","text":"from pydts.examples_utils.plots import plot_models_coefficients a = new_res_dict [ 'alpha' ] b = new_res_dict [ 'beta' ] times = [ t + 1 for t in list ( a [ 1 ] . reset_index () . index )] n_cov = 5 plot_models_coefficients ( a , b , times , counts_df )","title":"Comparison of the Estimated Coefficients"},{"location":"ModelsComparison/#computational-time-comparison","text":"from pydts.examples_utils.plots import plot_times plot_times ( times_dict ) <AxesSubplot:xlabel='Model type', ylabel='Fitting Time [seconds]'>","title":"Computational Time Comparison"},{"location":"ModelsComparison/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881","title":"References"},{"location":"PaperCodeFigure/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df # Data Generation real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t )}, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ])}} patients_df = generate_quick_start_df ( n_patients = 50000 , n_cov = 5 , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) train_df , test_df = train_test_split ( patients_df , test_size = 0.2 ) # DataExpansionFitter Usage from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 )) pred_df = fitter . predict_cumulative_incident_function ( test_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 )) # TwoStagesFitter Usage from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 )) pred_df = fitter . predict_cumulative_incident_function ( test_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 )) # Training with Regularization L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 1 }} L1_regularized_fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0 }} L2_regularized_fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) EN_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0.5 }} EN_regularized_fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs )","title":"PaperCodeFigure"},{"location":"PerformanceMeasures/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Performance Measures \u00a4 Model evaluation on test data or by CV, can be done using the evaluation functions available in PyDTS and the measures of performance presented in the Methods section. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) train_df , test_df = train_test_split ( patients_df , test_size = 0.2 ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 31 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 31 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 31 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 For example, in the following code, the survival models are estimated based on the two-stage approach and the dataset train_df. Assume that the event of main interest is \\(j=1\\) . Then, \\(\\pi_{i1}(t)\\) are calculated and stored in pred_df, and finally \\(\\widehat{\\mbox{AUC}}_1(t)\\) , \\(t=1,\\ldots,d\\) , are provided by from pydts.fitters import TwoStagesFitter from pydts.evaluation import * fitter = TwoStagesFitter () fitter . fit ( df = train_df ) pred_df = fitter . predict_prob_event_j_all ( test_df , event = 1 ) auc_1 = event_specific_auc_at_t_all ( pred_df , event = 1 ) print ( f 'AUC(t) for event 1 is:' ) auc_1 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. AUC(t) for event 1 is: 1 0.988007 2 0.989981 3 0.990528 4 0.992057 5 0.991414 6 0.992463 7 0.992465 8 0.993415 9 0.993273 10 0.992431 11 0.994390 12 0.995089 13 0.992101 14 0.993462 15 0.995626 16 0.994383 17 0.994551 18 0.992995 19 0.993614 20 0.993318 21 0.994786 22 0.992859 23 0.994097 24 0.997663 25 0.998358 26 0.997495 27 0.997700 28 0.998260 29 0.994920 30 0.988650 Name: 1, dtype: float64 Other measures such as \\(\\widehat{\\mbox{AUC}}_1\\) , \\(\\widehat{\\mbox{BS}}_1\\) , \\(\\widehat{\\mbox{AUC}}\\) , and \\(\\widehat{\\mbox{BS}}\\) can be calculated by pred_df = fitter . predict_prob_events ( test_df ) ibs_1 = event_specific_integrated_brier_score ( pred_df , event = 1 ) iauc_1 = event_specific_integrated_auc ( pred_df , event = 1 ) bs = global_brier_score ( pred_df ) auc = global_auc ( pred_df ) Model evaluation based on K-fold CV and TwoStagesFitter can be done by from pydts.cross_validation import TwoStagesCV cross_validator = TwoStagesCV () cross_validator . cross_validate ( full_df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), n_splits = 5 , seed = 0 , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]) Fitting fold 1/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 2/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 3/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 4/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 5/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 10 ... 21 22 23 24 25 26 27 28 29 30 metric fold BS 0 1 0.063049 0.053907 0.045755 0.043523 0.042300 0.046062 0.037669 0.039918 0.038884 0.039600 ... 0.036635 0.058504 0.046140 0.053152 0.061784 0.050324 0.065542 0.062469 0.090904 0.072492 2 0.024376 0.019168 0.019545 0.017727 0.019594 0.017552 0.021547 0.020059 0.016670 0.019916 ... 0.018437 0.034735 0.019682 0.032066 0.023314 0.031648 0.027103 0.019322 0.048674 0.048384 AUC 0 1 0.652021 0.643642 0.641287 0.643345 0.667766 0.643727 0.655765 0.667565 0.570066 0.653903 ... 0.514947 0.531114 0.499528 0.478013 0.578898 0.500812 0.414534 0.475877 0.582044 0.412960 2 0.672119 0.677260 0.688408 0.646792 0.686449 0.659598 0.629191 0.626242 0.693669 0.635776 ... 0.612318 0.495373 0.510674 0.517435 0.542218 0.504568 0.725689 0.297011 0.563298 0.587978 BS 1 1 0.063104 0.051763 0.047522 0.045185 0.039390 0.040410 0.040533 0.037199 0.035189 0.039389 ... 0.048439 0.039984 0.049740 0.037045 0.054407 0.063798 0.056508 0.063322 0.055487 0.046272 2 0.026567 0.020499 0.023158 0.019674 0.017881 0.018550 0.012963 0.019118 0.018793 0.020397 ... 0.016248 0.024147 0.021958 0.020464 0.024052 0.030794 0.022079 0.047698 0.055665 0.039775 AUC 1 1 0.637347 0.653555 0.670154 0.653391 0.674059 0.664124 0.626964 0.639071 0.622290 0.624590 ... 0.470615 0.619649 0.576780 0.408460 0.472122 0.534623 0.390513 0.458682 0.399079 0.547307 2 0.702004 0.665337 0.698247 0.670620 0.612123 0.662859 0.732118 0.652722 0.610839 0.666786 ... 0.564700 0.564492 0.474570 0.610849 0.528472 0.602485 0.547596 0.466056 0.532451 0.328246 BS 2 1 0.064886 0.049643 0.045128 0.044718 0.040647 0.041574 0.037698 0.035049 0.041622 0.038163 ... 0.048869 0.041845 0.053532 0.052515 0.027061 0.060276 0.048393 0.064393 0.074945 0.069480 2 0.024451 0.016186 0.021169 0.021732 0.021547 0.021889 0.017873 0.020496 0.015362 0.014820 ... 0.018748 0.021711 0.017476 0.020739 0.033913 0.033076 0.031291 0.034467 0.043016 0.055695 AUC 2 1 0.659157 0.654793 0.649652 0.659449 0.674832 0.660598 0.594342 0.621234 0.648151 0.634931 ... 0.611217 0.482856 0.548870 0.517922 0.367427 0.536657 0.581426 0.453719 0.453618 0.531055 2 0.657810 0.657075 0.706675 0.713073 0.645342 0.666765 0.653272 0.674614 0.565575 0.620555 ... 0.552351 0.616195 0.470557 0.589532 0.554307 0.558689 0.494114 0.490396 0.369201 0.509630 BS 3 1 0.059994 0.050895 0.045448 0.043562 0.037141 0.043832 0.038703 0.043434 0.035425 0.037136 ... 0.039925 0.050829 0.056631 0.044824 0.062753 0.075129 0.069417 0.059778 0.069127 0.041407 2 0.024225 0.021932 0.019773 0.015853 0.017388 0.017645 0.017031 0.018022 0.015532 0.019680 ... 0.022589 0.030680 0.029422 0.024571 0.031540 0.020359 0.031400 0.025750 0.042687 0.055311 AUC 3 1 0.659007 0.665953 0.682254 0.636794 0.677254 0.626931 0.650321 0.615777 0.639428 0.600239 ... 0.464096 0.516016 0.450649 0.644558 0.493753 0.510769 0.481684 0.635580 0.504782 0.445025 2 0.702751 0.677411 0.691677 0.699105 0.657049 0.655707 0.675905 0.646852 0.718233 0.665166 ... 0.594124 0.541976 0.492703 0.830657 0.479912 0.431244 0.579373 0.386118 0.505585 0.381362 BS 4 1 0.064885 0.053155 0.047227 0.045384 0.041575 0.035789 0.038320 0.040585 0.031461 0.032940 ... 0.038013 0.043923 0.066064 0.044236 0.061176 0.055079 0.067624 0.050423 0.075282 0.103932 2 0.023856 0.019616 0.022393 0.021449 0.020102 0.016353 0.019711 0.020459 0.021224 0.017895 ... 0.025120 0.015200 0.019491 0.019334 0.022800 0.033254 0.037406 0.033727 0.043146 0.062432 AUC 4 1 0.659357 0.636783 0.654152 0.637880 0.631202 0.647121 0.638941 0.625363 0.599964 0.641299 ... 0.524715 0.494028 0.502059 0.530022 0.579939 0.578329 0.511615 0.481707 0.389172 0.331990 2 0.672312 0.657230 0.687828 0.674431 0.653015 0.616802 0.636077 0.697158 0.613994 0.613293 ... 0.524942 0.592352 0.463863 0.605593 0.477705 0.490352 0.519561 0.739740 0.509843 0.459805 20 rows \u00d7 30 columns Results of the AUC(t), BS(t) from the cross-validation procedure to each of the folds and each of the risks: cross_validator . results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 10 ... 21 22 23 24 25 26 27 28 29 30 metric fold BS 0 1 0.063049 0.053907 0.045755 0.043523 0.042300 0.046062 0.037669 0.039918 0.038884 0.039600 ... 0.036635 0.058504 0.046140 0.053152 0.061784 0.050324 0.065542 0.062469 0.090904 0.072492 2 0.024376 0.019168 0.019545 0.017727 0.019594 0.017552 0.021547 0.020059 0.016670 0.019916 ... 0.018437 0.034735 0.019682 0.032066 0.023314 0.031648 0.027103 0.019322 0.048674 0.048384 AUC 0 1 0.652021 0.643642 0.641287 0.643345 0.667766 0.643727 0.655765 0.667565 0.570066 0.653903 ... 0.514947 0.531114 0.499528 0.478013 0.578898 0.500812 0.414534 0.475877 0.582044 0.412960 2 0.672119 0.677260 0.688408 0.646792 0.686449 0.659598 0.629191 0.626242 0.693669 0.635776 ... 0.612318 0.495373 0.510674 0.517435 0.542218 0.504568 0.725689 0.297011 0.563298 0.587978 BS 1 1 0.063104 0.051763 0.047522 0.045185 0.039390 0.040410 0.040533 0.037199 0.035189 0.039389 ... 0.048439 0.039984 0.049740 0.037045 0.054407 0.063798 0.056508 0.063322 0.055487 0.046272 2 0.026567 0.020499 0.023158 0.019674 0.017881 0.018550 0.012963 0.019118 0.018793 0.020397 ... 0.016248 0.024147 0.021958 0.020464 0.024052 0.030794 0.022079 0.047698 0.055665 0.039775 AUC 1 1 0.637347 0.653555 0.670154 0.653391 0.674059 0.664124 0.626964 0.639071 0.622290 0.624590 ... 0.470615 0.619649 0.576780 0.408460 0.472122 0.534623 0.390513 0.458682 0.399079 0.547307 2 0.702004 0.665337 0.698247 0.670620 0.612123 0.662859 0.732118 0.652722 0.610839 0.666786 ... 0.564700 0.564492 0.474570 0.610849 0.528472 0.602485 0.547596 0.466056 0.532451 0.328246 BS 2 1 0.064886 0.049643 0.045128 0.044718 0.040647 0.041574 0.037698 0.035049 0.041622 0.038163 ... 0.048869 0.041845 0.053532 0.052515 0.027061 0.060276 0.048393 0.064393 0.074945 0.069480 2 0.024451 0.016186 0.021169 0.021732 0.021547 0.021889 0.017873 0.020496 0.015362 0.014820 ... 0.018748 0.021711 0.017476 0.020739 0.033913 0.033076 0.031291 0.034467 0.043016 0.055695 AUC 2 1 0.659157 0.654793 0.649652 0.659449 0.674832 0.660598 0.594342 0.621234 0.648151 0.634931 ... 0.611217 0.482856 0.548870 0.517922 0.367427 0.536657 0.581426 0.453719 0.453618 0.531055 2 0.657810 0.657075 0.706675 0.713073 0.645342 0.666765 0.653272 0.674614 0.565575 0.620555 ... 0.552351 0.616195 0.470557 0.589532 0.554307 0.558689 0.494114 0.490396 0.369201 0.509630 BS 3 1 0.059994 0.050895 0.045448 0.043562 0.037141 0.043832 0.038703 0.043434 0.035425 0.037136 ... 0.039925 0.050829 0.056631 0.044824 0.062753 0.075129 0.069417 0.059778 0.069127 0.041407 2 0.024225 0.021932 0.019773 0.015853 0.017388 0.017645 0.017031 0.018022 0.015532 0.019680 ... 0.022589 0.030680 0.029422 0.024571 0.031540 0.020359 0.031400 0.025750 0.042687 0.055311 AUC 3 1 0.659007 0.665953 0.682254 0.636794 0.677254 0.626931 0.650321 0.615777 0.639428 0.600239 ... 0.464096 0.516016 0.450649 0.644558 0.493753 0.510769 0.481684 0.635580 0.504782 0.445025 2 0.702751 0.677411 0.691677 0.699105 0.657049 0.655707 0.675905 0.646852 0.718233 0.665166 ... 0.594124 0.541976 0.492703 0.830657 0.479912 0.431244 0.579373 0.386118 0.505585 0.381362 BS 4 1 0.064885 0.053155 0.047227 0.045384 0.041575 0.035789 0.038320 0.040585 0.031461 0.032940 ... 0.038013 0.043923 0.066064 0.044236 0.061176 0.055079 0.067624 0.050423 0.075282 0.103932 2 0.023856 0.019616 0.022393 0.021449 0.020102 0.016353 0.019711 0.020459 0.021224 0.017895 ... 0.025120 0.015200 0.019491 0.019334 0.022800 0.033254 0.037406 0.033727 0.043146 0.062432 AUC 4 1 0.659357 0.636783 0.654152 0.637880 0.631202 0.647121 0.638941 0.625363 0.599964 0.641299 ... 0.524715 0.494028 0.502059 0.530022 0.579939 0.578329 0.511615 0.481707 0.389172 0.331990 2 0.672312 0.657230 0.687828 0.674431 0.653015 0.616802 0.636077 0.697158 0.613994 0.613293 ... 0.524942 0.592352 0.463863 0.605593 0.477705 0.490352 0.519561 0.739740 0.509843 0.459805 20 rows \u00d7 30 columns with the integrated AUC and BS to each of folds and each of the risks: pd . DataFrame . from_records ( cross_validator . integrated_auc ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 1 0.629493 0.627465 0.636286 0.634377 0.622095 2 0.651277 0.647620 0.646869 0.655923 0.642630 pd . DataFrame . from_records ( cross_validator . integrated_bs ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 1 0.048382 0.046735 0.046411 0.046305 0.047747 2 0.020957 0.021362 0.021003 0.020547 0.021381 and lastly, the global AUC and global BS to each of the folds: print ( cross_validator . global_auc ) {0: 0.635987860592661, 1: 0.633667449693418, 2: 0.6395581436224567, 3: 0.6408781976004154, 4: 0.6284087721165604} print ( cross_validator . global_bs ) {0: 0.0402049045296208, 1: 0.03892613679526756, 2: 0.03855544852975435, 3: 0.0385327046054388, 4: 0.039640037102306486} References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022)","title":"Evaluation"},{"location":"PerformanceMeasures/#performance-measures","text":"Model evaluation on test data or by CV, can be done using the evaluation functions available in PyDTS and the measures of performance presented in the Methods section. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) train_df , test_df = train_test_split ( patients_df , test_size = 0.2 ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 31 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 31 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 31 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 For example, in the following code, the survival models are estimated based on the two-stage approach and the dataset train_df. Assume that the event of main interest is \\(j=1\\) . Then, \\(\\pi_{i1}(t)\\) are calculated and stored in pred_df, and finally \\(\\widehat{\\mbox{AUC}}_1(t)\\) , \\(t=1,\\ldots,d\\) , are provided by from pydts.fitters import TwoStagesFitter from pydts.evaluation import * fitter = TwoStagesFitter () fitter . fit ( df = train_df ) pred_df = fitter . predict_prob_event_j_all ( test_df , event = 1 ) auc_1 = event_specific_auc_at_t_all ( pred_df , event = 1 ) print ( f 'AUC(t) for event 1 is:' ) auc_1 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. AUC(t) for event 1 is: 1 0.988007 2 0.989981 3 0.990528 4 0.992057 5 0.991414 6 0.992463 7 0.992465 8 0.993415 9 0.993273 10 0.992431 11 0.994390 12 0.995089 13 0.992101 14 0.993462 15 0.995626 16 0.994383 17 0.994551 18 0.992995 19 0.993614 20 0.993318 21 0.994786 22 0.992859 23 0.994097 24 0.997663 25 0.998358 26 0.997495 27 0.997700 28 0.998260 29 0.994920 30 0.988650 Name: 1, dtype: float64 Other measures such as \\(\\widehat{\\mbox{AUC}}_1\\) , \\(\\widehat{\\mbox{BS}}_1\\) , \\(\\widehat{\\mbox{AUC}}\\) , and \\(\\widehat{\\mbox{BS}}\\) can be calculated by pred_df = fitter . predict_prob_events ( test_df ) ibs_1 = event_specific_integrated_brier_score ( pred_df , event = 1 ) iauc_1 = event_specific_integrated_auc ( pred_df , event = 1 ) bs = global_brier_score ( pred_df ) auc = global_auc ( pred_df ) Model evaluation based on K-fold CV and TwoStagesFitter can be done by from pydts.cross_validation import TwoStagesCV cross_validator = TwoStagesCV () cross_validator . cross_validate ( full_df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), n_splits = 5 , seed = 0 , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]) Fitting fold 1/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 2/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 3/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 4/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Fitting fold 5/5 INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 10 ... 21 22 23 24 25 26 27 28 29 30 metric fold BS 0 1 0.063049 0.053907 0.045755 0.043523 0.042300 0.046062 0.037669 0.039918 0.038884 0.039600 ... 0.036635 0.058504 0.046140 0.053152 0.061784 0.050324 0.065542 0.062469 0.090904 0.072492 2 0.024376 0.019168 0.019545 0.017727 0.019594 0.017552 0.021547 0.020059 0.016670 0.019916 ... 0.018437 0.034735 0.019682 0.032066 0.023314 0.031648 0.027103 0.019322 0.048674 0.048384 AUC 0 1 0.652021 0.643642 0.641287 0.643345 0.667766 0.643727 0.655765 0.667565 0.570066 0.653903 ... 0.514947 0.531114 0.499528 0.478013 0.578898 0.500812 0.414534 0.475877 0.582044 0.412960 2 0.672119 0.677260 0.688408 0.646792 0.686449 0.659598 0.629191 0.626242 0.693669 0.635776 ... 0.612318 0.495373 0.510674 0.517435 0.542218 0.504568 0.725689 0.297011 0.563298 0.587978 BS 1 1 0.063104 0.051763 0.047522 0.045185 0.039390 0.040410 0.040533 0.037199 0.035189 0.039389 ... 0.048439 0.039984 0.049740 0.037045 0.054407 0.063798 0.056508 0.063322 0.055487 0.046272 2 0.026567 0.020499 0.023158 0.019674 0.017881 0.018550 0.012963 0.019118 0.018793 0.020397 ... 0.016248 0.024147 0.021958 0.020464 0.024052 0.030794 0.022079 0.047698 0.055665 0.039775 AUC 1 1 0.637347 0.653555 0.670154 0.653391 0.674059 0.664124 0.626964 0.639071 0.622290 0.624590 ... 0.470615 0.619649 0.576780 0.408460 0.472122 0.534623 0.390513 0.458682 0.399079 0.547307 2 0.702004 0.665337 0.698247 0.670620 0.612123 0.662859 0.732118 0.652722 0.610839 0.666786 ... 0.564700 0.564492 0.474570 0.610849 0.528472 0.602485 0.547596 0.466056 0.532451 0.328246 BS 2 1 0.064886 0.049643 0.045128 0.044718 0.040647 0.041574 0.037698 0.035049 0.041622 0.038163 ... 0.048869 0.041845 0.053532 0.052515 0.027061 0.060276 0.048393 0.064393 0.074945 0.069480 2 0.024451 0.016186 0.021169 0.021732 0.021547 0.021889 0.017873 0.020496 0.015362 0.014820 ... 0.018748 0.021711 0.017476 0.020739 0.033913 0.033076 0.031291 0.034467 0.043016 0.055695 AUC 2 1 0.659157 0.654793 0.649652 0.659449 0.674832 0.660598 0.594342 0.621234 0.648151 0.634931 ... 0.611217 0.482856 0.548870 0.517922 0.367427 0.536657 0.581426 0.453719 0.453618 0.531055 2 0.657810 0.657075 0.706675 0.713073 0.645342 0.666765 0.653272 0.674614 0.565575 0.620555 ... 0.552351 0.616195 0.470557 0.589532 0.554307 0.558689 0.494114 0.490396 0.369201 0.509630 BS 3 1 0.059994 0.050895 0.045448 0.043562 0.037141 0.043832 0.038703 0.043434 0.035425 0.037136 ... 0.039925 0.050829 0.056631 0.044824 0.062753 0.075129 0.069417 0.059778 0.069127 0.041407 2 0.024225 0.021932 0.019773 0.015853 0.017388 0.017645 0.017031 0.018022 0.015532 0.019680 ... 0.022589 0.030680 0.029422 0.024571 0.031540 0.020359 0.031400 0.025750 0.042687 0.055311 AUC 3 1 0.659007 0.665953 0.682254 0.636794 0.677254 0.626931 0.650321 0.615777 0.639428 0.600239 ... 0.464096 0.516016 0.450649 0.644558 0.493753 0.510769 0.481684 0.635580 0.504782 0.445025 2 0.702751 0.677411 0.691677 0.699105 0.657049 0.655707 0.675905 0.646852 0.718233 0.665166 ... 0.594124 0.541976 0.492703 0.830657 0.479912 0.431244 0.579373 0.386118 0.505585 0.381362 BS 4 1 0.064885 0.053155 0.047227 0.045384 0.041575 0.035789 0.038320 0.040585 0.031461 0.032940 ... 0.038013 0.043923 0.066064 0.044236 0.061176 0.055079 0.067624 0.050423 0.075282 0.103932 2 0.023856 0.019616 0.022393 0.021449 0.020102 0.016353 0.019711 0.020459 0.021224 0.017895 ... 0.025120 0.015200 0.019491 0.019334 0.022800 0.033254 0.037406 0.033727 0.043146 0.062432 AUC 4 1 0.659357 0.636783 0.654152 0.637880 0.631202 0.647121 0.638941 0.625363 0.599964 0.641299 ... 0.524715 0.494028 0.502059 0.530022 0.579939 0.578329 0.511615 0.481707 0.389172 0.331990 2 0.672312 0.657230 0.687828 0.674431 0.653015 0.616802 0.636077 0.697158 0.613994 0.613293 ... 0.524942 0.592352 0.463863 0.605593 0.477705 0.490352 0.519561 0.739740 0.509843 0.459805 20 rows \u00d7 30 columns Results of the AUC(t), BS(t) from the cross-validation procedure to each of the folds and each of the risks: cross_validator . results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 10 ... 21 22 23 24 25 26 27 28 29 30 metric fold BS 0 1 0.063049 0.053907 0.045755 0.043523 0.042300 0.046062 0.037669 0.039918 0.038884 0.039600 ... 0.036635 0.058504 0.046140 0.053152 0.061784 0.050324 0.065542 0.062469 0.090904 0.072492 2 0.024376 0.019168 0.019545 0.017727 0.019594 0.017552 0.021547 0.020059 0.016670 0.019916 ... 0.018437 0.034735 0.019682 0.032066 0.023314 0.031648 0.027103 0.019322 0.048674 0.048384 AUC 0 1 0.652021 0.643642 0.641287 0.643345 0.667766 0.643727 0.655765 0.667565 0.570066 0.653903 ... 0.514947 0.531114 0.499528 0.478013 0.578898 0.500812 0.414534 0.475877 0.582044 0.412960 2 0.672119 0.677260 0.688408 0.646792 0.686449 0.659598 0.629191 0.626242 0.693669 0.635776 ... 0.612318 0.495373 0.510674 0.517435 0.542218 0.504568 0.725689 0.297011 0.563298 0.587978 BS 1 1 0.063104 0.051763 0.047522 0.045185 0.039390 0.040410 0.040533 0.037199 0.035189 0.039389 ... 0.048439 0.039984 0.049740 0.037045 0.054407 0.063798 0.056508 0.063322 0.055487 0.046272 2 0.026567 0.020499 0.023158 0.019674 0.017881 0.018550 0.012963 0.019118 0.018793 0.020397 ... 0.016248 0.024147 0.021958 0.020464 0.024052 0.030794 0.022079 0.047698 0.055665 0.039775 AUC 1 1 0.637347 0.653555 0.670154 0.653391 0.674059 0.664124 0.626964 0.639071 0.622290 0.624590 ... 0.470615 0.619649 0.576780 0.408460 0.472122 0.534623 0.390513 0.458682 0.399079 0.547307 2 0.702004 0.665337 0.698247 0.670620 0.612123 0.662859 0.732118 0.652722 0.610839 0.666786 ... 0.564700 0.564492 0.474570 0.610849 0.528472 0.602485 0.547596 0.466056 0.532451 0.328246 BS 2 1 0.064886 0.049643 0.045128 0.044718 0.040647 0.041574 0.037698 0.035049 0.041622 0.038163 ... 0.048869 0.041845 0.053532 0.052515 0.027061 0.060276 0.048393 0.064393 0.074945 0.069480 2 0.024451 0.016186 0.021169 0.021732 0.021547 0.021889 0.017873 0.020496 0.015362 0.014820 ... 0.018748 0.021711 0.017476 0.020739 0.033913 0.033076 0.031291 0.034467 0.043016 0.055695 AUC 2 1 0.659157 0.654793 0.649652 0.659449 0.674832 0.660598 0.594342 0.621234 0.648151 0.634931 ... 0.611217 0.482856 0.548870 0.517922 0.367427 0.536657 0.581426 0.453719 0.453618 0.531055 2 0.657810 0.657075 0.706675 0.713073 0.645342 0.666765 0.653272 0.674614 0.565575 0.620555 ... 0.552351 0.616195 0.470557 0.589532 0.554307 0.558689 0.494114 0.490396 0.369201 0.509630 BS 3 1 0.059994 0.050895 0.045448 0.043562 0.037141 0.043832 0.038703 0.043434 0.035425 0.037136 ... 0.039925 0.050829 0.056631 0.044824 0.062753 0.075129 0.069417 0.059778 0.069127 0.041407 2 0.024225 0.021932 0.019773 0.015853 0.017388 0.017645 0.017031 0.018022 0.015532 0.019680 ... 0.022589 0.030680 0.029422 0.024571 0.031540 0.020359 0.031400 0.025750 0.042687 0.055311 AUC 3 1 0.659007 0.665953 0.682254 0.636794 0.677254 0.626931 0.650321 0.615777 0.639428 0.600239 ... 0.464096 0.516016 0.450649 0.644558 0.493753 0.510769 0.481684 0.635580 0.504782 0.445025 2 0.702751 0.677411 0.691677 0.699105 0.657049 0.655707 0.675905 0.646852 0.718233 0.665166 ... 0.594124 0.541976 0.492703 0.830657 0.479912 0.431244 0.579373 0.386118 0.505585 0.381362 BS 4 1 0.064885 0.053155 0.047227 0.045384 0.041575 0.035789 0.038320 0.040585 0.031461 0.032940 ... 0.038013 0.043923 0.066064 0.044236 0.061176 0.055079 0.067624 0.050423 0.075282 0.103932 2 0.023856 0.019616 0.022393 0.021449 0.020102 0.016353 0.019711 0.020459 0.021224 0.017895 ... 0.025120 0.015200 0.019491 0.019334 0.022800 0.033254 0.037406 0.033727 0.043146 0.062432 AUC 4 1 0.659357 0.636783 0.654152 0.637880 0.631202 0.647121 0.638941 0.625363 0.599964 0.641299 ... 0.524715 0.494028 0.502059 0.530022 0.579939 0.578329 0.511615 0.481707 0.389172 0.331990 2 0.672312 0.657230 0.687828 0.674431 0.653015 0.616802 0.636077 0.697158 0.613994 0.613293 ... 0.524942 0.592352 0.463863 0.605593 0.477705 0.490352 0.519561 0.739740 0.509843 0.459805 20 rows \u00d7 30 columns with the integrated AUC and BS to each of folds and each of the risks: pd . DataFrame . from_records ( cross_validator . integrated_auc ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 1 0.629493 0.627465 0.636286 0.634377 0.622095 2 0.651277 0.647620 0.646869 0.655923 0.642630 pd . DataFrame . from_records ( cross_validator . integrated_bs ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 1 0.048382 0.046735 0.046411 0.046305 0.047747 2 0.020957 0.021362 0.021003 0.020547 0.021381 and lastly, the global AUC and global BS to each of the folds: print ( cross_validator . global_auc ) {0: 0.635987860592661, 1: 0.633667449693418, 2: 0.6395581436224567, 3: 0.6408781976004154, 4: 0.6284087721165604} print ( cross_validator . global_bs ) {0: 0.0402049045296208, 1: 0.03892613679526756, 2: 0.03855544852975435, 3: 0.0385327046054388, 4: 0.039640037102306486}","title":"Performance Measures"},{"location":"PerformanceMeasures/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022)","title":"References"},{"location":"Regularization/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regularization \u00a4 Regularized regression can be easily accommodated only with TwoStagesFitter where we first estimate \\(\\beta_j\\) and then \\(\\alpha_{jt}\\) . Regularization is introduced by CoxPHFitter of lifelines with event-specific tuning parameters, \\(\\eta_j \\geq 0\\) , and l1_ratio argument. For each \\(j\\) , usually, a path of models in \\(\\eta_j\\) are fitted, and the value of l1_ratio defines the type of prediction model. In particular, ridge regression is performed by setting l1_ratio=0, lasso by l1_ratio=1, and elastic net by 0 < l1_ratio < 1. In the following, we present how to use PyDTS to fit a lasso regularized model, and how to tune the regularization parameters \\(\\eta_j\\) . We start by generating data, as discussed in previous sections: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) train_df , test_df = train_test_split ( patients_df , test_size = 0.2 ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 31 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 31 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 31 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 Predefined Regularization Parameters \u00a4 Lasso with \\(\\eta_1=0.003\\) and \\(\\eta_2=0.005\\) , can be applied by from pydts.fitters import TwoStagesFitter L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 1 : { 'penalizer' : 0.003 , 'l1_ratio' : 1 }, 2 : { 'penalizer' : 0.005 , 'l1_ratio' : 1 } }} L1_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L1_regularized_fitter . print_summary () INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000101 1.981085e-08 0.000024 Z2 -0.772797 0.025400 -7.976064e-07 0.000071 Z3 -0.761229 0.025532 -1.720702e-01 0.038499 Z4 -0.550481 0.025318 -8.073968e-07 0.000072 Z5 -0.338471 0.025211 -3.409610e-07 0.000031 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 3374 True -1.471644 2 2328 True -1.714213 3 1805 True -1.859723 4 1524 True -1.920774 5 1214 True -2.050566 6 1114 True -2.038532 7 916 True -2.142666 8 830 True -2.151764 9 683 True -2.257665 10 626 True -2.258146 11 569 True -2.268550 12 516 True -2.281249 13 419 True -2.406485 14 410 True -2.340125 15 326 True -2.482320 16 320 True -2.415531 17 280 True -2.460601 18 240 True -2.526029 19 243 True -2.422234 20 204 True -2.506867 21 176 True -2.564875 22 167 True -2.524524 23 166 True -2.431232 24 118 True -2.667097 25 114 True -2.596554 26 109 True -2.527812 27 89 True -2.614859 28 70 True -2.731941 29 67 True -2.645782 30 47 True -2.856479 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 1250 True -3.578498 2 839 True -3.857174 3 805 True -3.793806 4 644 True -3.922340 5 570 True -3.953290 6 483 True -4.033435 7 416 True -4.097484 8 409 True -4.031967 9 323 True -4.185559 10 306 True -4.159369 11 240 True -4.326672 12 246 True -4.222570 13 226 True -4.228361 14 198 True -4.280986 15 170 True -4.351662 16 162 True -4.321422 17 147 True -4.335179 18 115 True -4.497948 19 125 True -4.329126 20 118 True -4.299702 21 83 True -4.569291 22 89 True -4.409830 23 65 True -4.633946 24 59 True -4.627577 25 58 True -4.544617 26 53 True -4.528494 27 43 True -4.624557 28 38 True -4.626009 29 43 True -4.376003 30 37 True -4.384266 Tuning Regularization Parameters \u00a4 In penalized regression, one should fit a path of models in each \\(\\eta_j\\) , \\(j=1,\\ldots,M\\) . The final set of values of \\(\\eta_1,\\ldots,\\eta_M\\) corresponds to the values yielding the best results in terms of pre-specified criteria, such as maximizing \\(\\widehat{\\mbox{AUC}}_j\\) and \\(\\widehat{\\mbox{AUC}}\\) , or minimizing \\(\\widehat{\\mbox{BS}}_j\\) and \\(\\widehat{\\mbox{BS}}\\) . The default criteria in PyDTS is maximizing the global AUC, \\(\\widehat{\\mbox{AUC}}\\) . Two \\(M\\) -dimensional grid search options are implemented, PenaltyGridSearch when the user provides train and test datasets, and PenaltyGridSearchCV for applying a K-fold cross validation (CV) approach. PenaltyGridSearch \u00a4 When train and test sets are available, by excecuting the following code, all the four optimization criteria are calculated over the \\(M\\) -dimensional grid and optimal_set includes the optimal values of \\(\\eta_1,\\ldots,\\eta_M\\) based on \\(\\widehat{\\mbox{AUC}}\\) . Here, the optimal set based on \\(\\widehat{\\mbox{AUC}}\\) is \\(\\log\\eta_1 = -6\\) and \\(\\log\\eta_2 = -6\\) . It is noted, that we estimate the parameters of each \\(\\eta_j\\) once. However, since our performance measures requires the evaluation of the overall survival function, we must check each possible combination of \\(\\eta_j\\) seperately. This can be time consuming, especially when we would like to choose between a large number of possible penalizers. from pydts.model_selection import PenaltyGridSearch penalizers = np . exp ([ - 2 , - 3 , - 4 , - 5 , - 6 ]) grid_search = PenaltyGridSearch () optimal_set = grid_search . evaluate ( train_df , test_df , l1_ratio = 1 , penalizers = penalizers , metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) print ( optimal_set ) Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 199 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 204 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 206 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 207 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 213 seconds (0.0024787521766663585, 0.0024787521766663585) The user can choose the set of \\(\\eta_j\\) , \\(j=1,\\ldots,M\\) , values that optimizes other desired criteria. For example, the set that minimizes \\(\\widehat{\\mbox{BS}}\\) can be selected as follows res = grid_search . convert_results_dict_to_df ( grid_search . global_bs ) res . columns = [ 'BS' ] res . index . set_names ([ 'eta_1' , 'eta_2' ], inplace = True ) res .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BS eta_1 eta_2 0.135335 0.135335 0.038662 0.049787 0.038662 0.018316 0.038633 0.006738 0.038468 0.002479 0.038137 0.049787 0.135335 0.038430 0.049787 0.038430 0.018316 0.038403 0.006738 0.038245 0.002479 0.037919 0.018316 0.135335 0.035197 0.049787 0.035197 0.018316 0.035199 0.006738 0.035164 0.002479 0.034911 0.006738 0.135335 0.031541 0.049787 0.031541 0.018316 0.031592 0.006738 0.031685 0.002479 0.031431 0.002479 0.135335 0.028503 0.049787 0.028503 0.018316 0.028563 0.006738 0.028698 0.002479 0.028441 grid_search . convert_results_dict_to_df ( grid_search . global_bs ) . idxmin () 0 (0.0024787521766663585, 0.0024787521766663585) dtype: object the final model can be retrieved by optimal_two_stages_fitter = grid_search . get_mixed_two_stages_fitter ( optimal_set ) PenaltyGridSearchCV \u00a4 Alternatively, 5-fold CV is performed by from pydts.cross_validation import PenaltyGridSearchCV penalizers = np . exp ([ - 2 , - 3 , - 4 , - 5 , - 6 ]) grid_search_cv = PenaltyGridSearchCV () results_df = grid_search_cv . cross_validate ( patients_df , l1_ratio = 1 , penalizers = penalizers , n_splits = 5 , metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) Starting fold 1/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 174 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 186 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 209 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 155 seconds Finished fold 1/5, 1003 seconds Starting fold 2/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 175 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 187 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 212 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 178 seconds Finished fold 2/5, 1031 seconds Starting fold 3/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 176 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 210 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 186 seconds Finished fold 3/5, 1039 seconds Starting fold 4/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 177 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 202 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 211 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 179 seconds Finished fold 4/5, 1037 seconds Starting fold 5/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 176 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 201 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 213 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 182 seconds Finished fold 5/5, 1040 seconds results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Mean SE 0.135335 0.135335 0.638475 0.002592 0.049787 0.639094 0.003754 0.018316 0.639103 0.003627 0.006738 0.637383 0.002958 0.002479 0.488831 0.003291 0.049787 0.135335 0.638764 0.002806 0.049787 0.638899 0.003862 0.018316 0.639014 0.003780 0.006738 0.637522 0.003173 0.002479 0.488832 0.003291 0.018316 0.135335 0.638666 0.002676 0.049787 0.638953 0.003742 0.018316 0.639031 0.003669 0.006738 0.637576 0.003055 0.002479 0.488833 0.003298 0.006738 0.135335 0.563873 0.001147 0.049787 0.563873 0.001147 0.018316 0.563872 0.001150 0.006738 0.563827 0.001226 0.002479 0.615068 0.002719 0.002479 0.135335 0.568879 0.001269 0.049787 0.568879 0.001269 0.018316 0.568879 0.001268 0.006738 0.568834 0.001290 0.002479 0.630541 0.004160 optimal_set = results_df [ 'Mean' ] . idxmax () optimal_set (0.1353352832366127, 0.01831563888873418) References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022)","title":"Regularization"},{"location":"Regularization/#regularization","text":"Regularized regression can be easily accommodated only with TwoStagesFitter where we first estimate \\(\\beta_j\\) and then \\(\\alpha_{jt}\\) . Regularization is introduced by CoxPHFitter of lifelines with event-specific tuning parameters, \\(\\eta_j \\geq 0\\) , and l1_ratio argument. For each \\(j\\) , usually, a path of models in \\(\\eta_j\\) are fitted, and the value of l1_ratio defines the type of prediction model. In particular, ridge regression is performed by setting l1_ratio=0, lasso by l1_ratio=1, and elastic net by 0 < l1_ratio < 1. In the following, we present how to use PyDTS to fit a lasso regularized model, and how to tune the regularization parameters \\(\\eta_j\\) . We start by generating data, as discussed in previous sections: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) train_df , test_df = train_test_split ( patients_df , test_size = 0.2 ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 31 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 31 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 31 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14","title":"Regularization"},{"location":"Regularization/#predefined-regularization-parameters","text":"Lasso with \\(\\eta_1=0.003\\) and \\(\\eta_2=0.005\\) , can be applied by from pydts.fitters import TwoStagesFitter L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 1 : { 'penalizer' : 0.003 , 'l1_ratio' : 1 }, 2 : { 'penalizer' : 0.005 , 'l1_ratio' : 1 } }} L1_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L1_regularized_fitter . print_summary () INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000101 1.981085e-08 0.000024 Z2 -0.772797 0.025400 -7.976064e-07 0.000071 Z3 -0.761229 0.025532 -1.720702e-01 0.038499 Z4 -0.550481 0.025318 -8.073968e-07 0.000072 Z5 -0.338471 0.025211 -3.409610e-07 0.000031 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 3374 True -1.471644 2 2328 True -1.714213 3 1805 True -1.859723 4 1524 True -1.920774 5 1214 True -2.050566 6 1114 True -2.038532 7 916 True -2.142666 8 830 True -2.151764 9 683 True -2.257665 10 626 True -2.258146 11 569 True -2.268550 12 516 True -2.281249 13 419 True -2.406485 14 410 True -2.340125 15 326 True -2.482320 16 320 True -2.415531 17 280 True -2.460601 18 240 True -2.526029 19 243 True -2.422234 20 204 True -2.506867 21 176 True -2.564875 22 167 True -2.524524 23 166 True -2.431232 24 118 True -2.667097 25 114 True -2.596554 26 109 True -2.527812 27 89 True -2.614859 28 70 True -2.731941 29 67 True -2.645782 30 47 True -2.856479 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 1250 True -3.578498 2 839 True -3.857174 3 805 True -3.793806 4 644 True -3.922340 5 570 True -3.953290 6 483 True -4.033435 7 416 True -4.097484 8 409 True -4.031967 9 323 True -4.185559 10 306 True -4.159369 11 240 True -4.326672 12 246 True -4.222570 13 226 True -4.228361 14 198 True -4.280986 15 170 True -4.351662 16 162 True -4.321422 17 147 True -4.335179 18 115 True -4.497948 19 125 True -4.329126 20 118 True -4.299702 21 83 True -4.569291 22 89 True -4.409830 23 65 True -4.633946 24 59 True -4.627577 25 58 True -4.544617 26 53 True -4.528494 27 43 True -4.624557 28 38 True -4.626009 29 43 True -4.376003 30 37 True -4.384266","title":"Predefined Regularization Parameters"},{"location":"Regularization/#tuning-regularization-parameters","text":"In penalized regression, one should fit a path of models in each \\(\\eta_j\\) , \\(j=1,\\ldots,M\\) . The final set of values of \\(\\eta_1,\\ldots,\\eta_M\\) corresponds to the values yielding the best results in terms of pre-specified criteria, such as maximizing \\(\\widehat{\\mbox{AUC}}_j\\) and \\(\\widehat{\\mbox{AUC}}\\) , or minimizing \\(\\widehat{\\mbox{BS}}_j\\) and \\(\\widehat{\\mbox{BS}}\\) . The default criteria in PyDTS is maximizing the global AUC, \\(\\widehat{\\mbox{AUC}}\\) . Two \\(M\\) -dimensional grid search options are implemented, PenaltyGridSearch when the user provides train and test datasets, and PenaltyGridSearchCV for applying a K-fold cross validation (CV) approach.","title":"Tuning Regularization Parameters"},{"location":"Regularization/#penaltygridsearch","text":"When train and test sets are available, by excecuting the following code, all the four optimization criteria are calculated over the \\(M\\) -dimensional grid and optimal_set includes the optimal values of \\(\\eta_1,\\ldots,\\eta_M\\) based on \\(\\widehat{\\mbox{AUC}}\\) . Here, the optimal set based on \\(\\widehat{\\mbox{AUC}}\\) is \\(\\log\\eta_1 = -6\\) and \\(\\log\\eta_2 = -6\\) . It is noted, that we estimate the parameters of each \\(\\eta_j\\) once. However, since our performance measures requires the evaluation of the overall survival function, we must check each possible combination of \\(\\eta_j\\) seperately. This can be time consuming, especially when we would like to choose between a large number of possible penalizers. from pydts.model_selection import PenaltyGridSearch penalizers = np . exp ([ - 2 , - 3 , - 4 , - 5 , - 6 ]) grid_search = PenaltyGridSearch () optimal_set = grid_search . evaluate ( train_df , test_df , l1_ratio = 1 , penalizers = penalizers , metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) print ( optimal_set ) Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 199 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 204 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 206 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 207 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 213 seconds (0.0024787521766663585, 0.0024787521766663585) The user can choose the set of \\(\\eta_j\\) , \\(j=1,\\ldots,M\\) , values that optimizes other desired criteria. For example, the set that minimizes \\(\\widehat{\\mbox{BS}}\\) can be selected as follows res = grid_search . convert_results_dict_to_df ( grid_search . global_bs ) res . columns = [ 'BS' ] res . index . set_names ([ 'eta_1' , 'eta_2' ], inplace = True ) res .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BS eta_1 eta_2 0.135335 0.135335 0.038662 0.049787 0.038662 0.018316 0.038633 0.006738 0.038468 0.002479 0.038137 0.049787 0.135335 0.038430 0.049787 0.038430 0.018316 0.038403 0.006738 0.038245 0.002479 0.037919 0.018316 0.135335 0.035197 0.049787 0.035197 0.018316 0.035199 0.006738 0.035164 0.002479 0.034911 0.006738 0.135335 0.031541 0.049787 0.031541 0.018316 0.031592 0.006738 0.031685 0.002479 0.031431 0.002479 0.135335 0.028503 0.049787 0.028503 0.018316 0.028563 0.006738 0.028698 0.002479 0.028441 grid_search . convert_results_dict_to_df ( grid_search . global_bs ) . idxmin () 0 (0.0024787521766663585, 0.0024787521766663585) dtype: object the final model can be retrieved by optimal_two_stages_fitter = grid_search . get_mixed_two_stages_fitter ( optimal_set )","title":"PenaltyGridSearch"},{"location":"Regularization/#penaltygridsearchcv","text":"Alternatively, 5-fold CV is performed by from pydts.cross_validation import PenaltyGridSearchCV penalizers = np . exp ([ - 2 , - 3 , - 4 , - 5 , - 6 ]) grid_search_cv = PenaltyGridSearchCV () results_df = grid_search_cv . cross_validate ( patients_df , l1_ratio = 1 , penalizers = penalizers , n_splits = 5 , metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) Starting fold 1/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 174 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 186 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 209 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 155 seconds Finished fold 1/5, 1003 seconds Starting fold 2/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 175 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 187 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 212 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 178 seconds Finished fold 2/5, 1031 seconds Starting fold 3/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 176 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 210 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 186 seconds Finished fold 3/5, 1039 seconds Starting fold 4/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 177 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 202 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 211 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 179 seconds Finished fold 4/5, 1037 seconds Starting fold 5/5 Started estimating the coefficients for penalizer 0.1353352832366127 (1/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 176 seconds Started estimating the coefficients for penalizer 0.049787068367863944 (2/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds Started estimating the coefficients for penalizer 0.01831563888873418 (3/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 201 seconds Started estimating the coefficients for penalizer 0.006737946999085467 (4/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 213 seconds Started estimating the coefficients for penalizer 0.0024787521766663585 (5/5) INFO: Pandarallel will run on 8 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Finished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 182 seconds Finished fold 5/5, 1040 seconds results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Mean SE 0.135335 0.135335 0.638475 0.002592 0.049787 0.639094 0.003754 0.018316 0.639103 0.003627 0.006738 0.637383 0.002958 0.002479 0.488831 0.003291 0.049787 0.135335 0.638764 0.002806 0.049787 0.638899 0.003862 0.018316 0.639014 0.003780 0.006738 0.637522 0.003173 0.002479 0.488832 0.003291 0.018316 0.135335 0.638666 0.002676 0.049787 0.638953 0.003742 0.018316 0.639031 0.003669 0.006738 0.637576 0.003055 0.002479 0.488833 0.003298 0.006738 0.135335 0.563873 0.001147 0.049787 0.563873 0.001147 0.018316 0.563872 0.001150 0.006738 0.563827 0.001226 0.002479 0.615068 0.002719 0.002479 0.135335 0.568879 0.001269 0.049787 0.568879 0.001269 0.018316 0.568879 0.001268 0.006738 0.568834 0.001290 0.002479 0.630541 0.004160 optimal_set = results_df [ 'Mean' ] . idxmax () optimal_set (0.1353352832366127, 0.01831563888873418)","title":"PenaltyGridSearchCV"},{"location":"Regularization/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022)","title":"References"},{"location":"Simple%20Simulation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Simple Example \u00a4 Introduction \u00a4 In this example, we present an existing method [1] and our new method on a very simple simulated dataset. We compare both methods based on simplicity, goodness of fit and performance. [1] On the Analysis of Discrete Time Competing Risks Data, Biometrics, Lee et al. 2018 Data Generation \u00a4 We simulate N=50,000 patients, with 5 covariate \\(Z_p\\) ( \\(p \\in [1,...,5]\\) ) randomly sampled from \\(\\mathbb{R} [0,1]\\) . Our timeline is discrete, i.e \\(t \\in [1,...,d]\\) . Here, we choose \\(d=30\\) . The event type \\(J_i\\) is randomly sampled from \\(J_i \\sim Uniform[1,...,M]\\) , where \\(M\\) is the number of competing events. In this example \\(M=2\\) . For each of the samples, we sample it's time-to-event \\(T_i\\) from the hazard function \\(\\lambda_{j}(T|Z) = \\frac{exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+exp(\\alpha_{jt}+Z^{T}\\beta_{j})}\\) such that \\(\\alpha_{1t} = -1-0.3 log(t)\\) \\(\\beta_{1} = -log([0.8, 3, 3, 2.5, 2])\\) \\(\\alpha_{2t} = -1.75-0.15 log(t)\\) \\(\\beta_{2} = -log([1, 3, 4, 3, 2])\\) and randomly sample a censoring time \\(C_i\\) such that \\(C_i \\sim Uniform[1, ... , d]\\) . We then calculate \\(X_i = min(T_i, C_i)\\) and for cencored samples we set the event type to be \\(J=0\\) . Lastly, we split the data into two separate datasets for training (75%) and for testing (25%). import pandas as pd from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline import numpy as np real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , real_coef_dict = real_coef_dict ) covariates = [ f 'Z { i + 1 } ' for i in range ( n_cov )] train_df , test_df = train_test_split ( patients_df , test_size = 0.25 ) events = sorted ( train_df [ 'J' ] . unique ()) times = sorted ( train_df [ 'X' ] . unique ()) train_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 31111 31111 0.661973 0.211958 0.770829 0.677775 0.653131 0 28 24 24 30206 30206 0.740999 0.927717 0.182855 0.897037 0.084195 0 20 17 17 44602 44602 0.442468 0.146094 0.137102 0.901129 0.077351 1 12 15 12 20383 20383 0.874138 0.374438 0.863105 0.472107 0.653651 0 30 17 17 32433 32433 0.748574 0.937747 0.171156 0.927878 0.666952 0 30 12 12 from pydts.fitters import repetitive_fitters rep_dict , times_dict , counts_df = repetitive_fitters ( rep = 15 , n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , test_size = 0.25 , verbose = 0 , allow_fails = 20 , real_coef_dict = real_coef_dict , censoring_prob = .8 ) 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 14/35 [05:35<08:23, 23.96s/it] final: 15 from pydts.examples_utils.plots import plot_reps_coef_std new_res_dict = plot_reps_coef_std ( rep_dict , True ) from pydts.examples_utils.plots import plot_models_coefficients a = new_res_dict [ 'alpha' ] b = new_res_dict [ 'beta' ] times = train_df [ 'X' ] . sort_values () . unique () n_cov = 5 temp_c_df = counts_df . loc [[ 1 , 2 ]] . groupby ([ 'X' ]) . sum () . values . flatten () . astype ( int ) plot_models_coefficients ( a , b , times , temp_c_df ) from pydts.examples_utils.plots import plot_times plot_times ( times_dict ) <AxesSubplot:xlabel='Model type', ylabel='Fitting Time [seconds]'> Lee et al. 2018 [1] \u00a4 Lee et al. suggested to expand the data so that for each patient we have row for each \\(t \\in [1, ... , T_i]\\) , with binary event columns which are 1 only at the time of the event. here \\(j_0 = 1 - j_1 - j_2\\) . Then, for each event we estimate { \\(\\alpha_{jt}, \\beta_{j}\\) } using a binary regression model. from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Model summary for event: 1 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_1 No. Observations: 357418 Model: GLM Df Residuals: 357383 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -54259. Date: Sun, 03 Jul 2022 Deviance: 1.0852e+05 Time: 15:23:56 Pearson chi2: 3.58e+05 No. Iterations: 7 Pseudo R-squ. (CS): 0.01510 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -0.9516 0.039 -24.639 0.000 -1.027 -0.876 C(X)[2] -1.2097 0.041 -29.216 0.000 -1.291 -1.129 C(X)[3] -1.3471 0.044 -30.809 0.000 -1.433 -1.261 C(X)[4] -1.3832 0.046 -30.366 0.000 -1.472 -1.294 C(X)[5] -1.5047 0.048 -31.144 0.000 -1.599 -1.410 C(X)[6] -1.4645 0.050 -29.574 0.000 -1.562 -1.367 C(X)[7] -1.5566 0.052 -29.750 0.000 -1.659 -1.454 C(X)[8] -1.5641 0.054 -28.857 0.000 -1.670 -1.458 C(X)[9] -1.7018 0.058 -29.119 0.000 -1.816 -1.587 C(X)[10] -1.6294 0.059 -27.517 0.000 -1.745 -1.513 C(X)[11] -1.6601 0.062 -26.763 0.000 -1.782 -1.539 C(X)[12] -1.6994 0.065 -26.040 0.000 -1.827 -1.571 C(X)[13] -1.7980 0.070 -25.626 0.000 -1.936 -1.661 C(X)[14] -1.7798 0.073 -24.514 0.000 -1.922 -1.638 C(X)[15] -1.8642 0.078 -23.809 0.000 -2.018 -1.711 C(X)[16] -1.7972 0.080 -22.479 0.000 -1.954 -1.641 C(X)[17] -1.7883 0.084 -21.385 0.000 -1.952 -1.624 C(X)[18] -1.9692 0.095 -20.833 0.000 -2.154 -1.784 C(X)[19] -1.8447 0.095 -19.479 0.000 -2.030 -1.659 C(X)[20] -1.9415 0.104 -18.631 0.000 -2.146 -1.737 C(X)[21] -2.0724 0.117 -17.710 0.000 -2.302 -1.843 C(X)[22] -1.9505 0.118 -16.555 0.000 -2.181 -1.720 C(X)[23] -1.8161 0.119 -15.214 0.000 -2.050 -1.582 C(X)[24] -2.0534 0.144 -14.272 0.000 -2.335 -1.771 C(X)[25] -2.0323 0.155 -13.081 0.000 -2.337 -1.728 C(X)[26] -2.0328 0.173 -11.771 0.000 -2.371 -1.694 C(X)[27] -1.9000 0.183 -10.385 0.000 -2.259 -1.541 C(X)[28] -2.0244 0.229 -8.832 0.000 -2.474 -1.575 C(X)[29] -2.2560 0.321 -7.023 0.000 -2.886 -1.626 C(X)[30] -2.0416 0.414 -4.927 0.000 -2.854 -1.229 Z1 0.1805 0.031 5.865 0.000 0.120 0.241 Z2 -1.1065 0.031 -35.243 0.000 -1.168 -1.045 Z3 -1.1032 0.032 -34.901 0.000 -1.165 -1.041 Z4 -0.8931 0.031 -28.536 0.000 -0.954 -0.832 Z5 -0.6629 0.031 -21.306 0.000 -0.724 -0.602 ============================================================================== Model summary for event: 2 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_2 No. Observations: 357418 Model: GLM Df Residuals: 357383 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -28316. Date: Sun, 03 Jul 2022 Deviance: 56631. Time: 15:23:56 Pearson chi2: 3.59e+05 No. Iterations: 8 Pseudo R-squ. (CS): 0.006804 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -1.7249 0.058 -29.825 0.000 -1.838 -1.612 C(X)[2] -1.9861 0.063 -31.491 0.000 -2.110 -1.862 C(X)[3] -1.8825 0.064 -29.415 0.000 -2.008 -1.757 C(X)[4] -1.9856 0.068 -29.285 0.000 -2.118 -1.853 C(X)[5] -1.9548 0.070 -28.038 0.000 -2.091 -1.818 C(X)[6] -2.0710 0.074 -27.870 0.000 -2.217 -1.925 C(X)[7] -2.0900 0.077 -27.041 0.000 -2.241 -1.939 C(X)[8] -2.0176 0.078 -25.728 0.000 -2.171 -1.864 C(X)[9] -2.1402 0.084 -25.383 0.000 -2.305 -1.975 C(X)[10] -2.0986 0.086 -24.341 0.000 -2.268 -1.930 C(X)[11] -2.1868 0.092 -23.735 0.000 -2.367 -2.006 C(X)[12] -2.1230 0.094 -22.634 0.000 -2.307 -1.939 C(X)[13] -2.1778 0.099 -21.913 0.000 -2.373 -1.983 C(X)[14] -2.1732 0.103 -21.039 0.000 -2.376 -1.971 C(X)[15] -2.1550 0.107 -20.087 0.000 -2.365 -1.945 C(X)[16] -2.2824 0.118 -19.375 0.000 -2.513 -2.052 C(X)[17] -2.2170 0.121 -18.375 0.000 -2.453 -1.981 C(X)[18] -2.4341 0.139 -17.550 0.000 -2.706 -2.162 C(X)[19] -2.1037 0.128 -16.470 0.000 -2.354 -1.853 C(X)[20] -2.2105 0.141 -15.677 0.000 -2.487 -1.934 C(X)[21] -2.4562 0.166 -14.765 0.000 -2.782 -2.130 C(X)[22] -2.2282 0.160 -13.940 0.000 -2.541 -1.915 C(X)[23] -2.4139 0.186 -12.968 0.000 -2.779 -2.049 C(X)[24] -2.4001 0.201 -11.917 0.000 -2.795 -2.005 C(X)[25] -2.4684 0.226 -10.905 0.000 -2.912 -2.025 C(X)[26] -2.1506 0.217 -9.894 0.000 -2.577 -1.725 C(X)[27] -2.0406 0.232 -8.784 0.000 -2.496 -1.585 C(X)[28] -2.2066 0.296 -7.454 0.000 -2.787 -1.626 C(X)[29] -2.1456 0.361 -5.951 0.000 -2.852 -1.439 C(X)[30] -1.8980 0.455 -4.172 0.000 -2.790 -1.006 Z1 0.0433 0.046 0.940 0.347 -0.047 0.133 Z2 -1.0656 0.047 -22.688 0.000 -1.158 -0.974 Z3 -1.3990 0.048 -29.224 0.000 -1.493 -1.305 Z4 -1.1034 0.047 -23.399 0.000 -1.196 -1.011 Z5 -0.7174 0.047 -15.386 0.000 -0.809 -0.626 ============================================================================== from pydts.examples_utils.plots import plot_first_model_coefs plot_first_model_coefs ( models = fitter . event_models , times = fitter . times , train_df = patients_df , n_cov = 5 ) New approach \u00a4 Here, for each event, we first estimate \\(\\beta_{j}\\) using a time-stratified CoxPH model and the expanded data as we used in the previous approach. Afterwards, we evaluate \\(\\alpha_{jt}\\) based on the original training data. from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 )) new_fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.175630 0.029922 0.042164 0.045464 Z2 -1.077021 0.030535 -1.053325 0.046400 Z3 -1.072990 0.030744 -1.383865 0.047306 Z4 -0.868595 0.030434 -1.090288 0.046584 Z5 -0.643514 0.030250 -0.709322 0.046064 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 2560 True -0.993701 2 1713 True -1.252696 3 1321 True -1.390031 4 1123 True -1.426362 5 888 True -1.545886 6 823 True -1.506916 7 675 True -1.602185 8 603 True -1.610078 9 473 True -1.746932 10 458 True -1.675599 11 400 True -1.706351 12 346 True -1.745275 13 283 True -1.841762 14 259 True -1.824197 15 213 True -1.905305 16 203 True -1.841176 17 182 True -1.832710 18 135 True -2.017525 19 135 True -1.886645 20 108 True -1.990557 21 83 True -2.119080 22 82 True -1.999757 23 80 True -1.859465 24 53 True -2.100951 25 45 True -2.080507 26 36 True -2.081074 27 32 True -1.950757 28 20 True -2.072971 29 10 True -2.292853 30 6 True -2.089735 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 942 True -1.741469 2 617 True -2.004244 3 596 True -1.891698 4 472 True -2.003929 5 430 True -1.973737 6 341 True -2.086884 7 299 True -2.104953 8 288 True -2.034741 9 228 True -2.150781 10 214 True -2.110933 11 176 True -2.191509 12 168 True -2.131838 13 143 True -2.180693 14 129 True -2.175601 15 117 True -2.157802 16 92 True -2.267452 17 87 True -2.209869 18 62 True -2.440465 19 76 True -2.107629 20 60 True -2.200231 21 41 True -2.457340 22 45 True -2.212729 23 32 True -2.418286 24 27 True -2.404584 25 21 True -2.465084 26 23 True -2.142334 27 20 True -2.045518 28 12 True -2.190062 29 8 True -2.137646 30 5 True -1.913849 from pydts.examples_utils.plots import plot_second_model_coefs plot_second_model_coefs ( new_fitter . alpha_df , new_fitter . beta_models , times , n_cov = 5 ) new_fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.175630 0.029922 0.042164 0.045464 Z2 -1.077021 0.030535 -1.053325 0.046400 Z3 -1.072990 0.030744 -1.383865 0.047306 Z4 -0.868595 0.030434 -1.090288 0.046584 Z5 -0.643514 0.030250 -0.709322 0.046064 new_fitter . plot_all_events_beta () <AxesSubplot:title={'center':'$\\\\beta_{j}$ for all events'}, xlabel='Value', ylabel='$\\\\beta_{j}$'> Prediction \u00a4 # pred_df = new_fitter.predict_cumulative_incident_function(test_df) # pred_df = new_fitter.predict_marginal_prob_all_events(pred_df) # pred_df.set_index(['pid']).head().T pred_df = new_fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid 0 1 2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.940455 0.958727 0.930894 overall_survival_t2 0.896901 0.928018 0.880776 overall_survival_t3 0.858059 0.900500 0.836577 overall_survival_t4 0.822953 0.875209 0.796861 overall_survival_t5 0.791664 0.852514 0.761776 overall_survival_t6 0.761745 0.830406 0.728355 overall_survival_t7 0.734941 0.810424 0.698630 overall_survival_t8 0.708674 0.790680 0.669710 overall_survival_t9 0.686386 0.773755 0.645310 overall_survival_t10 0.663455 0.756133 0.620329 overall_survival_t11 0.642267 0.739641 0.597350 overall_survival_t12 0.621925 0.723707 0.575441 overall_survival_t13 0.603735 0.709345 0.555961 overall_survival_t14 0.585841 0.695072 0.536886 overall_survival_t15 0.569274 0.681787 0.519339 overall_survival_t16 0.553066 0.668569 0.502199 overall_survival_t17 0.536944 0.655324 0.485244 overall_survival_t18 0.524081 0.644643 0.471762 overall_survival_t19 0.508820 0.631972 0.455896 overall_survival_t20 0.495393 0.620725 0.441999 overall_survival_t21 0.484366 0.611361 0.430605 overall_survival_t22 0.471714 0.600590 0.417611 overall_survival_t23 0.459024 0.589532 0.404558 overall_survival_t24 0.448504 0.580388 0.393826 overall_survival_t25 0.438286 0.571408 0.383426 overall_survival_t26 0.427112 0.561650 0.372165 overall_survival_t27 0.414887 0.550861 0.359891 overall_survival_t28 0.404448 0.541554 0.349448 overall_survival_t29 0.395303 0.533440 0.340383 overall_survival_t30 0.384281 0.523584 0.329504 hazard_j1_t1 0.044777 0.032540 0.052977 hazard_j1_t10 0.023154 0.016723 0.027508 hazard_j1_t11 0.022469 0.016225 0.026698 hazard_j1_t12 0.021630 0.015615 0.025705 hazard_j1_t13 0.019679 0.014199 0.023396 hazard_j1_t14 0.020021 0.014447 0.023800 hazard_j1_t15 0.018490 0.013337 0.021987 hazard_j1_t16 0.019691 0.014208 0.023409 hazard_j1_t17 0.019855 0.014327 0.023603 hazard_j1_t18 0.016560 0.011938 0.019699 hazard_j1_t19 0.018832 0.013585 0.022392 hazard_j1_t2 0.034917 0.025303 0.041389 hazard_j1_t20 0.017005 0.012260 0.020227 hazard_j1_t21 0.014985 0.010798 0.017831 hazard_j1_t22 0.016852 0.012149 0.020045 hazard_j1_t23 0.019341 0.013954 0.022995 hazard_j1_t24 0.015255 0.010993 0.018151 hazard_j1_t25 0.015565 0.011217 0.018519 hazard_j1_t26 0.015556 0.011211 0.018509 hazard_j1_t27 0.017683 0.012752 0.021031 hazard_j1_t28 0.015681 0.011301 0.018656 hazard_j1_t29 0.012625 0.009091 0.015029 hazard_j1_t3 0.030573 0.022128 0.036271 hazard_j1_t30 0.015424 0.011116 0.018352 hazard_j1_t4 0.029514 0.021355 0.035022 hazard_j1_t5 0.026277 0.018995 0.031200 hazard_j1_t6 0.027292 0.019735 0.032399 hazard_j1_t7 0.024874 0.017974 0.029542 hazard_j1_t8 0.024683 0.017835 0.029317 hazard_j1_t9 0.021594 0.015590 0.025663 hazard_j2_t1 0.014768 0.008733 0.016130 hazard_j2_t10 0.010253 0.006052 0.011203 hazard_j2_t11 0.009467 0.005586 0.010345 hazard_j2_t12 0.010043 0.005927 0.010974 hazard_j2_t13 0.009569 0.005646 0.010456 hazard_j2_t14 0.009617 0.005675 0.010509 hazard_j2_t15 0.009788 0.005776 0.010696 hazard_j2_t16 0.008781 0.005179 0.009596 hazard_j2_t17 0.009296 0.005485 0.010158 hazard_j2_t18 0.007396 0.004360 0.008083 hazard_j2_t19 0.010287 0.006071 0.011240 hazard_j2_t2 0.011395 0.006728 0.012449 hazard_j2_t20 0.009386 0.005537 0.010256 hazard_j2_t21 0.007273 0.004287 0.007949 hazard_j2_t22 0.009270 0.005469 0.010130 hazard_j2_t23 0.007561 0.004457 0.008263 hazard_j2_t24 0.007664 0.004519 0.008376 hazard_j2_t25 0.007218 0.004254 0.007888 hazard_j2_t26 0.009939 0.005866 0.010861 hazard_j2_t27 0.010939 0.006458 0.011951 hazard_j2_t28 0.009481 0.005594 0.010360 hazard_j2_t29 0.009986 0.005893 0.010911 hazard_j2_t3 0.012735 0.007524 0.013911 hazard_j2_t30 0.012459 0.007360 0.013610 hazard_j2_t4 0.011398 0.006730 0.012453 hazard_j2_t5 0.011743 0.006935 0.012829 hazard_j2_t6 0.010500 0.006198 0.011473 hazard_j2_t7 0.010314 0.006088 0.011270 hazard_j2_t8 0.011056 0.006528 0.012079 hazard_j2_t9 0.009857 0.005817 0.010770 prob_j1_at_t1 0.044777 0.032540 0.052977 prob_j1_at_t2 0.032838 0.024259 0.038529 prob_j1_at_t3 0.027421 0.020535 0.031946 prob_j1_at_t4 0.025325 0.019231 0.029299 prob_j1_at_t5 0.021625 0.016625 0.024862 prob_j1_at_t6 0.021606 0.016825 0.024681 prob_j1_at_t7 0.018948 0.014926 0.021517 prob_j1_at_t8 0.018141 0.014454 0.020481 prob_j1_at_t9 0.015303 0.012326 0.017187 prob_j1_at_t10 0.015893 0.012940 0.017751 prob_j1_at_t11 0.014907 0.012268 0.016561 prob_j1_at_t12 0.013892 0.011550 0.015355 prob_j1_at_t13 0.012239 0.010276 0.013463 prob_j1_at_t14 0.012087 0.010248 0.013232 prob_j1_at_t15 0.010832 0.009270 0.011804 prob_j1_at_t16 0.011209 0.009687 0.012157 prob_j1_at_t17 0.010981 0.009578 0.011854 prob_j1_at_t18 0.008892 0.007823 0.009559 prob_j1_at_t19 0.009869 0.008757 0.010564 prob_j1_at_t20 0.008652 0.007748 0.009221 prob_j1_at_t21 0.007423 0.006702 0.007881 prob_j1_at_t22 0.008162 0.007428 0.008631 prob_j1_at_t23 0.009123 0.008380 0.009603 prob_j1_at_t24 0.007002 0.006481 0.007343 prob_j1_at_t25 0.006981 0.006510 0.007293 prob_j1_at_t26 0.006818 0.006406 0.007097 prob_j1_at_t27 0.007553 0.007162 0.007827 prob_j1_at_t28 0.006506 0.006225 0.006714 prob_j1_at_t29 0.005106 0.004923 0.005252 prob_j1_at_t30 0.006097 0.005929 0.006247 prob_j2_at_t1 0.014768 0.008733 0.016130 prob_j2_at_t2 0.010716 0.006451 0.011588 prob_j2_at_t3 0.011422 0.006982 0.012253 prob_j2_at_t4 0.009780 0.006061 0.010418 prob_j2_at_t5 0.009664 0.006070 0.010223 prob_j2_at_t6 0.008313 0.005284 0.008740 prob_j2_at_t7 0.007857 0.005055 0.008208 prob_j2_at_t8 0.008126 0.005290 0.008439 prob_j2_at_t9 0.006985 0.004599 0.007213 prob_j2_at_t10 0.007038 0.004682 0.007229 prob_j2_at_t11 0.006281 0.004224 0.006417 prob_j2_at_t12 0.006450 0.004384 0.006555 prob_j2_at_t13 0.005951 0.004086 0.006017 prob_j2_at_t14 0.005806 0.004025 0.005843 prob_j2_at_t15 0.005734 0.004015 0.005742 prob_j2_at_t16 0.004999 0.003531 0.004983 prob_j2_at_t17 0.005142 0.003667 0.005102 prob_j2_at_t18 0.003971 0.002857 0.003922 prob_j2_at_t19 0.005391 0.003914 0.005302 prob_j2_at_t20 0.004776 0.003500 0.004676 prob_j2_at_t21 0.003603 0.002661 0.003514 prob_j2_at_t22 0.004490 0.003344 0.004362 prob_j2_at_t23 0.003566 0.002677 0.003451 prob_j2_at_t24 0.003518 0.002664 0.003389 prob_j2_at_t25 0.003237 0.002469 0.003107 prob_j2_at_t26 0.004356 0.003352 0.004164 prob_j2_at_t27 0.004672 0.003627 0.004448 prob_j2_at_t28 0.003933 0.003081 0.003728 prob_j2_at_t29 0.004039 0.003191 0.003813 prob_j2_at_t30 0.004925 0.003926 0.004633 cif_j1_at_t1 0.044777 0.032540 0.052977 cif_j1_at_t2 0.077614 0.056799 0.091506 cif_j1_at_t3 0.105035 0.077334 0.123452 cif_j1_at_t4 0.130360 0.096565 0.152751 cif_j1_at_t5 0.151985 0.113190 0.177613 cif_j1_at_t6 0.173591 0.130014 0.202294 cif_j1_at_t7 0.192539 0.144940 0.223811 cif_j1_at_t8 0.210680 0.159394 0.244292 cif_j1_at_t9 0.225983 0.171721 0.261479 cif_j1_at_t10 0.241876 0.184661 0.279231 cif_j1_at_t11 0.256783 0.196929 0.295792 cif_j1_at_t12 0.270675 0.208478 0.311147 cif_j1_at_t13 0.282914 0.218755 0.324609 cif_j1_at_t14 0.295001 0.229003 0.337841 cif_j1_at_t15 0.305834 0.238273 0.349646 cif_j1_at_t16 0.317043 0.247959 0.361803 cif_j1_at_t17 0.328024 0.257537 0.373657 cif_j1_at_t18 0.336915 0.265361 0.383216 cif_j1_at_t19 0.346785 0.274118 0.393779 cif_j1_at_t20 0.355437 0.281866 0.403000 cif_j1_at_t21 0.362860 0.288568 0.410882 cif_j1_at_t22 0.371023 0.295996 0.419513 cif_j1_at_t23 0.380146 0.304376 0.429116 cif_j1_at_t24 0.387148 0.310857 0.436459 cif_j1_at_t25 0.394129 0.317368 0.443752 cif_j1_at_t26 0.400947 0.323774 0.450849 cif_j1_at_t27 0.408500 0.330936 0.458676 cif_j1_at_t28 0.415005 0.337161 0.465390 cif_j1_at_t29 0.420111 0.342084 0.470642 cif_j1_at_t30 0.426208 0.348014 0.476888 cif_j2_at_t1 0.014768 0.008733 0.016130 cif_j2_at_t2 0.025484 0.015183 0.027718 cif_j2_at_t3 0.036906 0.022166 0.039971 cif_j2_at_t4 0.046686 0.028226 0.050388 cif_j2_at_t5 0.056351 0.034296 0.060611 cif_j2_at_t6 0.064663 0.039580 0.069351 cif_j2_at_t7 0.072520 0.044635 0.077559 cif_j2_at_t8 0.080646 0.049925 0.085998 cif_j2_at_t9 0.087631 0.054524 0.093211 cif_j2_at_t10 0.094669 0.059207 0.100440 cif_j2_at_t11 0.100950 0.063430 0.106858 cif_j2_at_t12 0.107400 0.067814 0.113413 cif_j2_at_t13 0.113351 0.071900 0.119430 cif_j2_at_t14 0.119158 0.075926 0.125272 cif_j2_at_t15 0.124892 0.079941 0.131015 cif_j2_at_t16 0.129891 0.083472 0.135998 cif_j2_at_t17 0.135032 0.087139 0.141100 cif_j2_at_t18 0.139004 0.089996 0.145022 cif_j2_at_t19 0.144395 0.093910 0.150324 cif_j2_at_t20 0.149170 0.097409 0.155000 cif_j2_at_t21 0.152773 0.100071 0.158514 cif_j2_at_t22 0.157264 0.103414 0.162875 cif_j2_at_t23 0.160830 0.106091 0.166326 cif_j2_at_t24 0.164348 0.108755 0.169715 cif_j2_at_t25 0.167585 0.111224 0.172822 cif_j2_at_t26 0.171941 0.114576 0.176986 cif_j2_at_t27 0.176614 0.118203 0.181434 cif_j2_at_t28 0.180547 0.121285 0.185162 cif_j2_at_t29 0.184586 0.124476 0.188975 cif_j2_at_t30 0.189511 0.128402 0.193608 Comparison (WIP) \u00a4 pd . DataFrame ( index = pd . MultiIndex . from_product ([[ 'd=30' , 'd=60' , 'd=100' , 'd=150' ], [ 'N=1000' , 'N=10000' , 'N=100000' ]], names = [ 'N' , 'D' ]), columns = [ 'Train MSE (oracle)' , 'Test MSE (oracle)' , 'computation time' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Train MSE (oracle) Test MSE (oracle) computation time N D d=30 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=60 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=100 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=150 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN from pydts.examples_utils.plots import compare_beta_models_for_example from pydts.utils import present_coefs res_dict = compare_beta_models_for_example ( fitter . event_models , new_fitter . event_models , real_coef_dict = real_coef_dict ) present_coefs ( res_dict ) for coef: Alpha .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lee Ours real a1_1 -0.951613 -0.993701 -1.000000 a2_1 -1.209701 -1.252696 -1.207944 a3_1 -1.347113 -1.390031 -1.329584 a4_1 -1.383203 -1.426362 -1.415888 a5_1 -1.504718 -1.545886 -1.482831 a6_1 -1.464485 -1.506916 -1.537528 a7_1 -1.556570 -1.602185 -1.583773 a8_1 -1.564112 -1.610078 -1.623832 a9_1 -1.701809 -1.746932 -1.659167 a10_1 -1.629386 -1.675599 -1.690776 a11_1 -1.660134 -1.706351 -1.719369 a12_1 -1.699373 -1.745275 -1.745472 a13_1 -1.798048 -1.841762 -1.769485 a14_1 -1.779825 -1.824197 -1.791717 a15_1 -1.864221 -1.905305 -1.812415 a16_1 -1.797205 -1.841176 -1.831777 a17_1 -1.788325 -1.832710 -1.849964 a18_1 -1.969211 -2.017525 -1.867112 a19_1 -1.844675 -1.886645 -1.883332 a20_1 -1.941487 -1.990557 -1.898720 a21_1 -2.072392 -2.119080 -1.913357 a22_1 -1.950505 -1.999757 -1.927313 a23_1 -1.816114 -1.859465 -1.940648 a24_1 -2.053391 -2.100951 -1.953416 a25_1 -2.032329 -2.080507 -1.965663 a26_1 -2.032766 -2.081074 -1.977429 a27_1 -1.899979 -1.950757 -1.988751 a28_1 -2.024438 -2.072971 -1.999661 a29_1 -2.256035 -2.292853 -2.010189 a30_1 -2.041636 -2.089735 -2.020359 a1_2 -1.724923 -1.741469 -1.750000 a2_2 -1.986075 -2.004244 -1.853972 a3_2 -1.882549 -1.891698 -1.914792 a4_2 -1.985604 -2.003929 -1.957944 a5_2 -1.954779 -1.973737 -1.991416 a6_2 -2.071016 -2.086884 -2.018764 a7_2 -2.089997 -2.104953 -2.041887 a8_2 -2.017632 -2.034741 -2.061916 a9_2 -2.140155 -2.150781 -2.079584 a10_2 -2.098609 -2.110933 -2.095388 a11_2 -2.186827 -2.191509 -2.109684 a12_2 -2.123014 -2.131838 -2.122736 a13_2 -2.177759 -2.180693 -2.134742 a14_2 -2.173186 -2.175601 -2.145859 a15_2 -2.154986 -2.157802 -2.156208 a16_2 -2.282404 -2.267452 -2.165888 a17_2 -2.216986 -2.209869 -2.174982 a18_2 -2.434101 -2.440465 -2.183556 a19_2 -2.103665 -2.107629 -2.191666 a20_2 -2.210458 -2.200231 -2.199360 a21_2 -2.456157 -2.457340 -2.206678 a22_2 -2.228180 -2.212729 -2.213656 a23_2 -2.413939 -2.418286 -2.220324 a24_2 -2.400092 -2.404584 -2.226708 a25_2 -2.468446 -2.465084 -2.232831 a26_2 -2.150649 -2.142334 -2.238714 a27_2 -2.040635 -2.045518 -2.244376 a28_2 -2.206609 -2.190062 -2.249831 a29_2 -2.145625 -2.137646 -2.255094 a30_2 -1.898031 -1.913849 -2.260180 for coef: Beta .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lee Ours real Z1_1 0.180473 0.175630 0.223144 Z2_1 -1.106470 -1.077021 -1.098612 Z3_1 -1.103171 -1.072990 -1.098612 Z4_1 -0.893110 -0.868595 -0.916291 Z5_1 -0.662878 -0.643514 -0.693147 Z1_2 0.043267 0.042164 -0.000000 Z2_2 -1.065630 -1.053325 -1.098612 Z3_2 -1.398985 -1.383865 -1.386294 Z4_2 -1.103417 -1.090288 -1.098612 Z5_2 -0.717390 -0.709322 -0.693147 from pydts.utils import create_df_for_cif_plots df_for_plotting = create_df_for_cif_plots ( test_df , field = \"Z1\" , covariates = new_fitter . covariates , quantiles = [ 0.25 , 0.5 , 0.75 ], zero_others = True ) our_pred_df = new_fitter . predict_cumulative_incident_function ( df_for_plotting ) our_pred_df = new_fitter . predict_marginal_prob_all_events ( our_pred_df ) our_pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 overall_survival_t1 overall_survival_t2 overall_survival_t3 overall_survival_t4 overall_survival_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.570378 0.370758 0.245091 0.166491 0.115641 ... 0.362827 0.362884 0.362928 0.362975 0.363015 0.363042 0.363063 0.363085 0.636792 0.363085 4839 0.506276 0.0 0.0 0.0 0.0 0.560076 0.359013 0.234264 0.157197 0.107959 ... 0.356779 0.356825 0.356860 0.356897 0.356929 0.356950 0.356966 0.356983 0.642924 0.356983 4839 0.750906 0.0 0.0 0.0 0.0 0.549852 0.347522 0.223807 0.148330 0.100713 ... 0.351012 0.351049 0.351077 0.351107 0.351131 0.351148 0.351161 0.351173 0.648756 0.351173 3 rows \u00d7 217 columns lee_pred_df = fitter . predict_cumulative_incident_function ( df_for_plotting ) lee_pred_df = fitter . predict_marginal_prob_all_events ( lee_pred_df ) lee_pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 overall_survival_t1 overall_survival_t2 overall_survival_t3 overall_survival_t4 overall_survival_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.559414 0.358099 0.233707 0.156609 0.107446 ... 0.357832 0.357878 0.357912 0.357949 0.357981 0.358001 0.358018 0.358034 0.641872 0.358034 4839 0.506276 0.0 0.0 0.0 0.0 0.548640 0.346007 0.222712 0.147300 0.099849 ... 0.351764 0.351800 0.351827 0.351856 0.351881 0.351896 0.351909 0.351922 0.648008 0.351922 4839 0.750906 0.0 0.0 0.0 0.0 0.537951 0.334190 0.212116 0.138445 0.092709 ... 0.345988 0.346017 0.346038 0.346061 0.346080 0.346092 0.346101 0.346111 0.653837 0.346111 3 rows \u00d7 217 columns from pydts.utils import get_real_hazard df_temp = get_real_hazard ( df_for_plotting . copy (), real_coef_dict = real_coef_dict , times = times , events = [ 1 , 2 ]) df_temp .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 hazard_j1_t1 hazard_j1_t2 hazard_j1_t3 hazard_j1_t4 hazard_j1_t5 ... hazard_j2_t21 hazard_j2_t22 hazard_j2_t23 hazard_j2_t24 hazard_j2_t25 hazard_j2_t26 hazard_j2_t27 hazard_j2_t28 hazard_j2_t29 hazard_j2_t30 4839 0.255555 0.0 0.0 0.0 0.0 0.280300 0.240321 0.218820 0.204425 0.193753 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 4839 0.506276 0.0 0.0 0.0 0.0 0.291724 0.250683 0.228533 0.213675 0.202643 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 4839 0.750906 0.0 0.0 0.0 0.0 0.303129 0.261076 0.238300 0.222990 0.211606 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 3 rows \u00d7 65 columns real_pred_df = new_fitter . predict_cumulative_incident_function ( df_temp ) real_pred_df = new_fitter . predict_marginal_prob_all_events ( real_pred_df ) real_pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 hazard_j1_t1 hazard_j1_t2 hazard_j1_t3 hazard_j1_t4 hazard_j1_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.280300 0.240321 0.218820 0.204425 0.193753 ... 0.361174 0.361223 0.361260 0.361289 0.361311 0.361329 0.361342 0.361352 0.638562 0.361352 4839 0.506276 0.0 0.0 0.0 0.0 0.291724 0.250683 0.228533 0.213675 0.202643 ... 0.351030 0.351068 0.351096 0.351118 0.351135 0.351148 0.351158 0.351166 0.648772 0.351166 4839 0.750906 0.0 0.0 0.0 0.0 0.303129 0.261076 0.238300 0.222990 0.211606 ... 0.341411 0.341440 0.341462 0.341479 0.341491 0.341501 0.341508 0.341514 0.658441 0.341514 3 rows \u00d7 217 columns import matplotlib.pyplot as plt from pydts.examples_utils.plots import plot_cif_plots j_events = 2 preds = [ lee_pred_df , our_pred_df , real_pred_df ] names = [ 'Lee' , 'Our' , 'Real' ] fig = plt . figure ( constrained_layout = True , figsize = ( 20 , 20 )) subfigs = fig . subfigures ( nrows = len ( preds ), ncols = 1 ) for row , subfig in enumerate ( subfigs ): subfig . suptitle ( f 'CIF for { names [ row ] } method' , fontsize = 30 , fontweight = 'bold' ) pred_df = preds [ row ] # create 1x3 subplots per subfig axs = subfig . subplots ( nrows = 1 , ncols = j_events ) for col , ax in enumerate ( axs ): ax = plot_cif_plots ( pred_df , event = col + 1 , return_ax = True , ax = ax , pad = 0.05 , scale = 5 ) h , l = ax . get_legend_handles_labels () ax . legend ( h ,[ \"Z5 - Q25\" , \"Z5 - Q50\" , \"Z5 - Q75\" ], fontsize = 15 ) from lifelines.fitters.coxph_fitter import CoxPHFitter events = pd . get_dummies ( train_df [ 'J' ]) . add_prefix ( \"J_\" ) rel_train = train_df . drop ( columns = [ 'C' , 'T' , 'J' ]) . merge ( events , how = 'inner' , left_index = True , right_index = True ) . copy () fit_cols = [ * covariates , \"X\" ] models = {} for event in [ 1 , 2 ]: model = CoxPHFitter () event_col = f \"J_ { event } \" print ( event_col ) model . fit_right_censoring ( df = rel_train [[ * fit_cols , event_col ]] , duration_col = 'X' , event_col = event_col ) model . print_summary () models . update ({ event : model }) rel_times = models [ 1 ] . timeline . union ( models [ 2 ] . timeline ) J_1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'J_1' baseline estimation breslow number of observations 37500 number of events observed 13348 partial log-likelihood -130916.90 time fit was run 2022-07-03 12:24:34 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.18 1.19 0.03 0.12 0.23 1.12 1.26 5.87 <0.005 27.77 Z2 -1.08 0.34 0.03 -1.14 -1.02 0.32 0.36 -35.27 <0.005 902.87 Z3 -1.07 0.34 0.03 -1.13 -1.01 0.32 0.36 -34.90 <0.005 884.08 Z4 -0.87 0.42 0.03 -0.93 -0.81 0.40 0.45 -28.54 <0.005 592.75 Z5 -0.64 0.53 0.03 -0.70 -0.58 0.50 0.56 -21.27 <0.005 331.19 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.65 Partial AIC 261843.81 log-likelihood ratio test 3537.28 on 5 df -log2(p) of ll-ratio test inf J_2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'J_2' baseline estimation breslow number of observations 37500 number of events observed 5771 partial log-likelihood -56065.63 time fit was run 2022-07-03 12:24:34 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.04 1.04 0.05 -0.05 0.13 0.95 1.14 0.93 0.35 1.50 Z2 -1.05 0.35 0.05 -1.14 -0.96 0.32 0.38 -22.70 <0.005 376.56 Z3 -1.38 0.25 0.05 -1.48 -1.29 0.23 0.27 -29.25 <0.005 622.51 Z4 -1.09 0.34 0.05 -1.18 -1.00 0.31 0.37 -23.40 <0.005 400.02 Z5 -0.71 0.49 0.05 -0.80 -0.62 0.45 0.54 -15.40 <0.005 175.32 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.67 Partial AIC 112141.27 log-likelihood ratio test 2026.96 on 5 df -log2(p) of ll-ratio test inf from pydts.examples_utils.continues_data import hazard_func , survival_func hazards = hazard_func ( models , df_for_plotting ) surv_func = survival_func ( models , rel_times , df_for_plotting ) cif_1 = pd . DataFrame ( hazards [ 1 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cif_2 = pd . DataFrame ( hazards [ 2 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cont_cif = pd . merge ( cif_1 . reset_index ( drop = True ) . add_prefix ( \"cif_j1_at_t\" ), cif_2 . reset_index ( drop = True ) . add_prefix ( \"cif_j2_at_t\" ), right_index = True , left_index = True ) cont_cif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cif_j1_at_t1 cif_j1_at_t2 cif_j1_at_t3 cif_j1_at_t4 cif_j1_at_t5 cif_j1_at_t6 cif_j1_at_t7 cif_j1_at_t8 cif_j1_at_t9 cif_j1_at_t10 ... cif_j2_at_t21 cif_j2_at_t22 cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 0 0.056824 0.098869 0.133680 0.165468 0.192327 0.218796 0.241867 0.263644 0.281896 0.300560 ... 0.182505 0.187035 0.190665 0.194228 0.197451 0.201706 0.206258 0.209989 0.213826 0.218531 1 0.059204 0.102907 0.139013 0.171917 0.199668 0.226965 0.250715 0.273095 0.291823 0.310943 ... 0.181421 0.185837 0.189371 0.192836 0.195967 0.200095 0.204506 0.208117 0.211828 0.216372 2 0.061615 0.106988 0.144394 0.178411 0.207048 0.235161 0.259579 0.282547 0.301737 0.321297 ... 0.180266 0.184567 0.188005 0.191371 0.194410 0.198412 0.202682 0.206174 0.209759 0.214143 3 rows \u00d7 60 columns import matplotlib.pyplot as plt from pydts.examples_utils.plots import plot_cif_plots j_events = 2 preds = [ lee_pred_df , our_pred_df , real_pred_df , cont_cif ] names = [ 'Lee' , 'Our' , 'Real' , \"Continues\" ] fig = plt . figure ( constrained_layout = True , figsize = ( 20 , 20 )) subfigs = fig . subfigures ( nrows = len ( preds ), ncols = 1 ) for row , subfig in enumerate ( subfigs ): subfig . suptitle ( f 'CIF for { names [ row ] } method' , fontsize = 30 , fontweight = 'bold' ) pred_df = preds [ row ] # create 1x3 subplots per subfig axs = subfig . subplots ( nrows = 1 , ncols = j_events ) for col , ax in enumerate ( axs ): ax = plot_cif_plots ( pred_df , event = col + 1 , return_ax = True , ax = ax , pad = 0.05 , scale = 5 ) h , l = ax . get_legend_handles_labels () ax . legend ( h ,[ \"Z1 - Q25\" , \"Z1 - Q50\" , \"Z1 - Q75\" ], fontsize = 15 ) ax . set_ylim ([ 0 , 0.7 ]) plt . savefig ( \"comparison1.jpg\" ) df_for_plotting = create_df_for_cif_plots ( test_df , field = \"Z2\" , covariates = new_fitter . covariates , quantiles = [ 0.25 , 0.5 , 0.75 ], zero_others = True ) our_pred_df = new_fitter . predict_cumulative_incident_function ( df_for_plotting ) our_pred_df = new_fitter . predict_marginal_prob_all_events ( our_pred_df ) lee_pred_df = fitter . predict_cumulative_incident_function ( df_for_plotting ) lee_pred_df = fitter . predict_marginal_prob_all_events ( lee_pred_df ) df_temp = get_real_hazard ( df_for_plotting . copy (), real_coef_dict = real_coef_dict , times = times , events = [ 1 , 2 ]) real_pred_df = new_fitter . predict_cumulative_incident_function ( df_temp ) real_pred_df = new_fitter . predict_marginal_prob_all_events ( real_pred_df ) hazards = hazard_func ( models , df_for_plotting ) surv_func = survival_func ( models , rel_times , df_for_plotting ) cif_1 = pd . DataFrame ( hazards [ 1 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cif_2 = pd . DataFrame ( hazards [ 2 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cont_cif = pd . merge ( cif_1 . reset_index ( drop = True ) . add_prefix ( \"cif_j1_at_t\" ), cif_2 . reset_index ( drop = True ) . add_prefix ( \"cif_j2_at_t\" ), right_index = True , left_index = True ) import matplotlib.pyplot as plt from pydts.examples_utils.plots import plot_cif_plots j_events = 2 preds = [ lee_pred_df , our_pred_df , real_pred_df , cont_cif ] names = [ 'Lee' , 'Our' , 'Real' , \"Continues\" ] fig = plt . figure ( constrained_layout = True , figsize = ( 20 , 20 )) subfigs = fig . subfigures ( nrows = len ( preds ), ncols = 1 ) for row , subfig in enumerate ( subfigs ): subfig . suptitle ( f 'CIF for { names [ row ] } method' , fontsize = 30 , fontweight = 'bold' ) pred_df = preds [ row ] # create 1x3 subplots per subfig axs = subfig . subplots ( nrows = 1 , ncols = j_events ) for col , ax in enumerate ( axs ): ax = plot_cif_plots ( pred_df , event = col + 1 , return_ax = True , ax = ax , pad = 0.05 , scale = 5 ) h , l = ax . get_legend_handles_labels () ax . legend ( h ,[ \"Z2 - Q25\" , \"Z2 - Q50\" , \"Z2 - Q75\" ], fontsize = 15 ) ax . set_ylim ([ 0 , 0.7 ]) plt . savefig ( \"comparison2.jpg\" )","title":"Simple Simulation"},{"location":"Simple%20Simulation/#simple-example","text":"","title":"Simple Example"},{"location":"Simple%20Simulation/#introduction","text":"In this example, we present an existing method [1] and our new method on a very simple simulated dataset. We compare both methods based on simplicity, goodness of fit and performance. [1] On the Analysis of Discrete Time Competing Risks Data, Biometrics, Lee et al. 2018","title":"Introduction"},{"location":"Simple%20Simulation/#data-generation","text":"We simulate N=50,000 patients, with 5 covariate \\(Z_p\\) ( \\(p \\in [1,...,5]\\) ) randomly sampled from \\(\\mathbb{R} [0,1]\\) . Our timeline is discrete, i.e \\(t \\in [1,...,d]\\) . Here, we choose \\(d=30\\) . The event type \\(J_i\\) is randomly sampled from \\(J_i \\sim Uniform[1,...,M]\\) , where \\(M\\) is the number of competing events. In this example \\(M=2\\) . For each of the samples, we sample it's time-to-event \\(T_i\\) from the hazard function \\(\\lambda_{j}(T|Z) = \\frac{exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+exp(\\alpha_{jt}+Z^{T}\\beta_{j})}\\) such that \\(\\alpha_{1t} = -1-0.3 log(t)\\) \\(\\beta_{1} = -log([0.8, 3, 3, 2.5, 2])\\) \\(\\alpha_{2t} = -1.75-0.15 log(t)\\) \\(\\beta_{2} = -log([1, 3, 4, 3, 2])\\) and randomly sample a censoring time \\(C_i\\) such that \\(C_i \\sim Uniform[1, ... , d]\\) . We then calculate \\(X_i = min(T_i, C_i)\\) and for cencored samples we set the event type to be \\(J=0\\) . Lastly, we split the data into two separate datasets for training (75%) and for testing (25%). import pandas as pd from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline import numpy as np real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , real_coef_dict = real_coef_dict ) covariates = [ f 'Z { i + 1 } ' for i in range ( n_cov )] train_df , test_df = train_test_split ( patients_df , test_size = 0.25 ) events = sorted ( train_df [ 'J' ] . unique ()) times = sorted ( train_df [ 'X' ] . unique ()) train_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 31111 31111 0.661973 0.211958 0.770829 0.677775 0.653131 0 28 24 24 30206 30206 0.740999 0.927717 0.182855 0.897037 0.084195 0 20 17 17 44602 44602 0.442468 0.146094 0.137102 0.901129 0.077351 1 12 15 12 20383 20383 0.874138 0.374438 0.863105 0.472107 0.653651 0 30 17 17 32433 32433 0.748574 0.937747 0.171156 0.927878 0.666952 0 30 12 12 from pydts.fitters import repetitive_fitters rep_dict , times_dict , counts_df = repetitive_fitters ( rep = 15 , n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , test_size = 0.25 , verbose = 0 , allow_fails = 20 , real_coef_dict = real_coef_dict , censoring_prob = .8 ) 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 14/35 [05:35<08:23, 23.96s/it] final: 15 from pydts.examples_utils.plots import plot_reps_coef_std new_res_dict = plot_reps_coef_std ( rep_dict , True ) from pydts.examples_utils.plots import plot_models_coefficients a = new_res_dict [ 'alpha' ] b = new_res_dict [ 'beta' ] times = train_df [ 'X' ] . sort_values () . unique () n_cov = 5 temp_c_df = counts_df . loc [[ 1 , 2 ]] . groupby ([ 'X' ]) . sum () . values . flatten () . astype ( int ) plot_models_coefficients ( a , b , times , temp_c_df ) from pydts.examples_utils.plots import plot_times plot_times ( times_dict ) <AxesSubplot:xlabel='Model type', ylabel='Fitting Time [seconds]'>","title":"Data Generation"},{"location":"Simple%20Simulation/#lee-et-al-2018-1","text":"Lee et al. suggested to expand the data so that for each patient we have row for each \\(t \\in [1, ... , T_i]\\) , with binary event columns which are 1 only at the time of the event. here \\(j_0 = 1 - j_1 - j_2\\) . Then, for each event we estimate { \\(\\alpha_{jt}, \\beta_{j}\\) } using a binary regression model. from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Model summary for event: 1 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_1 No. Observations: 357418 Model: GLM Df Residuals: 357383 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -54259. Date: Sun, 03 Jul 2022 Deviance: 1.0852e+05 Time: 15:23:56 Pearson chi2: 3.58e+05 No. Iterations: 7 Pseudo R-squ. (CS): 0.01510 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -0.9516 0.039 -24.639 0.000 -1.027 -0.876 C(X)[2] -1.2097 0.041 -29.216 0.000 -1.291 -1.129 C(X)[3] -1.3471 0.044 -30.809 0.000 -1.433 -1.261 C(X)[4] -1.3832 0.046 -30.366 0.000 -1.472 -1.294 C(X)[5] -1.5047 0.048 -31.144 0.000 -1.599 -1.410 C(X)[6] -1.4645 0.050 -29.574 0.000 -1.562 -1.367 C(X)[7] -1.5566 0.052 -29.750 0.000 -1.659 -1.454 C(X)[8] -1.5641 0.054 -28.857 0.000 -1.670 -1.458 C(X)[9] -1.7018 0.058 -29.119 0.000 -1.816 -1.587 C(X)[10] -1.6294 0.059 -27.517 0.000 -1.745 -1.513 C(X)[11] -1.6601 0.062 -26.763 0.000 -1.782 -1.539 C(X)[12] -1.6994 0.065 -26.040 0.000 -1.827 -1.571 C(X)[13] -1.7980 0.070 -25.626 0.000 -1.936 -1.661 C(X)[14] -1.7798 0.073 -24.514 0.000 -1.922 -1.638 C(X)[15] -1.8642 0.078 -23.809 0.000 -2.018 -1.711 C(X)[16] -1.7972 0.080 -22.479 0.000 -1.954 -1.641 C(X)[17] -1.7883 0.084 -21.385 0.000 -1.952 -1.624 C(X)[18] -1.9692 0.095 -20.833 0.000 -2.154 -1.784 C(X)[19] -1.8447 0.095 -19.479 0.000 -2.030 -1.659 C(X)[20] -1.9415 0.104 -18.631 0.000 -2.146 -1.737 C(X)[21] -2.0724 0.117 -17.710 0.000 -2.302 -1.843 C(X)[22] -1.9505 0.118 -16.555 0.000 -2.181 -1.720 C(X)[23] -1.8161 0.119 -15.214 0.000 -2.050 -1.582 C(X)[24] -2.0534 0.144 -14.272 0.000 -2.335 -1.771 C(X)[25] -2.0323 0.155 -13.081 0.000 -2.337 -1.728 C(X)[26] -2.0328 0.173 -11.771 0.000 -2.371 -1.694 C(X)[27] -1.9000 0.183 -10.385 0.000 -2.259 -1.541 C(X)[28] -2.0244 0.229 -8.832 0.000 -2.474 -1.575 C(X)[29] -2.2560 0.321 -7.023 0.000 -2.886 -1.626 C(X)[30] -2.0416 0.414 -4.927 0.000 -2.854 -1.229 Z1 0.1805 0.031 5.865 0.000 0.120 0.241 Z2 -1.1065 0.031 -35.243 0.000 -1.168 -1.045 Z3 -1.1032 0.032 -34.901 0.000 -1.165 -1.041 Z4 -0.8931 0.031 -28.536 0.000 -0.954 -0.832 Z5 -0.6629 0.031 -21.306 0.000 -0.724 -0.602 ============================================================================== Model summary for event: 2 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_2 No. Observations: 357418 Model: GLM Df Residuals: 357383 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -28316. Date: Sun, 03 Jul 2022 Deviance: 56631. Time: 15:23:56 Pearson chi2: 3.59e+05 No. Iterations: 8 Pseudo R-squ. (CS): 0.006804 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -1.7249 0.058 -29.825 0.000 -1.838 -1.612 C(X)[2] -1.9861 0.063 -31.491 0.000 -2.110 -1.862 C(X)[3] -1.8825 0.064 -29.415 0.000 -2.008 -1.757 C(X)[4] -1.9856 0.068 -29.285 0.000 -2.118 -1.853 C(X)[5] -1.9548 0.070 -28.038 0.000 -2.091 -1.818 C(X)[6] -2.0710 0.074 -27.870 0.000 -2.217 -1.925 C(X)[7] -2.0900 0.077 -27.041 0.000 -2.241 -1.939 C(X)[8] -2.0176 0.078 -25.728 0.000 -2.171 -1.864 C(X)[9] -2.1402 0.084 -25.383 0.000 -2.305 -1.975 C(X)[10] -2.0986 0.086 -24.341 0.000 -2.268 -1.930 C(X)[11] -2.1868 0.092 -23.735 0.000 -2.367 -2.006 C(X)[12] -2.1230 0.094 -22.634 0.000 -2.307 -1.939 C(X)[13] -2.1778 0.099 -21.913 0.000 -2.373 -1.983 C(X)[14] -2.1732 0.103 -21.039 0.000 -2.376 -1.971 C(X)[15] -2.1550 0.107 -20.087 0.000 -2.365 -1.945 C(X)[16] -2.2824 0.118 -19.375 0.000 -2.513 -2.052 C(X)[17] -2.2170 0.121 -18.375 0.000 -2.453 -1.981 C(X)[18] -2.4341 0.139 -17.550 0.000 -2.706 -2.162 C(X)[19] -2.1037 0.128 -16.470 0.000 -2.354 -1.853 C(X)[20] -2.2105 0.141 -15.677 0.000 -2.487 -1.934 C(X)[21] -2.4562 0.166 -14.765 0.000 -2.782 -2.130 C(X)[22] -2.2282 0.160 -13.940 0.000 -2.541 -1.915 C(X)[23] -2.4139 0.186 -12.968 0.000 -2.779 -2.049 C(X)[24] -2.4001 0.201 -11.917 0.000 -2.795 -2.005 C(X)[25] -2.4684 0.226 -10.905 0.000 -2.912 -2.025 C(X)[26] -2.1506 0.217 -9.894 0.000 -2.577 -1.725 C(X)[27] -2.0406 0.232 -8.784 0.000 -2.496 -1.585 C(X)[28] -2.2066 0.296 -7.454 0.000 -2.787 -1.626 C(X)[29] -2.1456 0.361 -5.951 0.000 -2.852 -1.439 C(X)[30] -1.8980 0.455 -4.172 0.000 -2.790 -1.006 Z1 0.0433 0.046 0.940 0.347 -0.047 0.133 Z2 -1.0656 0.047 -22.688 0.000 -1.158 -0.974 Z3 -1.3990 0.048 -29.224 0.000 -1.493 -1.305 Z4 -1.1034 0.047 -23.399 0.000 -1.196 -1.011 Z5 -0.7174 0.047 -15.386 0.000 -0.809 -0.626 ============================================================================== from pydts.examples_utils.plots import plot_first_model_coefs plot_first_model_coefs ( models = fitter . event_models , times = fitter . times , train_df = patients_df , n_cov = 5 )","title":"Lee et al. 2018 [1]"},{"location":"Simple%20Simulation/#new-approach","text":"Here, for each event, we first estimate \\(\\beta_{j}\\) using a time-stratified CoxPH model and the expanded data as we used in the previous approach. Afterwards, we evaluate \\(\\alpha_{jt}\\) based on the original training data. from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = train_df . drop ([ 'C' , 'T' ], axis = 1 )) new_fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.175630 0.029922 0.042164 0.045464 Z2 -1.077021 0.030535 -1.053325 0.046400 Z3 -1.072990 0.030744 -1.383865 0.047306 Z4 -0.868595 0.030434 -1.090288 0.046584 Z5 -0.643514 0.030250 -0.709322 0.046064 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 2560 True -0.993701 2 1713 True -1.252696 3 1321 True -1.390031 4 1123 True -1.426362 5 888 True -1.545886 6 823 True -1.506916 7 675 True -1.602185 8 603 True -1.610078 9 473 True -1.746932 10 458 True -1.675599 11 400 True -1.706351 12 346 True -1.745275 13 283 True -1.841762 14 259 True -1.824197 15 213 True -1.905305 16 203 True -1.841176 17 182 True -1.832710 18 135 True -2.017525 19 135 True -1.886645 20 108 True -1.990557 21 83 True -2.119080 22 82 True -1.999757 23 80 True -1.859465 24 53 True -2.100951 25 45 True -2.080507 26 36 True -2.081074 27 32 True -1.950757 28 20 True -2.072971 29 10 True -2.292853 30 6 True -2.089735 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 942 True -1.741469 2 617 True -2.004244 3 596 True -1.891698 4 472 True -2.003929 5 430 True -1.973737 6 341 True -2.086884 7 299 True -2.104953 8 288 True -2.034741 9 228 True -2.150781 10 214 True -2.110933 11 176 True -2.191509 12 168 True -2.131838 13 143 True -2.180693 14 129 True -2.175601 15 117 True -2.157802 16 92 True -2.267452 17 87 True -2.209869 18 62 True -2.440465 19 76 True -2.107629 20 60 True -2.200231 21 41 True -2.457340 22 45 True -2.212729 23 32 True -2.418286 24 27 True -2.404584 25 21 True -2.465084 26 23 True -2.142334 27 20 True -2.045518 28 12 True -2.190062 29 8 True -2.137646 30 5 True -1.913849 from pydts.examples_utils.plots import plot_second_model_coefs plot_second_model_coefs ( new_fitter . alpha_df , new_fitter . beta_models , times , n_cov = 5 ) new_fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.175630 0.029922 0.042164 0.045464 Z2 -1.077021 0.030535 -1.053325 0.046400 Z3 -1.072990 0.030744 -1.383865 0.047306 Z4 -0.868595 0.030434 -1.090288 0.046584 Z5 -0.643514 0.030250 -0.709322 0.046064 new_fitter . plot_all_events_beta () <AxesSubplot:title={'center':'$\\\\beta_{j}$ for all events'}, xlabel='Value', ylabel='$\\\\beta_{j}$'>","title":"New approach"},{"location":"Simple%20Simulation/#prediction","text":"# pred_df = new_fitter.predict_cumulative_incident_function(test_df) # pred_df = new_fitter.predict_marginal_prob_all_events(pred_df) # pred_df.set_index(['pid']).head().T pred_df = new_fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid 0 1 2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.940455 0.958727 0.930894 overall_survival_t2 0.896901 0.928018 0.880776 overall_survival_t3 0.858059 0.900500 0.836577 overall_survival_t4 0.822953 0.875209 0.796861 overall_survival_t5 0.791664 0.852514 0.761776 overall_survival_t6 0.761745 0.830406 0.728355 overall_survival_t7 0.734941 0.810424 0.698630 overall_survival_t8 0.708674 0.790680 0.669710 overall_survival_t9 0.686386 0.773755 0.645310 overall_survival_t10 0.663455 0.756133 0.620329 overall_survival_t11 0.642267 0.739641 0.597350 overall_survival_t12 0.621925 0.723707 0.575441 overall_survival_t13 0.603735 0.709345 0.555961 overall_survival_t14 0.585841 0.695072 0.536886 overall_survival_t15 0.569274 0.681787 0.519339 overall_survival_t16 0.553066 0.668569 0.502199 overall_survival_t17 0.536944 0.655324 0.485244 overall_survival_t18 0.524081 0.644643 0.471762 overall_survival_t19 0.508820 0.631972 0.455896 overall_survival_t20 0.495393 0.620725 0.441999 overall_survival_t21 0.484366 0.611361 0.430605 overall_survival_t22 0.471714 0.600590 0.417611 overall_survival_t23 0.459024 0.589532 0.404558 overall_survival_t24 0.448504 0.580388 0.393826 overall_survival_t25 0.438286 0.571408 0.383426 overall_survival_t26 0.427112 0.561650 0.372165 overall_survival_t27 0.414887 0.550861 0.359891 overall_survival_t28 0.404448 0.541554 0.349448 overall_survival_t29 0.395303 0.533440 0.340383 overall_survival_t30 0.384281 0.523584 0.329504 hazard_j1_t1 0.044777 0.032540 0.052977 hazard_j1_t10 0.023154 0.016723 0.027508 hazard_j1_t11 0.022469 0.016225 0.026698 hazard_j1_t12 0.021630 0.015615 0.025705 hazard_j1_t13 0.019679 0.014199 0.023396 hazard_j1_t14 0.020021 0.014447 0.023800 hazard_j1_t15 0.018490 0.013337 0.021987 hazard_j1_t16 0.019691 0.014208 0.023409 hazard_j1_t17 0.019855 0.014327 0.023603 hazard_j1_t18 0.016560 0.011938 0.019699 hazard_j1_t19 0.018832 0.013585 0.022392 hazard_j1_t2 0.034917 0.025303 0.041389 hazard_j1_t20 0.017005 0.012260 0.020227 hazard_j1_t21 0.014985 0.010798 0.017831 hazard_j1_t22 0.016852 0.012149 0.020045 hazard_j1_t23 0.019341 0.013954 0.022995 hazard_j1_t24 0.015255 0.010993 0.018151 hazard_j1_t25 0.015565 0.011217 0.018519 hazard_j1_t26 0.015556 0.011211 0.018509 hazard_j1_t27 0.017683 0.012752 0.021031 hazard_j1_t28 0.015681 0.011301 0.018656 hazard_j1_t29 0.012625 0.009091 0.015029 hazard_j1_t3 0.030573 0.022128 0.036271 hazard_j1_t30 0.015424 0.011116 0.018352 hazard_j1_t4 0.029514 0.021355 0.035022 hazard_j1_t5 0.026277 0.018995 0.031200 hazard_j1_t6 0.027292 0.019735 0.032399 hazard_j1_t7 0.024874 0.017974 0.029542 hazard_j1_t8 0.024683 0.017835 0.029317 hazard_j1_t9 0.021594 0.015590 0.025663 hazard_j2_t1 0.014768 0.008733 0.016130 hazard_j2_t10 0.010253 0.006052 0.011203 hazard_j2_t11 0.009467 0.005586 0.010345 hazard_j2_t12 0.010043 0.005927 0.010974 hazard_j2_t13 0.009569 0.005646 0.010456 hazard_j2_t14 0.009617 0.005675 0.010509 hazard_j2_t15 0.009788 0.005776 0.010696 hazard_j2_t16 0.008781 0.005179 0.009596 hazard_j2_t17 0.009296 0.005485 0.010158 hazard_j2_t18 0.007396 0.004360 0.008083 hazard_j2_t19 0.010287 0.006071 0.011240 hazard_j2_t2 0.011395 0.006728 0.012449 hazard_j2_t20 0.009386 0.005537 0.010256 hazard_j2_t21 0.007273 0.004287 0.007949 hazard_j2_t22 0.009270 0.005469 0.010130 hazard_j2_t23 0.007561 0.004457 0.008263 hazard_j2_t24 0.007664 0.004519 0.008376 hazard_j2_t25 0.007218 0.004254 0.007888 hazard_j2_t26 0.009939 0.005866 0.010861 hazard_j2_t27 0.010939 0.006458 0.011951 hazard_j2_t28 0.009481 0.005594 0.010360 hazard_j2_t29 0.009986 0.005893 0.010911 hazard_j2_t3 0.012735 0.007524 0.013911 hazard_j2_t30 0.012459 0.007360 0.013610 hazard_j2_t4 0.011398 0.006730 0.012453 hazard_j2_t5 0.011743 0.006935 0.012829 hazard_j2_t6 0.010500 0.006198 0.011473 hazard_j2_t7 0.010314 0.006088 0.011270 hazard_j2_t8 0.011056 0.006528 0.012079 hazard_j2_t9 0.009857 0.005817 0.010770 prob_j1_at_t1 0.044777 0.032540 0.052977 prob_j1_at_t2 0.032838 0.024259 0.038529 prob_j1_at_t3 0.027421 0.020535 0.031946 prob_j1_at_t4 0.025325 0.019231 0.029299 prob_j1_at_t5 0.021625 0.016625 0.024862 prob_j1_at_t6 0.021606 0.016825 0.024681 prob_j1_at_t7 0.018948 0.014926 0.021517 prob_j1_at_t8 0.018141 0.014454 0.020481 prob_j1_at_t9 0.015303 0.012326 0.017187 prob_j1_at_t10 0.015893 0.012940 0.017751 prob_j1_at_t11 0.014907 0.012268 0.016561 prob_j1_at_t12 0.013892 0.011550 0.015355 prob_j1_at_t13 0.012239 0.010276 0.013463 prob_j1_at_t14 0.012087 0.010248 0.013232 prob_j1_at_t15 0.010832 0.009270 0.011804 prob_j1_at_t16 0.011209 0.009687 0.012157 prob_j1_at_t17 0.010981 0.009578 0.011854 prob_j1_at_t18 0.008892 0.007823 0.009559 prob_j1_at_t19 0.009869 0.008757 0.010564 prob_j1_at_t20 0.008652 0.007748 0.009221 prob_j1_at_t21 0.007423 0.006702 0.007881 prob_j1_at_t22 0.008162 0.007428 0.008631 prob_j1_at_t23 0.009123 0.008380 0.009603 prob_j1_at_t24 0.007002 0.006481 0.007343 prob_j1_at_t25 0.006981 0.006510 0.007293 prob_j1_at_t26 0.006818 0.006406 0.007097 prob_j1_at_t27 0.007553 0.007162 0.007827 prob_j1_at_t28 0.006506 0.006225 0.006714 prob_j1_at_t29 0.005106 0.004923 0.005252 prob_j1_at_t30 0.006097 0.005929 0.006247 prob_j2_at_t1 0.014768 0.008733 0.016130 prob_j2_at_t2 0.010716 0.006451 0.011588 prob_j2_at_t3 0.011422 0.006982 0.012253 prob_j2_at_t4 0.009780 0.006061 0.010418 prob_j2_at_t5 0.009664 0.006070 0.010223 prob_j2_at_t6 0.008313 0.005284 0.008740 prob_j2_at_t7 0.007857 0.005055 0.008208 prob_j2_at_t8 0.008126 0.005290 0.008439 prob_j2_at_t9 0.006985 0.004599 0.007213 prob_j2_at_t10 0.007038 0.004682 0.007229 prob_j2_at_t11 0.006281 0.004224 0.006417 prob_j2_at_t12 0.006450 0.004384 0.006555 prob_j2_at_t13 0.005951 0.004086 0.006017 prob_j2_at_t14 0.005806 0.004025 0.005843 prob_j2_at_t15 0.005734 0.004015 0.005742 prob_j2_at_t16 0.004999 0.003531 0.004983 prob_j2_at_t17 0.005142 0.003667 0.005102 prob_j2_at_t18 0.003971 0.002857 0.003922 prob_j2_at_t19 0.005391 0.003914 0.005302 prob_j2_at_t20 0.004776 0.003500 0.004676 prob_j2_at_t21 0.003603 0.002661 0.003514 prob_j2_at_t22 0.004490 0.003344 0.004362 prob_j2_at_t23 0.003566 0.002677 0.003451 prob_j2_at_t24 0.003518 0.002664 0.003389 prob_j2_at_t25 0.003237 0.002469 0.003107 prob_j2_at_t26 0.004356 0.003352 0.004164 prob_j2_at_t27 0.004672 0.003627 0.004448 prob_j2_at_t28 0.003933 0.003081 0.003728 prob_j2_at_t29 0.004039 0.003191 0.003813 prob_j2_at_t30 0.004925 0.003926 0.004633 cif_j1_at_t1 0.044777 0.032540 0.052977 cif_j1_at_t2 0.077614 0.056799 0.091506 cif_j1_at_t3 0.105035 0.077334 0.123452 cif_j1_at_t4 0.130360 0.096565 0.152751 cif_j1_at_t5 0.151985 0.113190 0.177613 cif_j1_at_t6 0.173591 0.130014 0.202294 cif_j1_at_t7 0.192539 0.144940 0.223811 cif_j1_at_t8 0.210680 0.159394 0.244292 cif_j1_at_t9 0.225983 0.171721 0.261479 cif_j1_at_t10 0.241876 0.184661 0.279231 cif_j1_at_t11 0.256783 0.196929 0.295792 cif_j1_at_t12 0.270675 0.208478 0.311147 cif_j1_at_t13 0.282914 0.218755 0.324609 cif_j1_at_t14 0.295001 0.229003 0.337841 cif_j1_at_t15 0.305834 0.238273 0.349646 cif_j1_at_t16 0.317043 0.247959 0.361803 cif_j1_at_t17 0.328024 0.257537 0.373657 cif_j1_at_t18 0.336915 0.265361 0.383216 cif_j1_at_t19 0.346785 0.274118 0.393779 cif_j1_at_t20 0.355437 0.281866 0.403000 cif_j1_at_t21 0.362860 0.288568 0.410882 cif_j1_at_t22 0.371023 0.295996 0.419513 cif_j1_at_t23 0.380146 0.304376 0.429116 cif_j1_at_t24 0.387148 0.310857 0.436459 cif_j1_at_t25 0.394129 0.317368 0.443752 cif_j1_at_t26 0.400947 0.323774 0.450849 cif_j1_at_t27 0.408500 0.330936 0.458676 cif_j1_at_t28 0.415005 0.337161 0.465390 cif_j1_at_t29 0.420111 0.342084 0.470642 cif_j1_at_t30 0.426208 0.348014 0.476888 cif_j2_at_t1 0.014768 0.008733 0.016130 cif_j2_at_t2 0.025484 0.015183 0.027718 cif_j2_at_t3 0.036906 0.022166 0.039971 cif_j2_at_t4 0.046686 0.028226 0.050388 cif_j2_at_t5 0.056351 0.034296 0.060611 cif_j2_at_t6 0.064663 0.039580 0.069351 cif_j2_at_t7 0.072520 0.044635 0.077559 cif_j2_at_t8 0.080646 0.049925 0.085998 cif_j2_at_t9 0.087631 0.054524 0.093211 cif_j2_at_t10 0.094669 0.059207 0.100440 cif_j2_at_t11 0.100950 0.063430 0.106858 cif_j2_at_t12 0.107400 0.067814 0.113413 cif_j2_at_t13 0.113351 0.071900 0.119430 cif_j2_at_t14 0.119158 0.075926 0.125272 cif_j2_at_t15 0.124892 0.079941 0.131015 cif_j2_at_t16 0.129891 0.083472 0.135998 cif_j2_at_t17 0.135032 0.087139 0.141100 cif_j2_at_t18 0.139004 0.089996 0.145022 cif_j2_at_t19 0.144395 0.093910 0.150324 cif_j2_at_t20 0.149170 0.097409 0.155000 cif_j2_at_t21 0.152773 0.100071 0.158514 cif_j2_at_t22 0.157264 0.103414 0.162875 cif_j2_at_t23 0.160830 0.106091 0.166326 cif_j2_at_t24 0.164348 0.108755 0.169715 cif_j2_at_t25 0.167585 0.111224 0.172822 cif_j2_at_t26 0.171941 0.114576 0.176986 cif_j2_at_t27 0.176614 0.118203 0.181434 cif_j2_at_t28 0.180547 0.121285 0.185162 cif_j2_at_t29 0.184586 0.124476 0.188975 cif_j2_at_t30 0.189511 0.128402 0.193608","title":"Prediction"},{"location":"Simple%20Simulation/#comparison-wip","text":"pd . DataFrame ( index = pd . MultiIndex . from_product ([[ 'd=30' , 'd=60' , 'd=100' , 'd=150' ], [ 'N=1000' , 'N=10000' , 'N=100000' ]], names = [ 'N' , 'D' ]), columns = [ 'Train MSE (oracle)' , 'Test MSE (oracle)' , 'computation time' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Train MSE (oracle) Test MSE (oracle) computation time N D d=30 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=60 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=100 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=150 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN from pydts.examples_utils.plots import compare_beta_models_for_example from pydts.utils import present_coefs res_dict = compare_beta_models_for_example ( fitter . event_models , new_fitter . event_models , real_coef_dict = real_coef_dict ) present_coefs ( res_dict ) for coef: Alpha .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lee Ours real a1_1 -0.951613 -0.993701 -1.000000 a2_1 -1.209701 -1.252696 -1.207944 a3_1 -1.347113 -1.390031 -1.329584 a4_1 -1.383203 -1.426362 -1.415888 a5_1 -1.504718 -1.545886 -1.482831 a6_1 -1.464485 -1.506916 -1.537528 a7_1 -1.556570 -1.602185 -1.583773 a8_1 -1.564112 -1.610078 -1.623832 a9_1 -1.701809 -1.746932 -1.659167 a10_1 -1.629386 -1.675599 -1.690776 a11_1 -1.660134 -1.706351 -1.719369 a12_1 -1.699373 -1.745275 -1.745472 a13_1 -1.798048 -1.841762 -1.769485 a14_1 -1.779825 -1.824197 -1.791717 a15_1 -1.864221 -1.905305 -1.812415 a16_1 -1.797205 -1.841176 -1.831777 a17_1 -1.788325 -1.832710 -1.849964 a18_1 -1.969211 -2.017525 -1.867112 a19_1 -1.844675 -1.886645 -1.883332 a20_1 -1.941487 -1.990557 -1.898720 a21_1 -2.072392 -2.119080 -1.913357 a22_1 -1.950505 -1.999757 -1.927313 a23_1 -1.816114 -1.859465 -1.940648 a24_1 -2.053391 -2.100951 -1.953416 a25_1 -2.032329 -2.080507 -1.965663 a26_1 -2.032766 -2.081074 -1.977429 a27_1 -1.899979 -1.950757 -1.988751 a28_1 -2.024438 -2.072971 -1.999661 a29_1 -2.256035 -2.292853 -2.010189 a30_1 -2.041636 -2.089735 -2.020359 a1_2 -1.724923 -1.741469 -1.750000 a2_2 -1.986075 -2.004244 -1.853972 a3_2 -1.882549 -1.891698 -1.914792 a4_2 -1.985604 -2.003929 -1.957944 a5_2 -1.954779 -1.973737 -1.991416 a6_2 -2.071016 -2.086884 -2.018764 a7_2 -2.089997 -2.104953 -2.041887 a8_2 -2.017632 -2.034741 -2.061916 a9_2 -2.140155 -2.150781 -2.079584 a10_2 -2.098609 -2.110933 -2.095388 a11_2 -2.186827 -2.191509 -2.109684 a12_2 -2.123014 -2.131838 -2.122736 a13_2 -2.177759 -2.180693 -2.134742 a14_2 -2.173186 -2.175601 -2.145859 a15_2 -2.154986 -2.157802 -2.156208 a16_2 -2.282404 -2.267452 -2.165888 a17_2 -2.216986 -2.209869 -2.174982 a18_2 -2.434101 -2.440465 -2.183556 a19_2 -2.103665 -2.107629 -2.191666 a20_2 -2.210458 -2.200231 -2.199360 a21_2 -2.456157 -2.457340 -2.206678 a22_2 -2.228180 -2.212729 -2.213656 a23_2 -2.413939 -2.418286 -2.220324 a24_2 -2.400092 -2.404584 -2.226708 a25_2 -2.468446 -2.465084 -2.232831 a26_2 -2.150649 -2.142334 -2.238714 a27_2 -2.040635 -2.045518 -2.244376 a28_2 -2.206609 -2.190062 -2.249831 a29_2 -2.145625 -2.137646 -2.255094 a30_2 -1.898031 -1.913849 -2.260180 for coef: Beta .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lee Ours real Z1_1 0.180473 0.175630 0.223144 Z2_1 -1.106470 -1.077021 -1.098612 Z3_1 -1.103171 -1.072990 -1.098612 Z4_1 -0.893110 -0.868595 -0.916291 Z5_1 -0.662878 -0.643514 -0.693147 Z1_2 0.043267 0.042164 -0.000000 Z2_2 -1.065630 -1.053325 -1.098612 Z3_2 -1.398985 -1.383865 -1.386294 Z4_2 -1.103417 -1.090288 -1.098612 Z5_2 -0.717390 -0.709322 -0.693147 from pydts.utils import create_df_for_cif_plots df_for_plotting = create_df_for_cif_plots ( test_df , field = \"Z1\" , covariates = new_fitter . covariates , quantiles = [ 0.25 , 0.5 , 0.75 ], zero_others = True ) our_pred_df = new_fitter . predict_cumulative_incident_function ( df_for_plotting ) our_pred_df = new_fitter . predict_marginal_prob_all_events ( our_pred_df ) our_pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 overall_survival_t1 overall_survival_t2 overall_survival_t3 overall_survival_t4 overall_survival_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.570378 0.370758 0.245091 0.166491 0.115641 ... 0.362827 0.362884 0.362928 0.362975 0.363015 0.363042 0.363063 0.363085 0.636792 0.363085 4839 0.506276 0.0 0.0 0.0 0.0 0.560076 0.359013 0.234264 0.157197 0.107959 ... 0.356779 0.356825 0.356860 0.356897 0.356929 0.356950 0.356966 0.356983 0.642924 0.356983 4839 0.750906 0.0 0.0 0.0 0.0 0.549852 0.347522 0.223807 0.148330 0.100713 ... 0.351012 0.351049 0.351077 0.351107 0.351131 0.351148 0.351161 0.351173 0.648756 0.351173 3 rows \u00d7 217 columns lee_pred_df = fitter . predict_cumulative_incident_function ( df_for_plotting ) lee_pred_df = fitter . predict_marginal_prob_all_events ( lee_pred_df ) lee_pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 overall_survival_t1 overall_survival_t2 overall_survival_t3 overall_survival_t4 overall_survival_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.559414 0.358099 0.233707 0.156609 0.107446 ... 0.357832 0.357878 0.357912 0.357949 0.357981 0.358001 0.358018 0.358034 0.641872 0.358034 4839 0.506276 0.0 0.0 0.0 0.0 0.548640 0.346007 0.222712 0.147300 0.099849 ... 0.351764 0.351800 0.351827 0.351856 0.351881 0.351896 0.351909 0.351922 0.648008 0.351922 4839 0.750906 0.0 0.0 0.0 0.0 0.537951 0.334190 0.212116 0.138445 0.092709 ... 0.345988 0.346017 0.346038 0.346061 0.346080 0.346092 0.346101 0.346111 0.653837 0.346111 3 rows \u00d7 217 columns from pydts.utils import get_real_hazard df_temp = get_real_hazard ( df_for_plotting . copy (), real_coef_dict = real_coef_dict , times = times , events = [ 1 , 2 ]) df_temp .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 hazard_j1_t1 hazard_j1_t2 hazard_j1_t3 hazard_j1_t4 hazard_j1_t5 ... hazard_j2_t21 hazard_j2_t22 hazard_j2_t23 hazard_j2_t24 hazard_j2_t25 hazard_j2_t26 hazard_j2_t27 hazard_j2_t28 hazard_j2_t29 hazard_j2_t30 4839 0.255555 0.0 0.0 0.0 0.0 0.280300 0.240321 0.218820 0.204425 0.193753 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 4839 0.506276 0.0 0.0 0.0 0.0 0.291724 0.250683 0.228533 0.213675 0.202643 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 4839 0.750906 0.0 0.0 0.0 0.0 0.303129 0.261076 0.238300 0.222990 0.211606 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 3 rows \u00d7 65 columns real_pred_df = new_fitter . predict_cumulative_incident_function ( df_temp ) real_pred_df = new_fitter . predict_marginal_prob_all_events ( real_pred_df ) real_pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Z1 Z2 Z3 Z4 Z5 hazard_j1_t1 hazard_j1_t2 hazard_j1_t3 hazard_j1_t4 hazard_j1_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.280300 0.240321 0.218820 0.204425 0.193753 ... 0.361174 0.361223 0.361260 0.361289 0.361311 0.361329 0.361342 0.361352 0.638562 0.361352 4839 0.506276 0.0 0.0 0.0 0.0 0.291724 0.250683 0.228533 0.213675 0.202643 ... 0.351030 0.351068 0.351096 0.351118 0.351135 0.351148 0.351158 0.351166 0.648772 0.351166 4839 0.750906 0.0 0.0 0.0 0.0 0.303129 0.261076 0.238300 0.222990 0.211606 ... 0.341411 0.341440 0.341462 0.341479 0.341491 0.341501 0.341508 0.341514 0.658441 0.341514 3 rows \u00d7 217 columns import matplotlib.pyplot as plt from pydts.examples_utils.plots import plot_cif_plots j_events = 2 preds = [ lee_pred_df , our_pred_df , real_pred_df ] names = [ 'Lee' , 'Our' , 'Real' ] fig = plt . figure ( constrained_layout = True , figsize = ( 20 , 20 )) subfigs = fig . subfigures ( nrows = len ( preds ), ncols = 1 ) for row , subfig in enumerate ( subfigs ): subfig . suptitle ( f 'CIF for { names [ row ] } method' , fontsize = 30 , fontweight = 'bold' ) pred_df = preds [ row ] # create 1x3 subplots per subfig axs = subfig . subplots ( nrows = 1 , ncols = j_events ) for col , ax in enumerate ( axs ): ax = plot_cif_plots ( pred_df , event = col + 1 , return_ax = True , ax = ax , pad = 0.05 , scale = 5 ) h , l = ax . get_legend_handles_labels () ax . legend ( h ,[ \"Z5 - Q25\" , \"Z5 - Q50\" , \"Z5 - Q75\" ], fontsize = 15 ) from lifelines.fitters.coxph_fitter import CoxPHFitter events = pd . get_dummies ( train_df [ 'J' ]) . add_prefix ( \"J_\" ) rel_train = train_df . drop ( columns = [ 'C' , 'T' , 'J' ]) . merge ( events , how = 'inner' , left_index = True , right_index = True ) . copy () fit_cols = [ * covariates , \"X\" ] models = {} for event in [ 1 , 2 ]: model = CoxPHFitter () event_col = f \"J_ { event } \" print ( event_col ) model . fit_right_censoring ( df = rel_train [[ * fit_cols , event_col ]] , duration_col = 'X' , event_col = event_col ) model . print_summary () models . update ({ event : model }) rel_times = models [ 1 ] . timeline . union ( models [ 2 ] . timeline ) J_1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'J_1' baseline estimation breslow number of observations 37500 number of events observed 13348 partial log-likelihood -130916.90 time fit was run 2022-07-03 12:24:34 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.18 1.19 0.03 0.12 0.23 1.12 1.26 5.87 <0.005 27.77 Z2 -1.08 0.34 0.03 -1.14 -1.02 0.32 0.36 -35.27 <0.005 902.87 Z3 -1.07 0.34 0.03 -1.13 -1.01 0.32 0.36 -34.90 <0.005 884.08 Z4 -0.87 0.42 0.03 -0.93 -0.81 0.40 0.45 -28.54 <0.005 592.75 Z5 -0.64 0.53 0.03 -0.70 -0.58 0.50 0.56 -21.27 <0.005 331.19 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.65 Partial AIC 261843.81 log-likelihood ratio test 3537.28 on 5 df -log2(p) of ll-ratio test inf J_2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'J_2' baseline estimation breslow number of observations 37500 number of events observed 5771 partial log-likelihood -56065.63 time fit was run 2022-07-03 12:24:34 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.04 1.04 0.05 -0.05 0.13 0.95 1.14 0.93 0.35 1.50 Z2 -1.05 0.35 0.05 -1.14 -0.96 0.32 0.38 -22.70 <0.005 376.56 Z3 -1.38 0.25 0.05 -1.48 -1.29 0.23 0.27 -29.25 <0.005 622.51 Z4 -1.09 0.34 0.05 -1.18 -1.00 0.31 0.37 -23.40 <0.005 400.02 Z5 -0.71 0.49 0.05 -0.80 -0.62 0.45 0.54 -15.40 <0.005 175.32 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.67 Partial AIC 112141.27 log-likelihood ratio test 2026.96 on 5 df -log2(p) of ll-ratio test inf from pydts.examples_utils.continues_data import hazard_func , survival_func hazards = hazard_func ( models , df_for_plotting ) surv_func = survival_func ( models , rel_times , df_for_plotting ) cif_1 = pd . DataFrame ( hazards [ 1 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cif_2 = pd . DataFrame ( hazards [ 2 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cont_cif = pd . merge ( cif_1 . reset_index ( drop = True ) . add_prefix ( \"cif_j1_at_t\" ), cif_2 . reset_index ( drop = True ) . add_prefix ( \"cif_j2_at_t\" ), right_index = True , left_index = True ) cont_cif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cif_j1_at_t1 cif_j1_at_t2 cif_j1_at_t3 cif_j1_at_t4 cif_j1_at_t5 cif_j1_at_t6 cif_j1_at_t7 cif_j1_at_t8 cif_j1_at_t9 cif_j1_at_t10 ... cif_j2_at_t21 cif_j2_at_t22 cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 0 0.056824 0.098869 0.133680 0.165468 0.192327 0.218796 0.241867 0.263644 0.281896 0.300560 ... 0.182505 0.187035 0.190665 0.194228 0.197451 0.201706 0.206258 0.209989 0.213826 0.218531 1 0.059204 0.102907 0.139013 0.171917 0.199668 0.226965 0.250715 0.273095 0.291823 0.310943 ... 0.181421 0.185837 0.189371 0.192836 0.195967 0.200095 0.204506 0.208117 0.211828 0.216372 2 0.061615 0.106988 0.144394 0.178411 0.207048 0.235161 0.259579 0.282547 0.301737 0.321297 ... 0.180266 0.184567 0.188005 0.191371 0.194410 0.198412 0.202682 0.206174 0.209759 0.214143 3 rows \u00d7 60 columns import matplotlib.pyplot as plt from pydts.examples_utils.plots import plot_cif_plots j_events = 2 preds = [ lee_pred_df , our_pred_df , real_pred_df , cont_cif ] names = [ 'Lee' , 'Our' , 'Real' , \"Continues\" ] fig = plt . figure ( constrained_layout = True , figsize = ( 20 , 20 )) subfigs = fig . subfigures ( nrows = len ( preds ), ncols = 1 ) for row , subfig in enumerate ( subfigs ): subfig . suptitle ( f 'CIF for { names [ row ] } method' , fontsize = 30 , fontweight = 'bold' ) pred_df = preds [ row ] # create 1x3 subplots per subfig axs = subfig . subplots ( nrows = 1 , ncols = j_events ) for col , ax in enumerate ( axs ): ax = plot_cif_plots ( pred_df , event = col + 1 , return_ax = True , ax = ax , pad = 0.05 , scale = 5 ) h , l = ax . get_legend_handles_labels () ax . legend ( h ,[ \"Z1 - Q25\" , \"Z1 - Q50\" , \"Z1 - Q75\" ], fontsize = 15 ) ax . set_ylim ([ 0 , 0.7 ]) plt . savefig ( \"comparison1.jpg\" ) df_for_plotting = create_df_for_cif_plots ( test_df , field = \"Z2\" , covariates = new_fitter . covariates , quantiles = [ 0.25 , 0.5 , 0.75 ], zero_others = True ) our_pred_df = new_fitter . predict_cumulative_incident_function ( df_for_plotting ) our_pred_df = new_fitter . predict_marginal_prob_all_events ( our_pred_df ) lee_pred_df = fitter . predict_cumulative_incident_function ( df_for_plotting ) lee_pred_df = fitter . predict_marginal_prob_all_events ( lee_pred_df ) df_temp = get_real_hazard ( df_for_plotting . copy (), real_coef_dict = real_coef_dict , times = times , events = [ 1 , 2 ]) real_pred_df = new_fitter . predict_cumulative_incident_function ( df_temp ) real_pred_df = new_fitter . predict_marginal_prob_all_events ( real_pred_df ) hazards = hazard_func ( models , df_for_plotting ) surv_func = survival_func ( models , rel_times , df_for_plotting ) cif_1 = pd . DataFrame ( hazards [ 1 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cif_2 = pd . DataFrame ( hazards [ 2 ] * surv_func , index = df_for_plotting . index , columns = rel_times . astype ( int )) . cumsum ( axis = 1 ) cont_cif = pd . merge ( cif_1 . reset_index ( drop = True ) . add_prefix ( \"cif_j1_at_t\" ), cif_2 . reset_index ( drop = True ) . add_prefix ( \"cif_j2_at_t\" ), right_index = True , left_index = True ) import matplotlib.pyplot as plt from pydts.examples_utils.plots import plot_cif_plots j_events = 2 preds = [ lee_pred_df , our_pred_df , real_pred_df , cont_cif ] names = [ 'Lee' , 'Our' , 'Real' , \"Continues\" ] fig = plt . figure ( constrained_layout = True , figsize = ( 20 , 20 )) subfigs = fig . subfigures ( nrows = len ( preds ), ncols = 1 ) for row , subfig in enumerate ( subfigs ): subfig . suptitle ( f 'CIF for { names [ row ] } method' , fontsize = 30 , fontweight = 'bold' ) pred_df = preds [ row ] # create 1x3 subplots per subfig axs = subfig . subplots ( nrows = 1 , ncols = j_events ) for col , ax in enumerate ( axs ): ax = plot_cif_plots ( pred_df , event = col + 1 , return_ax = True , ax = ax , pad = 0.05 , scale = 5 ) h , l = ax . get_legend_handles_labels () ax . legend ( h ,[ \"Z2 - Q25\" , \"Z2 - Q50\" , \"Z2 - Q75\" ], fontsize = 15 ) ax . set_ylim ([ 0 , 0.7 ]) plt . savefig ( \"comparison2.jpg\" )","title":"Comparison (WIP)"},{"location":"SimulatedDataset/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hospitalization length of stay \u00a4 In this example, we simulate a dataset of 10,000 hospitalized patietns as follows: We sample the covariates such that: \\(\\mbox{Admission Year} \\sim \\mbox{Uniform}(2000,2014)\\) \\(\\mbox{Gender} \\sim \\mbox{Bernoullie}(0.5), \\qquad\\) (1 is Female, 0 is Male) \\(\\mbox{Age} \\sim \\mbox{Normal}(72+5*\\mbox{gender}\\;,\\;12)\\) (years) \\(\\mbox{Height} \\sim \\mbox{Normal}(175-5*\\mbox{gender}\\;,\\;7)\\) (cm) \\(\\mbox{Weight} \\sim \\mbox{Normal}(\\frac{\\mbox{height}}{175}*80 - 5 * \\mbox{gender} + \\frac{\\mbox{age}}{20}\\;,\\;8)\\) (kg) \\(\\mbox{BMI} \\: (\\mbox{Body} \\: \\mbox{Mass} \\: \\mbox{Index}) = \\frac{\\mbox{Weight}}{(\\frac{\\mbox{Height}}{100})^2}\\) (kg/m^2) \\(\\mbox{Admission Serial} \\sim \\mbox{LogNormal}(0, 0.75)\\) \\(\\mbox{Smoking Status} \\sim \\mbox{Multinomial(No, Previously, Currently)} \\quad p=[0.5, 0.3, 0.2]\\) Precondition features: \\(\\mbox{General_p} = 0.003 * \\mbox{bmi} - 0.15 * \\mbox{gender} + 0.002 * \\mbox{age} + 0.1 * \\mbox{smoking}\\) \\(\\mbox{Preconditions_p} = max( min(\\mbox{General_p}, 0.65), 0.05)\\) \\(\\mbox{Hypertension} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\) \\(\\mbox{Diabetes} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p} + 0.003*\\mbox{BMI})\\) \\(\\mbox{Arterial Fibrillation} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\) \\(\\mbox{Chronic Obstructive Pulmonary Disease} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p} + 0.1*\\mbox{smoking})\\) \\(\\mbox{Chronic Renal Failure} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\) Finally, based on the above covariates, we sample LOS and the event type: discharged or in-hospital death. After sampling the LOS, for some patients we remove weight (and BMI) information based on year of admission, to reflect missingness which can occur in real world data. Loading Simulation Dataset \u00a4 import pandas as pd import numpy as np from matplotlib import pyplot as plt % matplotlib inline pd . set_option ( 'display.max_rows' , 500 ) from pydts.examples_utils.simulations_data_config import * from pydts.examples_utils.datasets import load_LOS_simulated_data import warnings warnings . filterwarnings ( 'ignore' ) warnings . simplefilter ( 'ignore' ) data_df = load_LOS_simulated_data () Data Description \u00a4 First, let's look at some descriptive statistics of the columns: paper_table = data_df . describe () . T . round ( 2 ) idx = [ r for r in paper_table . index if r not in [ 'ID' , 'Returning_patient' , 'Death_date_in_hosp_missing' , 'In_hospital_death' ,]] paper_table = paper_table . loc [ idx , :] paper_table . rename ({ 'Age' : 'Age (years)' , 'Gender' : 'Gender (1 - female)' , 'Admyear' : 'Admyear (year)' , 'Firstadm' : 'Firstadm (1 - yes)' , 'Admserial' : 'Admserial' , 'Weight' : 'Weight (kg)' , 'Height' : 'Height (cm)' , 'BMI' : 'Height (kg/m^2)' , 'Smoking' : f \"Smoking (0 - never, \\\\ \\qquad \\qquad \\, 1 - previously, \\\\ \\qquad \\qquad \\, 2 - currently)\" , 'Discharge_relative_date' : 'LOS (days)' , 'Hypertension' : 'Hypertension (1 - yes)' , 'Diabities' : 'Diabetes (1 - yes)' , 'AF' : 'AF (1 - yes)' , 'COPD' : 'COPD (1 - yes)' , 'CRF' : 'CRF (1 - yes)' , 'Death_relative_date_in_hosp' : 'In-Hospital Death (days)' }, inplace = True ) paper_table [ 'count' ] = paper_table [ 'count' ] . astype ( int ) paper_table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max Age (years) 10000 74.38 12.12 24.10 66.30 74.40 82.60 122.70 Gender (1 - female) 10000 0.49 0.50 0.00 0.00 0.00 1.00 1.00 Admyear (year) 10000 2006.96 4.29 2000.00 2003.00 2007.00 2011.00 2014.00 Firstadm (1 - yes) 10000 0.70 0.46 0.00 0.00 1.00 1.00 1.00 Admserial 10000 1.50 1.03 1.00 1.00 1.00 2.00 17.00 Weight (kg) 4497 80.05 9.40 46.92 73.66 80.08 86.41 116.46 Height (cm) 10000 172.47 7.41 145.68 167.50 172.41 177.49 201.24 Height (kg/m^2) 4497 26.94 2.97 15.16 24.86 26.89 28.85 40.35 Smoking (0 - never, \\ \\qquad \\qquad \\, 1 - previously, \\ \\qquad \\qquad \\, 2 - currently) 10000 0.70 0.78 0.00 0.00 1.00 1.00 2.00 Hypertension (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 Diabetes 10000 0.31 0.46 0.00 0.00 0.00 1.00 1.00 AF (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 COPD (1 - yes) 10000 0.29 0.46 0.00 0.00 0.00 1.00 1.00 CRF (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 LOS (days) 10000 12.96 10.26 1.00 4.00 10.00 21.00 31.00 In-Hospital Death (days) 2362 10.21 7.86 1.00 4.00 8.00 15.00 30.00 When dealing with healthcare data, changes of policy can lead to biased data, so it is also a good idea to see how the data looks like with stratification by year of admission: import tableone as to columns = [ AGE_COL , GENDER_COL , WEIGHT_COL , HEIGHT_COL , BMI_COL , * preconditions , RETURNING_PATIENT_COL ] categorical = [ GENDER_COL , * preconditions , RETURNING_PATIENT_COL ] groupby = ADMISSION_YEAR_COL mytable = to . TableOne ( data_df , columns , categorical , groupby ) mytable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } Grouped by Admyear Missing Overall 2000.0 2001.0 2002.0 2003.0 2004.0 2005.0 2006.0 2007.0 2008.0 2009.0 2010.0 2011.0 2012.0 2013.0 2014.0 n 10000 637 659 724 657 671 687 661 711 669 678 655 648 631 662 650 Age, mean (SD) 0 74.4 (12.1) 73.8 (12.4) 74.3 (11.8) 74.7 (12.1) 74.8 (12.0) 74.3 (11.7) 73.9 (11.9) 74.5 (13.1) 73.7 (11.9) 74.8 (12.0) 74.8 (12.1) 75.6 (11.6) 73.8 (12.4) 74.6 (11.9) 73.8 (12.5) 74.2 (12.1) Gender, n (%) 0.0 0 5064 (50.6) 307 (48.2) 349 (53.0) 362 (50.0) 317 (48.2) 346 (51.6) 339 (49.3) 350 (53.0) 367 (51.6) 327 (48.9) 355 (52.4) 343 (52.4) 331 (51.1) 318 (50.4) 332 (50.2) 321 (49.4) 1.0 4936 (49.4) 330 (51.8) 310 (47.0) 362 (50.0) 340 (51.8) 325 (48.4) 348 (50.7) 311 (47.0) 344 (48.4) 342 (51.1) 323 (47.6) 312 (47.6) 317 (48.9) 313 (49.6) 330 (49.8) 329 (50.6) Weight, mean (SD) 5503 80.1 (9.4) 78.4 (10.5) 80.1 (11.6) 81.2 (9.9) 79.8 (9.7) 78.1 (9.6) 80.0 (9.7) 79.8 (9.2) 80.7 (8.7) 80.9 (9.4) 80.9 (9.4) 79.9 (9.2) 80.4 (9.3) 79.6 (9.4) 79.8 (9.7) 79.8 (8.9) Height, mean (SD) 0 172.5 (7.4) 172.7 (7.5) 172.4 (7.8) 172.4 (7.4) 172.4 (7.1) 172.3 (7.3) 172.1 (7.5) 172.5 (7.5) 172.7 (7.2) 172.4 (7.3) 172.9 (7.6) 172.6 (7.5) 172.7 (7.5) 172.3 (7.1) 172.5 (7.7) 172.3 (7.3) BMI, mean (SD) 5503 26.9 (3.0) 26.7 (3.4) 27.0 (3.1) 27.0 (3.0) 26.7 (2.9) 26.4 (3.1) 26.9 (3.2) 26.9 (3.0) 27.2 (2.8) 27.2 (3.1) 27.2 (3.0) 26.9 (2.9) 26.9 (3.0) 26.9 (2.9) 26.9 (3.0) 26.9 (2.9) Smoking, n (%) 0.0 0 4982 (49.8) 323 (50.7) 349 (53.0) 371 (51.2) 325 (49.5) 332 (49.5) 334 (48.6) 325 (49.2) 352 (49.5) 319 (47.7) 336 (49.6) 310 (47.3) 305 (47.1) 327 (51.8) 322 (48.6) 352 (54.2) 1.0 3033 (30.3) 185 (29.0) 177 (26.9) 228 (31.5) 193 (29.4) 195 (29.1) 220 (32.0) 193 (29.2) 214 (30.1) 221 (33.0) 198 (29.2) 223 (34.0) 211 (32.6) 174 (27.6) 206 (31.1) 195 (30.0) 2.0 1985 (19.9) 129 (20.3) 133 (20.2) 125 (17.3) 139 (21.2) 144 (21.5) 133 (19.4) 143 (21.6) 145 (20.4) 129 (19.3) 144 (21.2) 122 (18.6) 132 (20.4) 130 (20.6) 134 (20.2) 103 (15.8) Hypertension, n (%) 0.0 0 7717 (77.2) 505 (79.3) 497 (75.4) 545 (75.3) 523 (79.6) 517 (77.0) 552 (80.3) 482 (72.9) 551 (77.5) 519 (77.6) 520 (76.7) 523 (79.8) 488 (75.3) 478 (75.8) 505 (76.3) 512 (78.8) 1.0 2283 (22.8) 132 (20.7) 162 (24.6) 179 (24.7) 134 (20.4) 154 (23.0) 135 (19.7) 179 (27.1) 160 (22.5) 150 (22.4) 158 (23.3) 132 (20.2) 160 (24.7) 153 (24.2) 157 (23.7) 138 (21.2) Diabetes, n (%) 0.0 0 6932 (69.3) 446 (70.0) 444 (67.4) 512 (70.7) 454 (69.1) 457 (68.1) 491 (71.5) 463 (70.0) 491 (69.1) 464 (69.4) 464 (68.4) 444 (67.8) 442 (68.2) 430 (68.1) 453 (68.4) 477 (73.4) 1.0 3068 (30.7) 191 (30.0) 215 (32.6) 212 (29.3) 203 (30.9) 214 (31.9) 196 (28.5) 198 (30.0) 220 (30.9) 205 (30.6) 214 (31.6) 211 (32.2) 206 (31.8) 201 (31.9) 209 (31.6) 173 (26.6) AF, n (%) 0.0 0 7735 (77.3) 493 (77.4) 500 (75.9) 574 (79.3) 524 (79.8) 527 (78.5) 540 (78.6) 504 (76.2) 551 (77.5) 519 (77.6) 527 (77.7) 487 (74.4) 480 (74.1) 471 (74.6) 518 (78.2) 520 (80.0) 1.0 2265 (22.7) 144 (22.6) 159 (24.1) 150 (20.7) 133 (20.2) 144 (21.5) 147 (21.4) 157 (23.8) 160 (22.5) 150 (22.4) 151 (22.3) 168 (25.6) 168 (25.9) 160 (25.4) 144 (21.8) 130 (20.0) COPD, n (%) 0.0 0 7068 (70.7) 450 (70.6) 468 (71.0) 503 (69.5) 472 (71.8) 469 (69.9) 484 (70.5) 467 (70.7) 512 (72.0) 465 (69.5) 462 (68.1) 453 (69.2) 453 (69.9) 440 (69.7) 481 (72.7) 489 (75.2) 1.0 2932 (29.3) 187 (29.4) 191 (29.0) 221 (30.5) 185 (28.2) 202 (30.1) 203 (29.5) 194 (29.3) 199 (28.0) 204 (30.5) 216 (31.9) 202 (30.8) 195 (30.1) 191 (30.3) 181 (27.3) 161 (24.8) CRF, n (%) 0.0 0 7729 (77.3) 486 (76.3) 513 (77.8) 589 (81.4) 510 (77.6) 505 (75.3) 522 (76.0) 511 (77.3) 555 (78.1) 522 (78.0) 500 (73.7) 501 (76.5) 508 (78.4) 491 (77.8) 501 (75.7) 515 (79.2) 1.0 2271 (22.7) 151 (23.7) 146 (22.2) 135 (18.6) 147 (22.4) 166 (24.7) 165 (24.0) 150 (22.7) 156 (21.9) 147 (22.0) 178 (26.3) 154 (23.5) 140 (21.6) 140 (22.2) 161 (24.3) 135 (20.8) Returning_patient, n (%) 0 0 7048 (70.5) 448 (70.3) 473 (71.8) 505 (69.8) 464 (70.6) 489 (72.9) 503 (73.2) 456 (69.0) 477 (67.1) 470 (70.3) 480 (70.8) 464 (70.8) 452 (69.8) 458 (72.6) 456 (68.9) 453 (69.7) 1 2744 (27.4) 176 (27.6) 173 (26.3) 200 (27.6) 183 (27.9) 171 (25.5) 168 (24.5) 192 (29.0) 218 (30.7) 187 (28.0) 186 (27.4) 176 (26.9) 181 (27.9) 161 (25.5) 189 (28.5) 183 (28.2) 2 185 (1.8) 12 (1.9) 13 (2.0) 17 (2.3) 7 (1.1) 9 (1.3) 15 (2.2) 12 (1.8) 13 (1.8) 11 (1.6) 9 (1.3) 12 (1.8) 14 (2.2) 12 (1.9) 16 (2.4) 13 (2.0) 3 23 (0.2) 1 (0.2) 2 (0.3) 3 (0.5) 2 (0.3) 1 (0.1) 1 (0.2) 3 (0.4) 1 (0.1) 3 (0.4) 3 (0.5) 1 (0.2) 1 (0.2) 1 (0.2) Let's visualize the data. With the following figures we can see: (a) How many patients were hospitalized in total, how many were discharged\\died, stratified by year of admission. (b) Age distributions by sex, males (0) and females (1). (c) Number of patients at each number of admissions, with a separation to 4 groups. (d) Kaplan-Meier curves for LOS with and without death as censoring from pydts.examples_utils.plots import * plot_LOS_simulation_figure1 ( data_df ) Next, with the following figures we can further visualize the possible outcomes: (a) Description of the events (death and release). (b) Distribution of age, by sex, among the patients who died. (c-d) Number of observed event, by event type. plot_LOS_simulation_figure2 ( data_df ) and a visualization of the missingness of the weight variable by year of admission: plot_LOS_simulation_figure3 ( data_df ) Data Preprocessing \u00a4 outcome_cols = [ DISCHARGE_RELATIVE_COL , IN_HOSPITAL_DEATH_COL , DEATH_RELATIVE_COL , DEATH_MISSING_COL ] y = data_df . set_index ( PATIENT_NO_COL )[ outcome_cols ] X = data_df . set_index ( PATIENT_NO_COL ) . drop ( columns = outcome_cols ) X . drop ([ ADMISSION_SERIAL_COL , FIRST_ADMISSION_COL ], axis = 1 , inplace = True ) Missing Values Imputation \u00a4 We search for missing data and use median imputation: to_impute = X . isna () . sum ( axis = 0 ) . to_frame ( \"value\" ) . query ( \"value > 0\" ) . index to_impute Index(['Weight', 'BMI'], dtype='object') from sklearn.impute import SimpleImputer imputer = SimpleImputer ( verbose = 1 , strategy = 'median' ) X [ to_impute ] = imputer . fit_transform ( X [ to_impute ]) Standardization \u00a4 In some applications it is customize to standardize the covariates, such that each will be with the mean 0 and standard deviation of 1. For Height, Weight, Age and BMI columns we use Standard scaling, and for Returning Patient and Smoking we use Min-Max scaling: from sklearn.preprocessing import StandardScaler , MinMaxScaler to_normalize = [ HEIGHT_COL , WEIGHT_COL , AGE_COL , BMI_COL ] to_minmax = [ RETURNING_PATIENT_COL , SMOKING_COL ] std_scaler = StandardScaler () X [ to_normalize ] = std_scaler . fit_transform ( X [ to_normalize ]) minmax_scaler = MinMaxScaler () X [ to_minmax ] = minmax_scaler . fit_transform ( X [ to_minmax ]) X . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Gender Admyear Weight Height BMI Smoking Hypertension Diabetes AF COPD CRF Returning_patient ID 0.0 -0.501435 1.0 2001.0 0.001731 0.198481 -0.010699 0.5 0.0 0.0 0.0 1.0 0.0 0.0 1.0 -1.829968 1.0 2003.0 0.001731 0.255134 -0.010699 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.216468 1.0 2013.0 -0.706832 0.715064 -1.496554 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.356748 1.0 2008.0 0.001731 -0.738724 -0.010699 1.0 0.0 1.0 1.0 1.0 0.0 0.0 4.0 -0.509686 0.0 2011.0 0.001731 -0.610285 -0.010699 0.5 1.0 1.0 0.0 0.0 0.0 0.0 Creating event type and event time: In_hospital_death = 1 means in hospital death (J=1) In_hospital_death = 0 with Discharge_relative_date <= 30 means a discharge event (J=2) Discharge_relative_date = 31 means right censored example, i.e. (J=0 at T=30) y . loc [(( y . In_hospital_death == 0 ) & ( y . Discharge_relative_date != 31 )), IN_HOSPITAL_DEATH_COL ] = 2 y [ DISCHARGE_RELATIVE_COL ] = y [ DISCHARGE_RELATIVE_COL ] . clip ( upper = 30 ) . astype ( int ) y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Discharge_relative_date In_hospital_death Death_relative_date_in_hosp Death_date_in_hosp_missing ID 0.0 11 2 NaN 1 1.0 15 2 NaN 1 2.0 6 2 NaN 1 3.0 4 2 NaN 1 4.0 1 1 1.0 0 ... ... ... ... ... 9995.0 10 1 10.0 0 9996.0 5 1 5.0 0 9997.0 1 1 1.0 0 9998.0 13 2 NaN 1 9999.0 30 0 NaN 1 10000 rows \u00d7 4 columns Estimation \u00a4 Now we can estimate the parameters of the model using a TwoStagesFitter: fit_df = pd . concat ([ X . drop ( ADMISSION_YEAR_COL , axis = 1 ), y [[ IN_HOSPITAL_DEATH_COL , DISCHARGE_RELATIVE_COL ]]], axis = 1 ) . reset_index () fit_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Age Gender Weight Height BMI Smoking Hypertension Diabetes AF COPD CRF Returning_patient In_hospital_death Discharge_relative_date 0 0.0 -0.501435 1.0 0.001731 0.198481 -0.010699 0.5 0.0 0.0 0.0 1.0 0.0 0.0 2 11 1 1.0 -1.829968 1.0 0.001731 0.255134 -0.010699 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 15 2 2.0 0.216468 1.0 -0.706832 0.715064 -1.496554 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 6 3 3.0 0.356748 1.0 0.001731 -0.738724 -0.010699 1.0 0.0 1.0 1.0 1.0 0.0 0.0 2 4 4 4.0 -0.509686 0.0 0.001731 -0.610285 -0.010699 0.5 1.0 1.0 0.0 0.0 0.0 0.0 1 1 from pydts.fitters import TwoStagesFitter fitter = TwoStagesFitter () fitter . fit ( df = fit_df , event_type_col = IN_HOSPITAL_DEATH_COL , duration_col = DISCHARGE_RELATIVE_COL , pid_col = PATIENT_NO_COL ) fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Age 0.012234 0.021289 0.018675 0.013089 Gender 0.072557 0.048761 0.032534 0.029895 Weight -0.085851 0.039237 -0.096352 0.024137 Height -0.109585 0.028974 -0.057043 0.017838 BMI -0.113686 0.038058 -0.069863 0.023324 Smoking 0.006120 0.060227 -0.086379 0.037199 Hypertension -0.066527 0.052036 0.021493 0.031215 Diabetes 0.045579 0.045944 0.034855 0.028225 AF 0.042571 0.051099 -0.005837 0.031592 COPD 0.030174 0.049191 0.058040 0.030069 CRF 0.024396 0.051150 0.039778 0.031179 Returning_patient -0.097966 0.122227 0.122055 0.073013 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt In_hospital_death Discharge_relative_date 1 1 212 True -3.901826 2 184 True -3.950053 3 174 True -3.925112 4 138 True -4.085360 5 139 True -3.992844 6 149 True -3.848771 7 128 True -3.922036 8 120 True -3.908204 9 81 True -4.240269 10 95 True -4.018168 11 82 True -4.095930 12 88 True -3.938631 13 74 True -4.050635 14 65 True -4.108051 15 60 True -4.120188 16 43 True -4.377010 17 56 True -4.067527 18 48 True -4.157987 19 52 True -4.023369 20 49 True -4.016882 21 37 True -4.229897 22 35 True -4.222299 23 42 True -3.979030 24 38 True -4.007955 25 29 True -4.217928 26 36 True -3.923879 27 33 True -3.954917 28 30 True -3.978909 29 27 True -4.023108 30 18 True -4.347235 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt In_hospital_death Discharge_relative_date 2 1 628 True -2.755714 2 515 True -2.869952 3 466 True -2.889717 4 383 True -3.012315 5 393 True -2.908478 6 330 True -3.011169 7 332 True -2.923762 8 251 True -3.136989 9 242 True -3.104749 10 232 True -3.082277 11 247 True -2.942857 12 215 True -3.012719 13 209 True -2.966362 14 174 True -3.083501 15 163 True -3.082805 16 145 True -3.138006 17 134 True -3.160356 18 111 True -3.290827 19 123 True -3.127649 20 114 True -3.138881 21 112 True -3.090689 22 105 True -3.093272 23 107 True -3.007764 24 77 True -3.276149 25 99 True -2.956023 26 82 True -3.082226 27 80 True -3.037766 28 59 True -3.280251 29 73 True -2.997231 30 69 True -2.985297 fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Age 0.012234 0.021289 0.018675 0.013089 Gender 0.072557 0.048761 0.032534 0.029895 Weight -0.085851 0.039237 -0.096352 0.024137 Height -0.109585 0.028974 -0.057043 0.017838 BMI -0.113686 0.038058 -0.069863 0.023324 Smoking 0.006120 0.060227 -0.086379 0.037199 Hypertension -0.066527 0.052036 0.021493 0.031215 Diabetes 0.045579 0.045944 0.034855 0.028225 AF 0.042571 0.051099 -0.005837 0.031592 COPD 0.030174 0.049191 0.058040 0.030069 CRF 0.024396 0.051150 0.039778 0.031179 Returning_patient -0.097966 0.122227 0.122055 0.073013 from pydts.examples_utils.plots import add_panel_text fig , axes = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax = axes [ 0 ] fitter . plot_all_events_alpha ( ax = ax , show = False ) ax . grid () ax . legend ( fontsize = 16 , loc = 'center right' ) add_panel_text ( ax , 'a' ) ax = axes [ 1 ] fitter . plot_all_events_beta ( ax = ax , show = False , xlabel = 'Value' ) ax . legend ( fontsize = 16 , loc = 'center right' ) add_panel_text ( ax , 'b' ) fig . tight_layout () Prediction \u00a4 pred_df = fitter . predict_cumulative_incident_function ( fit_df . iloc [ 2 : 5 ]) . T pred_df = pred_df . iloc [ 1 :] pred_df . columns = [ 'ID=2' , 'ID=3' , 'ID=4' ] pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=2 ID=3 ID=4 Age 0.216468 0.356748 -0.509686 Gender 1.000000 1.000000 0.000000 Weight -0.706832 0.001731 0.001731 Height 0.715064 -0.738724 -0.610285 BMI -1.496554 -0.010699 -0.010699 plot_example_pred_output ( pred_df )","title":"Hospitalization LOS Simulation"},{"location":"SimulatedDataset/#hospitalization-length-of-stay","text":"In this example, we simulate a dataset of 10,000 hospitalized patietns as follows: We sample the covariates such that: \\(\\mbox{Admission Year} \\sim \\mbox{Uniform}(2000,2014)\\) \\(\\mbox{Gender} \\sim \\mbox{Bernoullie}(0.5), \\qquad\\) (1 is Female, 0 is Male) \\(\\mbox{Age} \\sim \\mbox{Normal}(72+5*\\mbox{gender}\\;,\\;12)\\) (years) \\(\\mbox{Height} \\sim \\mbox{Normal}(175-5*\\mbox{gender}\\;,\\;7)\\) (cm) \\(\\mbox{Weight} \\sim \\mbox{Normal}(\\frac{\\mbox{height}}{175}*80 - 5 * \\mbox{gender} + \\frac{\\mbox{age}}{20}\\;,\\;8)\\) (kg) \\(\\mbox{BMI} \\: (\\mbox{Body} \\: \\mbox{Mass} \\: \\mbox{Index}) = \\frac{\\mbox{Weight}}{(\\frac{\\mbox{Height}}{100})^2}\\) (kg/m^2) \\(\\mbox{Admission Serial} \\sim \\mbox{LogNormal}(0, 0.75)\\) \\(\\mbox{Smoking Status} \\sim \\mbox{Multinomial(No, Previously, Currently)} \\quad p=[0.5, 0.3, 0.2]\\) Precondition features: \\(\\mbox{General_p} = 0.003 * \\mbox{bmi} - 0.15 * \\mbox{gender} + 0.002 * \\mbox{age} + 0.1 * \\mbox{smoking}\\) \\(\\mbox{Preconditions_p} = max( min(\\mbox{General_p}, 0.65), 0.05)\\) \\(\\mbox{Hypertension} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\) \\(\\mbox{Diabetes} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p} + 0.003*\\mbox{BMI})\\) \\(\\mbox{Arterial Fibrillation} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\) \\(\\mbox{Chronic Obstructive Pulmonary Disease} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p} + 0.1*\\mbox{smoking})\\) \\(\\mbox{Chronic Renal Failure} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\) Finally, based on the above covariates, we sample LOS and the event type: discharged or in-hospital death. After sampling the LOS, for some patients we remove weight (and BMI) information based on year of admission, to reflect missingness which can occur in real world data.","title":"Hospitalization length of stay"},{"location":"SimulatedDataset/#loading-simulation-dataset","text":"import pandas as pd import numpy as np from matplotlib import pyplot as plt % matplotlib inline pd . set_option ( 'display.max_rows' , 500 ) from pydts.examples_utils.simulations_data_config import * from pydts.examples_utils.datasets import load_LOS_simulated_data import warnings warnings . filterwarnings ( 'ignore' ) warnings . simplefilter ( 'ignore' ) data_df = load_LOS_simulated_data ()","title":"Loading Simulation Dataset"},{"location":"SimulatedDataset/#data-description","text":"First, let's look at some descriptive statistics of the columns: paper_table = data_df . describe () . T . round ( 2 ) idx = [ r for r in paper_table . index if r not in [ 'ID' , 'Returning_patient' , 'Death_date_in_hosp_missing' , 'In_hospital_death' ,]] paper_table = paper_table . loc [ idx , :] paper_table . rename ({ 'Age' : 'Age (years)' , 'Gender' : 'Gender (1 - female)' , 'Admyear' : 'Admyear (year)' , 'Firstadm' : 'Firstadm (1 - yes)' , 'Admserial' : 'Admserial' , 'Weight' : 'Weight (kg)' , 'Height' : 'Height (cm)' , 'BMI' : 'Height (kg/m^2)' , 'Smoking' : f \"Smoking (0 - never, \\\\ \\qquad \\qquad \\, 1 - previously, \\\\ \\qquad \\qquad \\, 2 - currently)\" , 'Discharge_relative_date' : 'LOS (days)' , 'Hypertension' : 'Hypertension (1 - yes)' , 'Diabities' : 'Diabetes (1 - yes)' , 'AF' : 'AF (1 - yes)' , 'COPD' : 'COPD (1 - yes)' , 'CRF' : 'CRF (1 - yes)' , 'Death_relative_date_in_hosp' : 'In-Hospital Death (days)' }, inplace = True ) paper_table [ 'count' ] = paper_table [ 'count' ] . astype ( int ) paper_table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max Age (years) 10000 74.38 12.12 24.10 66.30 74.40 82.60 122.70 Gender (1 - female) 10000 0.49 0.50 0.00 0.00 0.00 1.00 1.00 Admyear (year) 10000 2006.96 4.29 2000.00 2003.00 2007.00 2011.00 2014.00 Firstadm (1 - yes) 10000 0.70 0.46 0.00 0.00 1.00 1.00 1.00 Admserial 10000 1.50 1.03 1.00 1.00 1.00 2.00 17.00 Weight (kg) 4497 80.05 9.40 46.92 73.66 80.08 86.41 116.46 Height (cm) 10000 172.47 7.41 145.68 167.50 172.41 177.49 201.24 Height (kg/m^2) 4497 26.94 2.97 15.16 24.86 26.89 28.85 40.35 Smoking (0 - never, \\ \\qquad \\qquad \\, 1 - previously, \\ \\qquad \\qquad \\, 2 - currently) 10000 0.70 0.78 0.00 0.00 1.00 1.00 2.00 Hypertension (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 Diabetes 10000 0.31 0.46 0.00 0.00 0.00 1.00 1.00 AF (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 COPD (1 - yes) 10000 0.29 0.46 0.00 0.00 0.00 1.00 1.00 CRF (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 LOS (days) 10000 12.96 10.26 1.00 4.00 10.00 21.00 31.00 In-Hospital Death (days) 2362 10.21 7.86 1.00 4.00 8.00 15.00 30.00 When dealing with healthcare data, changes of policy can lead to biased data, so it is also a good idea to see how the data looks like with stratification by year of admission: import tableone as to columns = [ AGE_COL , GENDER_COL , WEIGHT_COL , HEIGHT_COL , BMI_COL , * preconditions , RETURNING_PATIENT_COL ] categorical = [ GENDER_COL , * preconditions , RETURNING_PATIENT_COL ] groupby = ADMISSION_YEAR_COL mytable = to . TableOne ( data_df , columns , categorical , groupby ) mytable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } Grouped by Admyear Missing Overall 2000.0 2001.0 2002.0 2003.0 2004.0 2005.0 2006.0 2007.0 2008.0 2009.0 2010.0 2011.0 2012.0 2013.0 2014.0 n 10000 637 659 724 657 671 687 661 711 669 678 655 648 631 662 650 Age, mean (SD) 0 74.4 (12.1) 73.8 (12.4) 74.3 (11.8) 74.7 (12.1) 74.8 (12.0) 74.3 (11.7) 73.9 (11.9) 74.5 (13.1) 73.7 (11.9) 74.8 (12.0) 74.8 (12.1) 75.6 (11.6) 73.8 (12.4) 74.6 (11.9) 73.8 (12.5) 74.2 (12.1) Gender, n (%) 0.0 0 5064 (50.6) 307 (48.2) 349 (53.0) 362 (50.0) 317 (48.2) 346 (51.6) 339 (49.3) 350 (53.0) 367 (51.6) 327 (48.9) 355 (52.4) 343 (52.4) 331 (51.1) 318 (50.4) 332 (50.2) 321 (49.4) 1.0 4936 (49.4) 330 (51.8) 310 (47.0) 362 (50.0) 340 (51.8) 325 (48.4) 348 (50.7) 311 (47.0) 344 (48.4) 342 (51.1) 323 (47.6) 312 (47.6) 317 (48.9) 313 (49.6) 330 (49.8) 329 (50.6) Weight, mean (SD) 5503 80.1 (9.4) 78.4 (10.5) 80.1 (11.6) 81.2 (9.9) 79.8 (9.7) 78.1 (9.6) 80.0 (9.7) 79.8 (9.2) 80.7 (8.7) 80.9 (9.4) 80.9 (9.4) 79.9 (9.2) 80.4 (9.3) 79.6 (9.4) 79.8 (9.7) 79.8 (8.9) Height, mean (SD) 0 172.5 (7.4) 172.7 (7.5) 172.4 (7.8) 172.4 (7.4) 172.4 (7.1) 172.3 (7.3) 172.1 (7.5) 172.5 (7.5) 172.7 (7.2) 172.4 (7.3) 172.9 (7.6) 172.6 (7.5) 172.7 (7.5) 172.3 (7.1) 172.5 (7.7) 172.3 (7.3) BMI, mean (SD) 5503 26.9 (3.0) 26.7 (3.4) 27.0 (3.1) 27.0 (3.0) 26.7 (2.9) 26.4 (3.1) 26.9 (3.2) 26.9 (3.0) 27.2 (2.8) 27.2 (3.1) 27.2 (3.0) 26.9 (2.9) 26.9 (3.0) 26.9 (2.9) 26.9 (3.0) 26.9 (2.9) Smoking, n (%) 0.0 0 4982 (49.8) 323 (50.7) 349 (53.0) 371 (51.2) 325 (49.5) 332 (49.5) 334 (48.6) 325 (49.2) 352 (49.5) 319 (47.7) 336 (49.6) 310 (47.3) 305 (47.1) 327 (51.8) 322 (48.6) 352 (54.2) 1.0 3033 (30.3) 185 (29.0) 177 (26.9) 228 (31.5) 193 (29.4) 195 (29.1) 220 (32.0) 193 (29.2) 214 (30.1) 221 (33.0) 198 (29.2) 223 (34.0) 211 (32.6) 174 (27.6) 206 (31.1) 195 (30.0) 2.0 1985 (19.9) 129 (20.3) 133 (20.2) 125 (17.3) 139 (21.2) 144 (21.5) 133 (19.4) 143 (21.6) 145 (20.4) 129 (19.3) 144 (21.2) 122 (18.6) 132 (20.4) 130 (20.6) 134 (20.2) 103 (15.8) Hypertension, n (%) 0.0 0 7717 (77.2) 505 (79.3) 497 (75.4) 545 (75.3) 523 (79.6) 517 (77.0) 552 (80.3) 482 (72.9) 551 (77.5) 519 (77.6) 520 (76.7) 523 (79.8) 488 (75.3) 478 (75.8) 505 (76.3) 512 (78.8) 1.0 2283 (22.8) 132 (20.7) 162 (24.6) 179 (24.7) 134 (20.4) 154 (23.0) 135 (19.7) 179 (27.1) 160 (22.5) 150 (22.4) 158 (23.3) 132 (20.2) 160 (24.7) 153 (24.2) 157 (23.7) 138 (21.2) Diabetes, n (%) 0.0 0 6932 (69.3) 446 (70.0) 444 (67.4) 512 (70.7) 454 (69.1) 457 (68.1) 491 (71.5) 463 (70.0) 491 (69.1) 464 (69.4) 464 (68.4) 444 (67.8) 442 (68.2) 430 (68.1) 453 (68.4) 477 (73.4) 1.0 3068 (30.7) 191 (30.0) 215 (32.6) 212 (29.3) 203 (30.9) 214 (31.9) 196 (28.5) 198 (30.0) 220 (30.9) 205 (30.6) 214 (31.6) 211 (32.2) 206 (31.8) 201 (31.9) 209 (31.6) 173 (26.6) AF, n (%) 0.0 0 7735 (77.3) 493 (77.4) 500 (75.9) 574 (79.3) 524 (79.8) 527 (78.5) 540 (78.6) 504 (76.2) 551 (77.5) 519 (77.6) 527 (77.7) 487 (74.4) 480 (74.1) 471 (74.6) 518 (78.2) 520 (80.0) 1.0 2265 (22.7) 144 (22.6) 159 (24.1) 150 (20.7) 133 (20.2) 144 (21.5) 147 (21.4) 157 (23.8) 160 (22.5) 150 (22.4) 151 (22.3) 168 (25.6) 168 (25.9) 160 (25.4) 144 (21.8) 130 (20.0) COPD, n (%) 0.0 0 7068 (70.7) 450 (70.6) 468 (71.0) 503 (69.5) 472 (71.8) 469 (69.9) 484 (70.5) 467 (70.7) 512 (72.0) 465 (69.5) 462 (68.1) 453 (69.2) 453 (69.9) 440 (69.7) 481 (72.7) 489 (75.2) 1.0 2932 (29.3) 187 (29.4) 191 (29.0) 221 (30.5) 185 (28.2) 202 (30.1) 203 (29.5) 194 (29.3) 199 (28.0) 204 (30.5) 216 (31.9) 202 (30.8) 195 (30.1) 191 (30.3) 181 (27.3) 161 (24.8) CRF, n (%) 0.0 0 7729 (77.3) 486 (76.3) 513 (77.8) 589 (81.4) 510 (77.6) 505 (75.3) 522 (76.0) 511 (77.3) 555 (78.1) 522 (78.0) 500 (73.7) 501 (76.5) 508 (78.4) 491 (77.8) 501 (75.7) 515 (79.2) 1.0 2271 (22.7) 151 (23.7) 146 (22.2) 135 (18.6) 147 (22.4) 166 (24.7) 165 (24.0) 150 (22.7) 156 (21.9) 147 (22.0) 178 (26.3) 154 (23.5) 140 (21.6) 140 (22.2) 161 (24.3) 135 (20.8) Returning_patient, n (%) 0 0 7048 (70.5) 448 (70.3) 473 (71.8) 505 (69.8) 464 (70.6) 489 (72.9) 503 (73.2) 456 (69.0) 477 (67.1) 470 (70.3) 480 (70.8) 464 (70.8) 452 (69.8) 458 (72.6) 456 (68.9) 453 (69.7) 1 2744 (27.4) 176 (27.6) 173 (26.3) 200 (27.6) 183 (27.9) 171 (25.5) 168 (24.5) 192 (29.0) 218 (30.7) 187 (28.0) 186 (27.4) 176 (26.9) 181 (27.9) 161 (25.5) 189 (28.5) 183 (28.2) 2 185 (1.8) 12 (1.9) 13 (2.0) 17 (2.3) 7 (1.1) 9 (1.3) 15 (2.2) 12 (1.8) 13 (1.8) 11 (1.6) 9 (1.3) 12 (1.8) 14 (2.2) 12 (1.9) 16 (2.4) 13 (2.0) 3 23 (0.2) 1 (0.2) 2 (0.3) 3 (0.5) 2 (0.3) 1 (0.1) 1 (0.2) 3 (0.4) 1 (0.1) 3 (0.4) 3 (0.5) 1 (0.2) 1 (0.2) 1 (0.2) Let's visualize the data. With the following figures we can see: (a) How many patients were hospitalized in total, how many were discharged\\died, stratified by year of admission. (b) Age distributions by sex, males (0) and females (1). (c) Number of patients at each number of admissions, with a separation to 4 groups. (d) Kaplan-Meier curves for LOS with and without death as censoring from pydts.examples_utils.plots import * plot_LOS_simulation_figure1 ( data_df ) Next, with the following figures we can further visualize the possible outcomes: (a) Description of the events (death and release). (b) Distribution of age, by sex, among the patients who died. (c-d) Number of observed event, by event type. plot_LOS_simulation_figure2 ( data_df ) and a visualization of the missingness of the weight variable by year of admission: plot_LOS_simulation_figure3 ( data_df )","title":"Data Description"},{"location":"SimulatedDataset/#data-preprocessing","text":"outcome_cols = [ DISCHARGE_RELATIVE_COL , IN_HOSPITAL_DEATH_COL , DEATH_RELATIVE_COL , DEATH_MISSING_COL ] y = data_df . set_index ( PATIENT_NO_COL )[ outcome_cols ] X = data_df . set_index ( PATIENT_NO_COL ) . drop ( columns = outcome_cols ) X . drop ([ ADMISSION_SERIAL_COL , FIRST_ADMISSION_COL ], axis = 1 , inplace = True )","title":"Data Preprocessing"},{"location":"SimulatedDataset/#missing-values-imputation","text":"We search for missing data and use median imputation: to_impute = X . isna () . sum ( axis = 0 ) . to_frame ( \"value\" ) . query ( \"value > 0\" ) . index to_impute Index(['Weight', 'BMI'], dtype='object') from sklearn.impute import SimpleImputer imputer = SimpleImputer ( verbose = 1 , strategy = 'median' ) X [ to_impute ] = imputer . fit_transform ( X [ to_impute ])","title":"Missing Values Imputation"},{"location":"SimulatedDataset/#standardization","text":"In some applications it is customize to standardize the covariates, such that each will be with the mean 0 and standard deviation of 1. For Height, Weight, Age and BMI columns we use Standard scaling, and for Returning Patient and Smoking we use Min-Max scaling: from sklearn.preprocessing import StandardScaler , MinMaxScaler to_normalize = [ HEIGHT_COL , WEIGHT_COL , AGE_COL , BMI_COL ] to_minmax = [ RETURNING_PATIENT_COL , SMOKING_COL ] std_scaler = StandardScaler () X [ to_normalize ] = std_scaler . fit_transform ( X [ to_normalize ]) minmax_scaler = MinMaxScaler () X [ to_minmax ] = minmax_scaler . fit_transform ( X [ to_minmax ]) X . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Gender Admyear Weight Height BMI Smoking Hypertension Diabetes AF COPD CRF Returning_patient ID 0.0 -0.501435 1.0 2001.0 0.001731 0.198481 -0.010699 0.5 0.0 0.0 0.0 1.0 0.0 0.0 1.0 -1.829968 1.0 2003.0 0.001731 0.255134 -0.010699 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.216468 1.0 2013.0 -0.706832 0.715064 -1.496554 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.356748 1.0 2008.0 0.001731 -0.738724 -0.010699 1.0 0.0 1.0 1.0 1.0 0.0 0.0 4.0 -0.509686 0.0 2011.0 0.001731 -0.610285 -0.010699 0.5 1.0 1.0 0.0 0.0 0.0 0.0 Creating event type and event time: In_hospital_death = 1 means in hospital death (J=1) In_hospital_death = 0 with Discharge_relative_date <= 30 means a discharge event (J=2) Discharge_relative_date = 31 means right censored example, i.e. (J=0 at T=30) y . loc [(( y . In_hospital_death == 0 ) & ( y . Discharge_relative_date != 31 )), IN_HOSPITAL_DEATH_COL ] = 2 y [ DISCHARGE_RELATIVE_COL ] = y [ DISCHARGE_RELATIVE_COL ] . clip ( upper = 30 ) . astype ( int ) y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Discharge_relative_date In_hospital_death Death_relative_date_in_hosp Death_date_in_hosp_missing ID 0.0 11 2 NaN 1 1.0 15 2 NaN 1 2.0 6 2 NaN 1 3.0 4 2 NaN 1 4.0 1 1 1.0 0 ... ... ... ... ... 9995.0 10 1 10.0 0 9996.0 5 1 5.0 0 9997.0 1 1 1.0 0 9998.0 13 2 NaN 1 9999.0 30 0 NaN 1 10000 rows \u00d7 4 columns","title":"Standardization"},{"location":"SimulatedDataset/#estimation","text":"Now we can estimate the parameters of the model using a TwoStagesFitter: fit_df = pd . concat ([ X . drop ( ADMISSION_YEAR_COL , axis = 1 ), y [[ IN_HOSPITAL_DEATH_COL , DISCHARGE_RELATIVE_COL ]]], axis = 1 ) . reset_index () fit_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Age Gender Weight Height BMI Smoking Hypertension Diabetes AF COPD CRF Returning_patient In_hospital_death Discharge_relative_date 0 0.0 -0.501435 1.0 0.001731 0.198481 -0.010699 0.5 0.0 0.0 0.0 1.0 0.0 0.0 2 11 1 1.0 -1.829968 1.0 0.001731 0.255134 -0.010699 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 15 2 2.0 0.216468 1.0 -0.706832 0.715064 -1.496554 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 6 3 3.0 0.356748 1.0 0.001731 -0.738724 -0.010699 1.0 0.0 1.0 1.0 1.0 0.0 0.0 2 4 4 4.0 -0.509686 0.0 0.001731 -0.610285 -0.010699 0.5 1.0 1.0 0.0 0.0 0.0 0.0 1 1 from pydts.fitters import TwoStagesFitter fitter = TwoStagesFitter () fitter . fit ( df = fit_df , event_type_col = IN_HOSPITAL_DEATH_COL , duration_col = DISCHARGE_RELATIVE_COL , pid_col = PATIENT_NO_COL ) fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Age 0.012234 0.021289 0.018675 0.013089 Gender 0.072557 0.048761 0.032534 0.029895 Weight -0.085851 0.039237 -0.096352 0.024137 Height -0.109585 0.028974 -0.057043 0.017838 BMI -0.113686 0.038058 -0.069863 0.023324 Smoking 0.006120 0.060227 -0.086379 0.037199 Hypertension -0.066527 0.052036 0.021493 0.031215 Diabetes 0.045579 0.045944 0.034855 0.028225 AF 0.042571 0.051099 -0.005837 0.031592 COPD 0.030174 0.049191 0.058040 0.030069 CRF 0.024396 0.051150 0.039778 0.031179 Returning_patient -0.097966 0.122227 0.122055 0.073013 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt In_hospital_death Discharge_relative_date 1 1 212 True -3.901826 2 184 True -3.950053 3 174 True -3.925112 4 138 True -4.085360 5 139 True -3.992844 6 149 True -3.848771 7 128 True -3.922036 8 120 True -3.908204 9 81 True -4.240269 10 95 True -4.018168 11 82 True -4.095930 12 88 True -3.938631 13 74 True -4.050635 14 65 True -4.108051 15 60 True -4.120188 16 43 True -4.377010 17 56 True -4.067527 18 48 True -4.157987 19 52 True -4.023369 20 49 True -4.016882 21 37 True -4.229897 22 35 True -4.222299 23 42 True -3.979030 24 38 True -4.007955 25 29 True -4.217928 26 36 True -3.923879 27 33 True -3.954917 28 30 True -3.978909 29 27 True -4.023108 30 18 True -4.347235 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt In_hospital_death Discharge_relative_date 2 1 628 True -2.755714 2 515 True -2.869952 3 466 True -2.889717 4 383 True -3.012315 5 393 True -2.908478 6 330 True -3.011169 7 332 True -2.923762 8 251 True -3.136989 9 242 True -3.104749 10 232 True -3.082277 11 247 True -2.942857 12 215 True -3.012719 13 209 True -2.966362 14 174 True -3.083501 15 163 True -3.082805 16 145 True -3.138006 17 134 True -3.160356 18 111 True -3.290827 19 123 True -3.127649 20 114 True -3.138881 21 112 True -3.090689 22 105 True -3.093272 23 107 True -3.007764 24 77 True -3.276149 25 99 True -2.956023 26 82 True -3.082226 27 80 True -3.037766 28 59 True -3.280251 29 73 True -2.997231 30 69 True -2.985297 fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Age 0.012234 0.021289 0.018675 0.013089 Gender 0.072557 0.048761 0.032534 0.029895 Weight -0.085851 0.039237 -0.096352 0.024137 Height -0.109585 0.028974 -0.057043 0.017838 BMI -0.113686 0.038058 -0.069863 0.023324 Smoking 0.006120 0.060227 -0.086379 0.037199 Hypertension -0.066527 0.052036 0.021493 0.031215 Diabetes 0.045579 0.045944 0.034855 0.028225 AF 0.042571 0.051099 -0.005837 0.031592 COPD 0.030174 0.049191 0.058040 0.030069 CRF 0.024396 0.051150 0.039778 0.031179 Returning_patient -0.097966 0.122227 0.122055 0.073013 from pydts.examples_utils.plots import add_panel_text fig , axes = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax = axes [ 0 ] fitter . plot_all_events_alpha ( ax = ax , show = False ) ax . grid () ax . legend ( fontsize = 16 , loc = 'center right' ) add_panel_text ( ax , 'a' ) ax = axes [ 1 ] fitter . plot_all_events_beta ( ax = ax , show = False , xlabel = 'Value' ) ax . legend ( fontsize = 16 , loc = 'center right' ) add_panel_text ( ax , 'b' ) fig . tight_layout ()","title":"Estimation"},{"location":"SimulatedDataset/#prediction","text":"pred_df = fitter . predict_cumulative_incident_function ( fit_df . iloc [ 2 : 5 ]) . T pred_df = pred_df . iloc [ 1 :] pred_df . columns = [ 'ID=2' , 'ID=3' , 'ID=4' ] pred_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=2 ID=3 ID=4 Age 0.216468 0.356748 -0.509686 Gender 1.000000 1.000000 0.000000 Weight -0.706832 0.001731 0.001731 Height 0.715064 -0.738724 -0.610285 BMI -1.496554 -0.010699 -0.010699 plot_example_pred_output ( pred_df )","title":"Prediction"},{"location":"UsageExample-DataPreparation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Data Preparation \u00a4 Data Generation \u00a4 For simplicity of presentation, we considered \\(M=2\\) competing events, though PyDTS can handle any number of competing events as long as there are enough observed failures of each failure type, at each discrete time point. Here, \\(d=30\\) discrete time points, \\(n=50,000\\) observations, and \\(Z\\) with 5 covariates. Failure times of observations were generated based on the model: \\[ \\lambda_{j}(t|Z) = \\frac{\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})} \\] with \\(\\alpha_{1t} = -1 -0.3 \\log(t)\\) , \\(\\alpha_{2t} = -1.75 -0.15\\log(t)\\) , \\(t=1,\\ldots,d\\) , \\(\\beta_1 = (-\\log 0.8, \\log 3, \\log 3, \\log 2.5, \\log 2)\\) , \\(\\beta_{2} = (-\\log 1, \\log 3, \\log 4, \\log 3, \\log 2)\\) . Censoring time for each observation was sampled from a discrete uniform distribution, i.e. \\(C_i \\sim \\mbox{Uniform}\\{1,...,d+1\\}\\) . Our goal is estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T,\\alpha_{21},\\ldots,\\alpha_{2d},\\beta_2^T\\}\\) (70 parameters in total) along with the standard error of the estimators. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 Checking the Data \u00a4 Both estimation methods require enough observed failures of each failure type, at each discrete time point. Therefore, the first step is to make sure this is in fact the case with the data at hand. As shown below, in our example, the data comply with this requirement. Preprocessing suggestions for cases when the data do not comply with this requirement are shown in Data Regrouping Example. patients_df . groupby ([ 'J' , 'X' ])[ 'pid' ] . count () . unstack ( 'J' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } J 0 1 2 X 1 1236 3374 1250 2 1124 2328 839 3 1029 1805 805 4 972 1524 644 5 939 1214 570 6 889 1114 483 7 830 916 416 8 832 830 409 9 797 683 323 10 685 626 306 11 703 569 240 12 648 516 246 13 679 419 226 14 647 410 198 15 603 326 170 16 601 320 162 17 585 280 147 18 564 240 115 19 505 243 125 20 465 204 118 21 488 176 83 22 465 167 89 23 497 166 65 24 457 118 59 25 440 114 58 26 427 109 53 27 430 89 43 28 396 70 38 29 398 67 43 30 3245 47 37","title":"Data Preparation"},{"location":"UsageExample-DataPreparation/#data-preparation","text":"","title":"Data Preparation"},{"location":"UsageExample-DataPreparation/#data-generation","text":"For simplicity of presentation, we considered \\(M=2\\) competing events, though PyDTS can handle any number of competing events as long as there are enough observed failures of each failure type, at each discrete time point. Here, \\(d=30\\) discrete time points, \\(n=50,000\\) observations, and \\(Z\\) with 5 covariates. Failure times of observations were generated based on the model: \\[ \\lambda_{j}(t|Z) = \\frac{\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})} \\] with \\(\\alpha_{1t} = -1 -0.3 \\log(t)\\) , \\(\\alpha_{2t} = -1.75 -0.15\\log(t)\\) , \\(t=1,\\ldots,d\\) , \\(\\beta_1 = (-\\log 0.8, \\log 3, \\log 3, \\log 2.5, \\log 2)\\) , \\(\\beta_{2} = (-\\log 1, \\log 3, \\log 4, \\log 3, \\log 2)\\) . Censoring time for each observation was sampled from a discrete uniform distribution, i.e. \\(C_i \\sim \\mbox{Uniform}\\{1,...,d+1\\}\\) . Our goal is estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T,\\alpha_{21},\\ldots,\\alpha_{2d},\\beta_2^T\\}\\) (70 parameters in total) along with the standard error of the estimators. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14","title":"Data Generation"},{"location":"UsageExample-DataPreparation/#checking-the-data","text":"Both estimation methods require enough observed failures of each failure type, at each discrete time point. Therefore, the first step is to make sure this is in fact the case with the data at hand. As shown below, in our example, the data comply with this requirement. Preprocessing suggestions for cases when the data do not comply with this requirement are shown in Data Regrouping Example. patients_df . groupby ([ 'J' , 'X' ])[ 'pid' ] . count () . unstack ( 'J' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } J 0 1 2 X 1 1236 3374 1250 2 1124 2328 839 3 1029 1805 805 4 972 1524 644 5 939 1214 570 6 889 1114 483 7 830 916 416 8 832 830 409 9 797 683 323 10 685 626 306 11 703 569 240 12 648 516 246 13 679 419 226 14 647 410 198 15 603 326 170 16 601 320 162 17 585 280 147 18 564 240 115 19 505 243 125 20 465 204 118 21 488 176 83 22 465 167 89 23 497 166 65 24 457 118 59 25 440 114 58 26 427 109 53 27 430 89 43 28 396 70 38 29 398 67 43 30 3245 47 37","title":"Checking the Data"},{"location":"UsageExample-FittingDataExpansionFitter-FULL/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Estimating with DataExpansionFitter \u00a4 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.examples_utils.plots import plot_events_occurrence , plot_example_pred_output import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 plot_events_occurrence ( patients_df [[ 'pid' , 'J' , 'X' ]]) <AxesSubplot:xlabel='Time', ylabel='Number of Observations'> Estimation \u00a4 In the following we apply the estimation method of Lee et al. (2018). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Model summary for event: 1 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_1 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -78272. Date: Tue, 02 Aug 2022 Deviance: 1.5654e+05 Time: 16:47:21 Pearson chi2: 5.35e+05 No. Iterations: 7 Pseudo R-squ. (CS): 0.01509 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -0.9459 0.033 -28.924 0.000 -1.010 -0.882 C(X)[2] -1.1780 0.035 -33.675 0.000 -1.247 -1.109 C(X)[3] -1.3158 0.037 -35.614 0.000 -1.388 -1.243 C(X)[4] -1.3671 0.039 -35.452 0.000 -1.443 -1.291 C(X)[5] -1.4895 0.041 -36.429 0.000 -1.570 -1.409 C(X)[6] -1.4702 0.042 -35.004 0.000 -1.553 -1.388 C(X)[7] -1.5688 0.044 -35.325 0.000 -1.656 -1.482 C(X)[8] -1.5724 0.046 -34.301 0.000 -1.662 -1.483 C(X)[9] -1.6733 0.049 -34.334 0.000 -1.769 -1.578 C(X)[10] -1.6693 0.050 -33.240 0.000 -1.768 -1.571 C(X)[11] -1.6748 0.052 -32.246 0.000 -1.777 -1.573 C(X)[12] -1.6825 0.054 -31.287 0.000 -1.788 -1.577 C(X)[13] -1.8026 0.058 -31.121 0.000 -1.916 -1.689 C(X)[14] -1.7319 0.058 -29.610 0.000 -1.847 -1.617 C(X)[15] -1.8695 0.064 -29.319 0.000 -1.994 -1.745 C(X)[16] -1.7987 0.064 -27.960 0.000 -1.925 -1.673 C(X)[17] -1.8400 0.068 -27.122 0.000 -1.973 -1.707 C(X)[18] -1.9016 0.072 -26.333 0.000 -2.043 -1.760 C(X)[19] -1.7936 0.072 -24.918 0.000 -1.935 -1.653 C(X)[20] -1.8749 0.077 -24.232 0.000 -2.027 -1.723 C(X)[21] -1.9294 0.082 -23.424 0.000 -2.091 -1.768 C(X)[22] -1.8858 0.084 -22.362 0.000 -2.051 -1.721 C(X)[23] -1.7888 0.085 -21.123 0.000 -1.955 -1.623 C(X)[24] -2.0205 0.098 -20.568 0.000 -2.213 -1.828 C(X)[25] -1.9474 0.100 -19.500 0.000 -2.143 -1.752 C(X)[26] -1.8743 0.102 -18.373 0.000 -2.074 -1.674 C(X)[27] -1.9588 0.112 -17.518 0.000 -2.178 -1.740 C(X)[28] -2.0736 0.125 -16.608 0.000 -2.318 -1.829 C(X)[29] -1.9838 0.128 -15.552 0.000 -2.234 -1.734 C(X)[30] -2.1912 0.151 -14.550 0.000 -2.486 -1.896 Z1 0.1930 0.026 7.495 0.000 0.143 0.244 Z2 -1.1306 0.026 -42.971 0.000 -1.182 -1.079 Z3 -1.1237 0.026 -42.515 0.000 -1.176 -1.072 Z4 -0.8986 0.026 -34.377 0.000 -0.950 -0.847 Z5 -0.6720 0.026 -25.869 0.000 -0.723 -0.621 ============================================================================== Model summary for event: 2 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_2 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -41269. Date: Tue, 02 Aug 2022 Deviance: 82537. Time: 16:47:22 Pearson chi2: 5.39e+05 No. Iterations: 8 Pseudo R-squ. (CS): 0.006763 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -1.7207 0.049 -35.253 0.000 -1.816 -1.625 C(X)[2] -1.9635 0.053 -36.941 0.000 -2.068 -1.859 C(X)[3] -1.8726 0.054 -34.671 0.000 -1.978 -1.767 C(X)[4] -1.9732 0.057 -34.515 0.000 -2.085 -1.861 C(X)[5] -1.9804 0.059 -33.427 0.000 -2.096 -1.864 C(X)[6] -2.0393 0.062 -32.819 0.000 -2.161 -1.918 C(X)[7] -2.0853 0.065 -32.085 0.000 -2.213 -1.958 C(X)[8] -2.0027 0.066 -30.546 0.000 -2.131 -1.874 C(X)[9] -2.1411 0.071 -30.347 0.000 -2.279 -2.003 C(X)[10] -2.1014 0.072 -29.209 0.000 -2.242 -1.960 C(X)[11] -2.2544 0.078 -28.862 0.000 -2.408 -2.101 C(X)[12] -2.1354 0.078 -27.505 0.000 -2.288 -1.983 C(X)[13] -2.1257 0.080 -26.538 0.000 -2.283 -1.969 C(X)[14] -2.1671 0.084 -25.786 0.000 -2.332 -2.002 C(X)[15] -2.2224 0.089 -24.964 0.000 -2.397 -2.048 C(X)[16] -2.1811 0.091 -24.026 0.000 -2.359 -2.003 C(X)[17] -2.1826 0.094 -23.134 0.000 -2.368 -1.998 C(X)[18] -2.3342 0.104 -22.438 0.000 -2.538 -2.130 C(X)[19] -2.1546 0.101 -21.382 0.000 -2.352 -1.957 C(X)[20] -2.1133 0.103 -20.467 0.000 -2.316 -1.911 C(X)[21] -2.3724 0.119 -19.867 0.000 -2.606 -2.138 C(X)[22] -2.2038 0.116 -18.983 0.000 -2.431 -1.976 C(X)[23] -2.4194 0.133 -18.207 0.000 -2.680 -2.159 C(X)[24] -2.3982 0.139 -17.275 0.000 -2.670 -2.126 C(X)[25] -2.3070 0.140 -16.480 0.000 -2.581 -2.033 C(X)[26] -2.2794 0.146 -15.630 0.000 -2.565 -1.994 C(X)[27] -2.3684 0.160 -14.774 0.000 -2.683 -2.054 C(X)[28] -2.3635 0.170 -13.926 0.000 -2.696 -2.031 C(X)[29] -2.1045 0.161 -13.103 0.000 -2.419 -1.790 C(X)[30] -2.1030 0.172 -12.215 0.000 -2.440 -1.766 Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 ============================================================================== Standard Errors \u00a4 summary = fitter . event_models [ 1 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta1_summary = summary_df . iloc [ - 5 :] summary = fitter . event_models [ 2 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta2_summary = summary_df . iloc [ - 5 :] beta2_summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 from pydts.examples_utils.plots import plot_first_model_coefs plot_first_model_coefs ( models = fitter . event_models , times = fitter . times , train_df = patients_df , n_cov = 5 ) Prediction \u00a4 Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.942684 0.960628 0.932938 overall_survival_t2 0.899636 0.930545 0.883002 overall_survival_t3 0.861480 0.903726 0.839277 overall_survival_t4 0.827201 0.879254 0.800236 overall_survival_t5 0.797018 0.857533 0.766167 overall_survival_t6 0.768048 0.836364 0.733620 overall_survival_t7 0.742313 0.817381 0.704914 overall_survival_t8 0.716876 0.798470 0.676759 overall_survival_t9 0.694881 0.781915 0.652527 overall_survival_t10 0.673241 0.765482 0.628832 overall_survival_t11 0.653276 0.750077 0.607015 overall_survival_t12 0.633323 0.734605 0.585385 overall_survival_t13 0.615405 0.720649 0.566115 overall_survival_t14 0.597401 0.706425 0.546797 overall_survival_t15 0.581732 0.693972 0.530095 overall_survival_t16 0.565528 0.680961 0.512891 overall_survival_t17 0.550207 0.668566 0.496713 overall_survival_t18 0.536576 0.657400 0.482353 overall_survival_t19 0.521450 0.644937 0.466518 overall_survival_t20 0.507307 0.633237 0.451823 overall_survival_t21 0.495118 0.622977 0.439151 overall_survival_t22 0.482190 0.612062 0.425808 overall_survival_t23 0.469586 0.601189 0.412767 overall_survival_t24 0.459059 0.592128 0.401980 overall_survival_t25 0.447933 0.582483 0.390629 overall_survival_t26 0.436435 0.572414 0.378937 overall_survival_t27 0.426143 0.563323 0.368512 overall_survival_t28 0.416810 0.555060 0.359122 overall_survival_t29 0.406209 0.545669 0.348543 overall_survival_t30 0.397051 0.537568 0.339489 hazard_j1_t1 0.043097 0.031017 0.051717 hazard_j1_t10 0.021381 0.015290 0.025774 hazard_j1_t11 0.021267 0.015208 0.025637 hazard_j1_t12 0.021106 0.015092 0.025444 hazard_j1_t13 0.018764 0.013408 0.022632 hazard_j1_t14 0.020110 0.014376 0.024248 hazard_j1_t15 0.017570 0.012551 0.021198 hazard_j1_t16 0.018835 0.013460 0.022717 hazard_j1_t17 0.018086 0.012922 0.021818 hazard_j1_t18 0.017025 0.012160 0.020542 hazard_j1_t19 0.018930 0.013528 0.022831 hazard_j1_t2 0.034478 0.024750 0.041448 hazard_j1_t20 0.017476 0.012484 0.021085 hazard_j1_t21 0.016565 0.011830 0.019989 hazard_j1_t22 0.017291 0.012351 0.020862 hazard_j1_t23 0.019019 0.013592 0.022938 hazard_j1_t24 0.015144 0.010811 0.018280 hazard_j1_t25 0.016275 0.011622 0.019640 hazard_j1_t26 0.017487 0.012491 0.021097 hazard_j1_t27 0.016094 0.011491 0.019422 hazard_j1_t28 0.014374 0.010258 0.017352 hazard_j1_t29 0.015702 0.011211 0.018951 hazard_j1_t3 0.030175 0.021634 0.036308 hazard_j1_t30 0.012799 0.009130 0.015457 hazard_j1_t4 0.028709 0.020575 0.034555 hazard_j1_t5 0.025486 0.018248 0.030696 hazard_j1_t6 0.025969 0.018596 0.031275 hazard_j1_t7 0.023589 0.016880 0.028423 hazard_j1_t8 0.023506 0.016820 0.028323 hazard_j1_t9 0.021298 0.015231 0.025675 hazard_j2_t1 0.014218 0.008355 0.015345 hazard_j2_t10 0.009761 0.005725 0.010538 hazard_j2_t11 0.008387 0.004917 0.009056 hazard_j2_t12 0.009438 0.005535 0.010190 hazard_j2_t13 0.009528 0.005588 0.010287 hazard_j2_t14 0.009146 0.005363 0.009875 hazard_j2_t15 0.008657 0.005076 0.009348 hazard_j2_t16 0.009020 0.005289 0.009739 hazard_j2_t17 0.009006 0.005281 0.009724 hazard_j2_t18 0.007749 0.004542 0.008368 hazard_j2_t19 0.009260 0.005430 0.009998 hazard_j2_t2 0.011188 0.006566 0.012077 hazard_j2_t20 0.009647 0.005658 0.010415 hazard_j2_t21 0.007461 0.004372 0.008057 hazard_j2_t22 0.008819 0.005171 0.009522 hazard_j2_t23 0.007121 0.004172 0.007690 hazard_j2_t24 0.007273 0.004261 0.007853 hazard_j2_t25 0.007961 0.004666 0.008596 hazard_j2_t26 0.008182 0.004796 0.008835 hazard_j2_t27 0.007490 0.004389 0.008088 hazard_j2_t28 0.007527 0.004411 0.008128 hazard_j2_t29 0.009730 0.005707 0.010505 hazard_j2_t3 0.012239 0.007186 0.013211 hazard_j2_t30 0.009745 0.005716 0.010521 hazard_j2_t4 0.011081 0.006503 0.011962 hazard_j2_t5 0.011003 0.006457 0.011878 hazard_j2_t6 0.010379 0.006089 0.011205 hazard_j2_t7 0.009917 0.005817 0.010707 hazard_j2_t8 0.010762 0.006315 0.011618 hazard_j2_t9 0.009384 0.005504 0.010132 prob_j1_at_t1 0.043097 0.031017 0.051717 prob_j1_at_t2 0.032501 0.023776 0.038668 prob_j1_at_t3 0.027146 0.020132 0.032060 prob_j1_at_t4 0.024733 0.018594 0.029001 prob_j1_at_t5 0.021082 0.016044 0.024564 prob_j1_at_t6 0.020698 0.015947 0.023962 prob_j1_at_t7 0.018118 0.014118 0.020852 prob_j1_at_t8 0.017449 0.013749 0.019965 prob_j1_at_t9 0.015268 0.012161 0.017376 prob_j1_at_t10 0.014857 0.011956 0.016819 prob_j1_at_t11 0.014318 0.011641 0.016121 prob_j1_at_t12 0.013788 0.011321 0.015445 prob_j1_at_t13 0.011884 0.009850 0.013248 prob_j1_at_t14 0.012376 0.010360 0.013727 prob_j1_at_t15 0.010497 0.008867 0.011591 prob_j1_at_t16 0.010957 0.009341 0.012042 prob_j1_at_t17 0.010228 0.008799 0.011190 prob_j1_at_t18 0.009367 0.008130 0.010204 prob_j1_at_t19 0.010157 0.008893 0.011013 prob_j1_at_t20 0.009113 0.008051 0.009836 prob_j1_at_t21 0.008404 0.007491 0.009032 prob_j1_at_t22 0.008561 0.007694 0.009162 prob_j1_at_t23 0.009171 0.008319 0.009767 prob_j1_at_t24 0.007112 0.006499 0.007545 prob_j1_at_t25 0.007471 0.006881 0.007895 prob_j1_at_t26 0.007833 0.007276 0.008241 prob_j1_at_t27 0.007024 0.006578 0.007360 prob_j1_at_t28 0.006125 0.005779 0.006395 prob_j1_at_t29 0.006545 0.006223 0.006806 prob_j1_at_t30 0.005199 0.004982 0.005387 prob_j2_at_t1 0.014218 0.008355 0.015345 prob_j2_at_t2 0.010546 0.006308 0.011267 prob_j2_at_t3 0.011010 0.006687 0.011665 prob_j2_at_t4 0.009546 0.005877 0.010040 prob_j2_at_t5 0.009101 0.005677 0.009505 prob_j2_at_t6 0.008272 0.005222 0.008585 prob_j2_at_t7 0.007617 0.004865 0.007855 prob_j2_at_t8 0.007989 0.005162 0.008190 prob_j2_at_t9 0.006727 0.004394 0.006857 prob_j2_at_t10 0.006783 0.004477 0.006877 prob_j2_at_t11 0.005647 0.003764 0.005695 prob_j2_at_t12 0.006165 0.004152 0.006185 prob_j2_at_t13 0.006035 0.004105 0.006022 prob_j2_at_t14 0.005628 0.003865 0.005590 prob_j2_at_t15 0.005172 0.003586 0.005111 prob_j2_at_t16 0.005247 0.003670 0.005162 prob_j2_at_t17 0.005093 0.003596 0.004987 prob_j2_at_t18 0.004264 0.003036 0.004156 prob_j2_at_t19 0.004969 0.003570 0.004822 prob_j2_at_t20 0.005030 0.003649 0.004859 prob_j2_at_t21 0.003785 0.002769 0.003640 prob_j2_at_t22 0.004366 0.003221 0.004181 prob_j2_at_t23 0.003434 0.002554 0.003274 prob_j2_at_t24 0.003415 0.002562 0.003242 prob_j2_at_t25 0.003655 0.002763 0.003456 prob_j2_at_t26 0.003665 0.002794 0.003451 prob_j2_at_t27 0.003269 0.002513 0.003065 prob_j2_at_t28 0.003208 0.002485 0.002995 prob_j2_at_t29 0.004056 0.003168 0.003773 prob_j2_at_t30 0.003958 0.003119 0.003667 cif_j1_at_t1 0.043097 0.031017 0.051717 cif_j1_at_t2 0.075599 0.054792 0.090385 cif_j1_at_t3 0.102745 0.074924 0.122445 cif_j1_at_t4 0.127478 0.093518 0.151447 cif_j1_at_t5 0.148560 0.109563 0.176011 cif_j1_at_t6 0.169258 0.125510 0.199972 cif_j1_at_t7 0.187375 0.139628 0.220824 cif_j1_at_t8 0.204824 0.153376 0.240789 cif_j1_at_t9 0.220092 0.165537 0.258165 cif_j1_at_t10 0.234950 0.177493 0.274983 cif_j1_at_t11 0.249267 0.189135 0.291105 cif_j1_at_t12 0.263055 0.200455 0.306550 cif_j1_at_t13 0.274939 0.210305 0.319798 cif_j1_at_t14 0.287314 0.220665 0.333525 cif_j1_at_t15 0.297811 0.229531 0.345116 cif_j1_at_t16 0.308768 0.238872 0.357158 cif_j1_at_t17 0.318996 0.247671 0.368348 cif_j1_at_t18 0.328364 0.255801 0.378552 cif_j1_at_t19 0.338521 0.264694 0.389565 cif_j1_at_t20 0.347634 0.272745 0.399401 cif_j1_at_t21 0.356038 0.280236 0.408432 cif_j1_at_t22 0.364599 0.287931 0.417594 cif_j1_at_t23 0.373770 0.296250 0.427361 cif_j1_at_t24 0.380881 0.302749 0.434907 cif_j1_at_t25 0.388352 0.309630 0.442802 cif_j1_at_t26 0.396185 0.316906 0.451043 cif_j1_at_t27 0.403209 0.323484 0.458403 cif_j1_at_t28 0.409334 0.329263 0.464797 cif_j1_at_t29 0.415879 0.335485 0.471603 cif_j1_at_t30 0.421078 0.340468 0.476990 cif_j2_at_t1 0.014218 0.008355 0.015345 cif_j2_at_t2 0.024765 0.014663 0.026612 cif_j2_at_t3 0.035775 0.021350 0.038278 cif_j2_at_t4 0.045321 0.027227 0.048317 cif_j2_at_t5 0.054422 0.032905 0.057822 cif_j2_at_t6 0.062695 0.038126 0.066407 cif_j2_at_t7 0.070311 0.042992 0.074262 cif_j2_at_t8 0.078300 0.048154 0.082451 cif_j2_at_t9 0.085027 0.052548 0.089308 cif_j2_at_t10 0.091810 0.057025 0.096185 cif_j2_at_t11 0.097457 0.060789 0.101880 cif_j2_at_t12 0.103622 0.064940 0.108065 cif_j2_at_t13 0.109657 0.069046 0.114087 cif_j2_at_t14 0.115285 0.072911 0.119677 cif_j2_at_t15 0.120457 0.076496 0.124789 cif_j2_at_t16 0.125704 0.080167 0.129951 cif_j2_at_t17 0.130797 0.083763 0.134938 cif_j2_at_t18 0.135061 0.086799 0.139095 cif_j2_at_t19 0.140029 0.090369 0.143917 cif_j2_at_t20 0.145060 0.094018 0.148776 cif_j2_at_t21 0.148845 0.096787 0.152416 cif_j2_at_t22 0.153211 0.100008 0.156598 cif_j2_at_t23 0.156645 0.102561 0.159872 cif_j2_at_t24 0.160060 0.105123 0.163114 cif_j2_at_t25 0.163714 0.107886 0.166569 cif_j2_at_t26 0.167379 0.110680 0.170020 cif_j2_at_t27 0.170648 0.113192 0.173085 cif_j2_at_t28 0.173856 0.115677 0.176081 cif_j2_at_t29 0.177912 0.118845 0.179853 cif_j2_at_t30 0.181870 0.121964 0.183520","title":"UsageExample FittingDataExpansionFitter FULL"},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#estimating-with-dataexpansionfitter","text":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.examples_utils.plots import plot_events_occurrence , plot_example_pred_output import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 plot_events_occurrence ( patients_df [[ 'pid' , 'J' , 'X' ]]) <AxesSubplot:xlabel='Time', ylabel='Number of Observations'>","title":"Estimating with DataExpansionFitter"},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#estimation","text":"In the following we apply the estimation method of Lee et al. (2018). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Model summary for event: 1 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_1 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -78272. Date: Tue, 02 Aug 2022 Deviance: 1.5654e+05 Time: 16:47:21 Pearson chi2: 5.35e+05 No. Iterations: 7 Pseudo R-squ. (CS): 0.01509 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -0.9459 0.033 -28.924 0.000 -1.010 -0.882 C(X)[2] -1.1780 0.035 -33.675 0.000 -1.247 -1.109 C(X)[3] -1.3158 0.037 -35.614 0.000 -1.388 -1.243 C(X)[4] -1.3671 0.039 -35.452 0.000 -1.443 -1.291 C(X)[5] -1.4895 0.041 -36.429 0.000 -1.570 -1.409 C(X)[6] -1.4702 0.042 -35.004 0.000 -1.553 -1.388 C(X)[7] -1.5688 0.044 -35.325 0.000 -1.656 -1.482 C(X)[8] -1.5724 0.046 -34.301 0.000 -1.662 -1.483 C(X)[9] -1.6733 0.049 -34.334 0.000 -1.769 -1.578 C(X)[10] -1.6693 0.050 -33.240 0.000 -1.768 -1.571 C(X)[11] -1.6748 0.052 -32.246 0.000 -1.777 -1.573 C(X)[12] -1.6825 0.054 -31.287 0.000 -1.788 -1.577 C(X)[13] -1.8026 0.058 -31.121 0.000 -1.916 -1.689 C(X)[14] -1.7319 0.058 -29.610 0.000 -1.847 -1.617 C(X)[15] -1.8695 0.064 -29.319 0.000 -1.994 -1.745 C(X)[16] -1.7987 0.064 -27.960 0.000 -1.925 -1.673 C(X)[17] -1.8400 0.068 -27.122 0.000 -1.973 -1.707 C(X)[18] -1.9016 0.072 -26.333 0.000 -2.043 -1.760 C(X)[19] -1.7936 0.072 -24.918 0.000 -1.935 -1.653 C(X)[20] -1.8749 0.077 -24.232 0.000 -2.027 -1.723 C(X)[21] -1.9294 0.082 -23.424 0.000 -2.091 -1.768 C(X)[22] -1.8858 0.084 -22.362 0.000 -2.051 -1.721 C(X)[23] -1.7888 0.085 -21.123 0.000 -1.955 -1.623 C(X)[24] -2.0205 0.098 -20.568 0.000 -2.213 -1.828 C(X)[25] -1.9474 0.100 -19.500 0.000 -2.143 -1.752 C(X)[26] -1.8743 0.102 -18.373 0.000 -2.074 -1.674 C(X)[27] -1.9588 0.112 -17.518 0.000 -2.178 -1.740 C(X)[28] -2.0736 0.125 -16.608 0.000 -2.318 -1.829 C(X)[29] -1.9838 0.128 -15.552 0.000 -2.234 -1.734 C(X)[30] -2.1912 0.151 -14.550 0.000 -2.486 -1.896 Z1 0.1930 0.026 7.495 0.000 0.143 0.244 Z2 -1.1306 0.026 -42.971 0.000 -1.182 -1.079 Z3 -1.1237 0.026 -42.515 0.000 -1.176 -1.072 Z4 -0.8986 0.026 -34.377 0.000 -0.950 -0.847 Z5 -0.6720 0.026 -25.869 0.000 -0.723 -0.621 ============================================================================== Model summary for event: 2 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_2 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -41269. Date: Tue, 02 Aug 2022 Deviance: 82537. Time: 16:47:22 Pearson chi2: 5.39e+05 No. Iterations: 8 Pseudo R-squ. (CS): 0.006763 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -1.7207 0.049 -35.253 0.000 -1.816 -1.625 C(X)[2] -1.9635 0.053 -36.941 0.000 -2.068 -1.859 C(X)[3] -1.8726 0.054 -34.671 0.000 -1.978 -1.767 C(X)[4] -1.9732 0.057 -34.515 0.000 -2.085 -1.861 C(X)[5] -1.9804 0.059 -33.427 0.000 -2.096 -1.864 C(X)[6] -2.0393 0.062 -32.819 0.000 -2.161 -1.918 C(X)[7] -2.0853 0.065 -32.085 0.000 -2.213 -1.958 C(X)[8] -2.0027 0.066 -30.546 0.000 -2.131 -1.874 C(X)[9] -2.1411 0.071 -30.347 0.000 -2.279 -2.003 C(X)[10] -2.1014 0.072 -29.209 0.000 -2.242 -1.960 C(X)[11] -2.2544 0.078 -28.862 0.000 -2.408 -2.101 C(X)[12] -2.1354 0.078 -27.505 0.000 -2.288 -1.983 C(X)[13] -2.1257 0.080 -26.538 0.000 -2.283 -1.969 C(X)[14] -2.1671 0.084 -25.786 0.000 -2.332 -2.002 C(X)[15] -2.2224 0.089 -24.964 0.000 -2.397 -2.048 C(X)[16] -2.1811 0.091 -24.026 0.000 -2.359 -2.003 C(X)[17] -2.1826 0.094 -23.134 0.000 -2.368 -1.998 C(X)[18] -2.3342 0.104 -22.438 0.000 -2.538 -2.130 C(X)[19] -2.1546 0.101 -21.382 0.000 -2.352 -1.957 C(X)[20] -2.1133 0.103 -20.467 0.000 -2.316 -1.911 C(X)[21] -2.3724 0.119 -19.867 0.000 -2.606 -2.138 C(X)[22] -2.2038 0.116 -18.983 0.000 -2.431 -1.976 C(X)[23] -2.4194 0.133 -18.207 0.000 -2.680 -2.159 C(X)[24] -2.3982 0.139 -17.275 0.000 -2.670 -2.126 C(X)[25] -2.3070 0.140 -16.480 0.000 -2.581 -2.033 C(X)[26] -2.2794 0.146 -15.630 0.000 -2.565 -1.994 C(X)[27] -2.3684 0.160 -14.774 0.000 -2.683 -2.054 C(X)[28] -2.3635 0.170 -13.926 0.000 -2.696 -2.031 C(X)[29] -2.1045 0.161 -13.103 0.000 -2.419 -1.790 C(X)[30] -2.1030 0.172 -12.215 0.000 -2.440 -1.766 Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 ==============================================================================","title":"Estimation"},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#standard-errors","text":"summary = fitter . event_models [ 1 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta1_summary = summary_df . iloc [ - 5 :] summary = fitter . event_models [ 2 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta2_summary = summary_df . iloc [ - 5 :] beta2_summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 from pydts.examples_utils.plots import plot_first_model_coefs plot_first_model_coefs ( models = fitter . event_models , times = fitter . times , train_df = patients_df , n_cov = 5 )","title":"Standard Errors"},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#prediction","text":"Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.942684 0.960628 0.932938 overall_survival_t2 0.899636 0.930545 0.883002 overall_survival_t3 0.861480 0.903726 0.839277 overall_survival_t4 0.827201 0.879254 0.800236 overall_survival_t5 0.797018 0.857533 0.766167 overall_survival_t6 0.768048 0.836364 0.733620 overall_survival_t7 0.742313 0.817381 0.704914 overall_survival_t8 0.716876 0.798470 0.676759 overall_survival_t9 0.694881 0.781915 0.652527 overall_survival_t10 0.673241 0.765482 0.628832 overall_survival_t11 0.653276 0.750077 0.607015 overall_survival_t12 0.633323 0.734605 0.585385 overall_survival_t13 0.615405 0.720649 0.566115 overall_survival_t14 0.597401 0.706425 0.546797 overall_survival_t15 0.581732 0.693972 0.530095 overall_survival_t16 0.565528 0.680961 0.512891 overall_survival_t17 0.550207 0.668566 0.496713 overall_survival_t18 0.536576 0.657400 0.482353 overall_survival_t19 0.521450 0.644937 0.466518 overall_survival_t20 0.507307 0.633237 0.451823 overall_survival_t21 0.495118 0.622977 0.439151 overall_survival_t22 0.482190 0.612062 0.425808 overall_survival_t23 0.469586 0.601189 0.412767 overall_survival_t24 0.459059 0.592128 0.401980 overall_survival_t25 0.447933 0.582483 0.390629 overall_survival_t26 0.436435 0.572414 0.378937 overall_survival_t27 0.426143 0.563323 0.368512 overall_survival_t28 0.416810 0.555060 0.359122 overall_survival_t29 0.406209 0.545669 0.348543 overall_survival_t30 0.397051 0.537568 0.339489 hazard_j1_t1 0.043097 0.031017 0.051717 hazard_j1_t10 0.021381 0.015290 0.025774 hazard_j1_t11 0.021267 0.015208 0.025637 hazard_j1_t12 0.021106 0.015092 0.025444 hazard_j1_t13 0.018764 0.013408 0.022632 hazard_j1_t14 0.020110 0.014376 0.024248 hazard_j1_t15 0.017570 0.012551 0.021198 hazard_j1_t16 0.018835 0.013460 0.022717 hazard_j1_t17 0.018086 0.012922 0.021818 hazard_j1_t18 0.017025 0.012160 0.020542 hazard_j1_t19 0.018930 0.013528 0.022831 hazard_j1_t2 0.034478 0.024750 0.041448 hazard_j1_t20 0.017476 0.012484 0.021085 hazard_j1_t21 0.016565 0.011830 0.019989 hazard_j1_t22 0.017291 0.012351 0.020862 hazard_j1_t23 0.019019 0.013592 0.022938 hazard_j1_t24 0.015144 0.010811 0.018280 hazard_j1_t25 0.016275 0.011622 0.019640 hazard_j1_t26 0.017487 0.012491 0.021097 hazard_j1_t27 0.016094 0.011491 0.019422 hazard_j1_t28 0.014374 0.010258 0.017352 hazard_j1_t29 0.015702 0.011211 0.018951 hazard_j1_t3 0.030175 0.021634 0.036308 hazard_j1_t30 0.012799 0.009130 0.015457 hazard_j1_t4 0.028709 0.020575 0.034555 hazard_j1_t5 0.025486 0.018248 0.030696 hazard_j1_t6 0.025969 0.018596 0.031275 hazard_j1_t7 0.023589 0.016880 0.028423 hazard_j1_t8 0.023506 0.016820 0.028323 hazard_j1_t9 0.021298 0.015231 0.025675 hazard_j2_t1 0.014218 0.008355 0.015345 hazard_j2_t10 0.009761 0.005725 0.010538 hazard_j2_t11 0.008387 0.004917 0.009056 hazard_j2_t12 0.009438 0.005535 0.010190 hazard_j2_t13 0.009528 0.005588 0.010287 hazard_j2_t14 0.009146 0.005363 0.009875 hazard_j2_t15 0.008657 0.005076 0.009348 hazard_j2_t16 0.009020 0.005289 0.009739 hazard_j2_t17 0.009006 0.005281 0.009724 hazard_j2_t18 0.007749 0.004542 0.008368 hazard_j2_t19 0.009260 0.005430 0.009998 hazard_j2_t2 0.011188 0.006566 0.012077 hazard_j2_t20 0.009647 0.005658 0.010415 hazard_j2_t21 0.007461 0.004372 0.008057 hazard_j2_t22 0.008819 0.005171 0.009522 hazard_j2_t23 0.007121 0.004172 0.007690 hazard_j2_t24 0.007273 0.004261 0.007853 hazard_j2_t25 0.007961 0.004666 0.008596 hazard_j2_t26 0.008182 0.004796 0.008835 hazard_j2_t27 0.007490 0.004389 0.008088 hazard_j2_t28 0.007527 0.004411 0.008128 hazard_j2_t29 0.009730 0.005707 0.010505 hazard_j2_t3 0.012239 0.007186 0.013211 hazard_j2_t30 0.009745 0.005716 0.010521 hazard_j2_t4 0.011081 0.006503 0.011962 hazard_j2_t5 0.011003 0.006457 0.011878 hazard_j2_t6 0.010379 0.006089 0.011205 hazard_j2_t7 0.009917 0.005817 0.010707 hazard_j2_t8 0.010762 0.006315 0.011618 hazard_j2_t9 0.009384 0.005504 0.010132 prob_j1_at_t1 0.043097 0.031017 0.051717 prob_j1_at_t2 0.032501 0.023776 0.038668 prob_j1_at_t3 0.027146 0.020132 0.032060 prob_j1_at_t4 0.024733 0.018594 0.029001 prob_j1_at_t5 0.021082 0.016044 0.024564 prob_j1_at_t6 0.020698 0.015947 0.023962 prob_j1_at_t7 0.018118 0.014118 0.020852 prob_j1_at_t8 0.017449 0.013749 0.019965 prob_j1_at_t9 0.015268 0.012161 0.017376 prob_j1_at_t10 0.014857 0.011956 0.016819 prob_j1_at_t11 0.014318 0.011641 0.016121 prob_j1_at_t12 0.013788 0.011321 0.015445 prob_j1_at_t13 0.011884 0.009850 0.013248 prob_j1_at_t14 0.012376 0.010360 0.013727 prob_j1_at_t15 0.010497 0.008867 0.011591 prob_j1_at_t16 0.010957 0.009341 0.012042 prob_j1_at_t17 0.010228 0.008799 0.011190 prob_j1_at_t18 0.009367 0.008130 0.010204 prob_j1_at_t19 0.010157 0.008893 0.011013 prob_j1_at_t20 0.009113 0.008051 0.009836 prob_j1_at_t21 0.008404 0.007491 0.009032 prob_j1_at_t22 0.008561 0.007694 0.009162 prob_j1_at_t23 0.009171 0.008319 0.009767 prob_j1_at_t24 0.007112 0.006499 0.007545 prob_j1_at_t25 0.007471 0.006881 0.007895 prob_j1_at_t26 0.007833 0.007276 0.008241 prob_j1_at_t27 0.007024 0.006578 0.007360 prob_j1_at_t28 0.006125 0.005779 0.006395 prob_j1_at_t29 0.006545 0.006223 0.006806 prob_j1_at_t30 0.005199 0.004982 0.005387 prob_j2_at_t1 0.014218 0.008355 0.015345 prob_j2_at_t2 0.010546 0.006308 0.011267 prob_j2_at_t3 0.011010 0.006687 0.011665 prob_j2_at_t4 0.009546 0.005877 0.010040 prob_j2_at_t5 0.009101 0.005677 0.009505 prob_j2_at_t6 0.008272 0.005222 0.008585 prob_j2_at_t7 0.007617 0.004865 0.007855 prob_j2_at_t8 0.007989 0.005162 0.008190 prob_j2_at_t9 0.006727 0.004394 0.006857 prob_j2_at_t10 0.006783 0.004477 0.006877 prob_j2_at_t11 0.005647 0.003764 0.005695 prob_j2_at_t12 0.006165 0.004152 0.006185 prob_j2_at_t13 0.006035 0.004105 0.006022 prob_j2_at_t14 0.005628 0.003865 0.005590 prob_j2_at_t15 0.005172 0.003586 0.005111 prob_j2_at_t16 0.005247 0.003670 0.005162 prob_j2_at_t17 0.005093 0.003596 0.004987 prob_j2_at_t18 0.004264 0.003036 0.004156 prob_j2_at_t19 0.004969 0.003570 0.004822 prob_j2_at_t20 0.005030 0.003649 0.004859 prob_j2_at_t21 0.003785 0.002769 0.003640 prob_j2_at_t22 0.004366 0.003221 0.004181 prob_j2_at_t23 0.003434 0.002554 0.003274 prob_j2_at_t24 0.003415 0.002562 0.003242 prob_j2_at_t25 0.003655 0.002763 0.003456 prob_j2_at_t26 0.003665 0.002794 0.003451 prob_j2_at_t27 0.003269 0.002513 0.003065 prob_j2_at_t28 0.003208 0.002485 0.002995 prob_j2_at_t29 0.004056 0.003168 0.003773 prob_j2_at_t30 0.003958 0.003119 0.003667 cif_j1_at_t1 0.043097 0.031017 0.051717 cif_j1_at_t2 0.075599 0.054792 0.090385 cif_j1_at_t3 0.102745 0.074924 0.122445 cif_j1_at_t4 0.127478 0.093518 0.151447 cif_j1_at_t5 0.148560 0.109563 0.176011 cif_j1_at_t6 0.169258 0.125510 0.199972 cif_j1_at_t7 0.187375 0.139628 0.220824 cif_j1_at_t8 0.204824 0.153376 0.240789 cif_j1_at_t9 0.220092 0.165537 0.258165 cif_j1_at_t10 0.234950 0.177493 0.274983 cif_j1_at_t11 0.249267 0.189135 0.291105 cif_j1_at_t12 0.263055 0.200455 0.306550 cif_j1_at_t13 0.274939 0.210305 0.319798 cif_j1_at_t14 0.287314 0.220665 0.333525 cif_j1_at_t15 0.297811 0.229531 0.345116 cif_j1_at_t16 0.308768 0.238872 0.357158 cif_j1_at_t17 0.318996 0.247671 0.368348 cif_j1_at_t18 0.328364 0.255801 0.378552 cif_j1_at_t19 0.338521 0.264694 0.389565 cif_j1_at_t20 0.347634 0.272745 0.399401 cif_j1_at_t21 0.356038 0.280236 0.408432 cif_j1_at_t22 0.364599 0.287931 0.417594 cif_j1_at_t23 0.373770 0.296250 0.427361 cif_j1_at_t24 0.380881 0.302749 0.434907 cif_j1_at_t25 0.388352 0.309630 0.442802 cif_j1_at_t26 0.396185 0.316906 0.451043 cif_j1_at_t27 0.403209 0.323484 0.458403 cif_j1_at_t28 0.409334 0.329263 0.464797 cif_j1_at_t29 0.415879 0.335485 0.471603 cif_j1_at_t30 0.421078 0.340468 0.476990 cif_j2_at_t1 0.014218 0.008355 0.015345 cif_j2_at_t2 0.024765 0.014663 0.026612 cif_j2_at_t3 0.035775 0.021350 0.038278 cif_j2_at_t4 0.045321 0.027227 0.048317 cif_j2_at_t5 0.054422 0.032905 0.057822 cif_j2_at_t6 0.062695 0.038126 0.066407 cif_j2_at_t7 0.070311 0.042992 0.074262 cif_j2_at_t8 0.078300 0.048154 0.082451 cif_j2_at_t9 0.085027 0.052548 0.089308 cif_j2_at_t10 0.091810 0.057025 0.096185 cif_j2_at_t11 0.097457 0.060789 0.101880 cif_j2_at_t12 0.103622 0.064940 0.108065 cif_j2_at_t13 0.109657 0.069046 0.114087 cif_j2_at_t14 0.115285 0.072911 0.119677 cif_j2_at_t15 0.120457 0.076496 0.124789 cif_j2_at_t16 0.125704 0.080167 0.129951 cif_j2_at_t17 0.130797 0.083763 0.134938 cif_j2_at_t18 0.135061 0.086799 0.139095 cif_j2_at_t19 0.140029 0.090369 0.143917 cif_j2_at_t20 0.145060 0.094018 0.148776 cif_j2_at_t21 0.148845 0.096787 0.152416 cif_j2_at_t22 0.153211 0.100008 0.156598 cif_j2_at_t23 0.156645 0.102561 0.159872 cif_j2_at_t24 0.160060 0.105123 0.163114 cif_j2_at_t25 0.163714 0.107886 0.166569 cif_j2_at_t26 0.167379 0.110680 0.170020 cif_j2_at_t27 0.170648 0.113192 0.173085 cif_j2_at_t28 0.173856 0.115677 0.176081 cif_j2_at_t29 0.177912 0.118845 0.179853 cif_j2_at_t30 0.181870 0.121964 0.183520","title":"Prediction"},{"location":"UsageExample-FittingDataExpansionFitter/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Estimating with DataExpansionFitter \u00a4 Estimation \u00a4 In the following we apply the estimation method of Lee et al. (2018). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Model summary for event: 1 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_1 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -78272. Date: Tue, 02 Aug 2022 Deviance: 1.5654e+05 Time: 16:47:21 Pearson chi2: 5.35e+05 No. Iterations: 7 Pseudo R-squ. (CS): 0.01509 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -0.9459 0.033 -28.924 0.000 -1.010 -0.882 C(X)[2] -1.1780 0.035 -33.675 0.000 -1.247 -1.109 C(X)[3] -1.3158 0.037 -35.614 0.000 -1.388 -1.243 C(X)[4] -1.3671 0.039 -35.452 0.000 -1.443 -1.291 C(X)[5] -1.4895 0.041 -36.429 0.000 -1.570 -1.409 C(X)[6] -1.4702 0.042 -35.004 0.000 -1.553 -1.388 C(X)[7] -1.5688 0.044 -35.325 0.000 -1.656 -1.482 C(X)[8] -1.5724 0.046 -34.301 0.000 -1.662 -1.483 C(X)[9] -1.6733 0.049 -34.334 0.000 -1.769 -1.578 C(X)[10] -1.6693 0.050 -33.240 0.000 -1.768 -1.571 C(X)[11] -1.6748 0.052 -32.246 0.000 -1.777 -1.573 C(X)[12] -1.6825 0.054 -31.287 0.000 -1.788 -1.577 C(X)[13] -1.8026 0.058 -31.121 0.000 -1.916 -1.689 C(X)[14] -1.7319 0.058 -29.610 0.000 -1.847 -1.617 C(X)[15] -1.8695 0.064 -29.319 0.000 -1.994 -1.745 C(X)[16] -1.7987 0.064 -27.960 0.000 -1.925 -1.673 C(X)[17] -1.8400 0.068 -27.122 0.000 -1.973 -1.707 C(X)[18] -1.9016 0.072 -26.333 0.000 -2.043 -1.760 C(X)[19] -1.7936 0.072 -24.918 0.000 -1.935 -1.653 C(X)[20] -1.8749 0.077 -24.232 0.000 -2.027 -1.723 C(X)[21] -1.9294 0.082 -23.424 0.000 -2.091 -1.768 C(X)[22] -1.8858 0.084 -22.362 0.000 -2.051 -1.721 C(X)[23] -1.7888 0.085 -21.123 0.000 -1.955 -1.623 C(X)[24] -2.0205 0.098 -20.568 0.000 -2.213 -1.828 C(X)[25] -1.9474 0.100 -19.500 0.000 -2.143 -1.752 C(X)[26] -1.8743 0.102 -18.373 0.000 -2.074 -1.674 C(X)[27] -1.9588 0.112 -17.518 0.000 -2.178 -1.740 C(X)[28] -2.0736 0.125 -16.608 0.000 -2.318 -1.829 C(X)[29] -1.9838 0.128 -15.552 0.000 -2.234 -1.734 C(X)[30] -2.1912 0.151 -14.550 0.000 -2.486 -1.896 Z1 0.1930 0.026 7.495 0.000 0.143 0.244 Z2 -1.1306 0.026 -42.971 0.000 -1.182 -1.079 Z3 -1.1237 0.026 -42.515 0.000 -1.176 -1.072 Z4 -0.8986 0.026 -34.377 0.000 -0.950 -0.847 Z5 -0.6720 0.026 -25.869 0.000 -0.723 -0.621 ============================================================================== Model summary for event: 2 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_2 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -41269. Date: Tue, 02 Aug 2022 Deviance: 82537. Time: 16:47:22 Pearson chi2: 5.39e+05 No. Iterations: 8 Pseudo R-squ. (CS): 0.006763 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -1.7207 0.049 -35.253 0.000 -1.816 -1.625 C(X)[2] -1.9635 0.053 -36.941 0.000 -2.068 -1.859 C(X)[3] -1.8726 0.054 -34.671 0.000 -1.978 -1.767 C(X)[4] -1.9732 0.057 -34.515 0.000 -2.085 -1.861 C(X)[5] -1.9804 0.059 -33.427 0.000 -2.096 -1.864 C(X)[6] -2.0393 0.062 -32.819 0.000 -2.161 -1.918 C(X)[7] -2.0853 0.065 -32.085 0.000 -2.213 -1.958 C(X)[8] -2.0027 0.066 -30.546 0.000 -2.131 -1.874 C(X)[9] -2.1411 0.071 -30.347 0.000 -2.279 -2.003 C(X)[10] -2.1014 0.072 -29.209 0.000 -2.242 -1.960 C(X)[11] -2.2544 0.078 -28.862 0.000 -2.408 -2.101 C(X)[12] -2.1354 0.078 -27.505 0.000 -2.288 -1.983 C(X)[13] -2.1257 0.080 -26.538 0.000 -2.283 -1.969 C(X)[14] -2.1671 0.084 -25.786 0.000 -2.332 -2.002 C(X)[15] -2.2224 0.089 -24.964 0.000 -2.397 -2.048 C(X)[16] -2.1811 0.091 -24.026 0.000 -2.359 -2.003 C(X)[17] -2.1826 0.094 -23.134 0.000 -2.368 -1.998 C(X)[18] -2.3342 0.104 -22.438 0.000 -2.538 -2.130 C(X)[19] -2.1546 0.101 -21.382 0.000 -2.352 -1.957 C(X)[20] -2.1133 0.103 -20.467 0.000 -2.316 -1.911 C(X)[21] -2.3724 0.119 -19.867 0.000 -2.606 -2.138 C(X)[22] -2.2038 0.116 -18.983 0.000 -2.431 -1.976 C(X)[23] -2.4194 0.133 -18.207 0.000 -2.680 -2.159 C(X)[24] -2.3982 0.139 -17.275 0.000 -2.670 -2.126 C(X)[25] -2.3070 0.140 -16.480 0.000 -2.581 -2.033 C(X)[26] -2.2794 0.146 -15.630 0.000 -2.565 -1.994 C(X)[27] -2.3684 0.160 -14.774 0.000 -2.683 -2.054 C(X)[28] -2.3635 0.170 -13.926 0.000 -2.696 -2.031 C(X)[29] -2.1045 0.161 -13.103 0.000 -2.419 -1.790 C(X)[30] -2.1030 0.172 -12.215 0.000 -2.440 -1.766 Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 ============================================================================== Standard Errors \u00a4 summary = fitter . event_models [ 1 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta1_summary = summary_df . iloc [ - 5 :] summary = fitter . event_models [ 2 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta2_summary = summary_df . iloc [ - 5 :] beta2_summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 from pydts.examples_utils.plots import plot_first_model_coefs plot_first_model_coefs ( models = fitter . event_models , times = fitter . times , train_df = patients_df , n_cov = 5 ) Prediction \u00a4 Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.942684 0.960628 0.932938 overall_survival_t2 0.899636 0.930545 0.883002 overall_survival_t3 0.861480 0.903726 0.839277 overall_survival_t4 0.827201 0.879254 0.800236 overall_survival_t5 0.797018 0.857533 0.766167 overall_survival_t6 0.768048 0.836364 0.733620 overall_survival_t7 0.742313 0.817381 0.704914 overall_survival_t8 0.716876 0.798470 0.676759 overall_survival_t9 0.694881 0.781915 0.652527 overall_survival_t10 0.673241 0.765482 0.628832 overall_survival_t11 0.653276 0.750077 0.607015 overall_survival_t12 0.633323 0.734605 0.585385 overall_survival_t13 0.615405 0.720649 0.566115 overall_survival_t14 0.597401 0.706425 0.546797 overall_survival_t15 0.581732 0.693972 0.530095 overall_survival_t16 0.565528 0.680961 0.512891 overall_survival_t17 0.550207 0.668566 0.496713 overall_survival_t18 0.536576 0.657400 0.482353 overall_survival_t19 0.521450 0.644937 0.466518 overall_survival_t20 0.507307 0.633237 0.451823 overall_survival_t21 0.495118 0.622977 0.439151 overall_survival_t22 0.482190 0.612062 0.425808 overall_survival_t23 0.469586 0.601189 0.412767 overall_survival_t24 0.459059 0.592128 0.401980 overall_survival_t25 0.447933 0.582483 0.390629 overall_survival_t26 0.436435 0.572414 0.378937 overall_survival_t27 0.426143 0.563323 0.368512 overall_survival_t28 0.416810 0.555060 0.359122 overall_survival_t29 0.406209 0.545669 0.348543 overall_survival_t30 0.397051 0.537568 0.339489 hazard_j1_t1 0.043097 0.031017 0.051717 hazard_j1_t10 0.021381 0.015290 0.025774 hazard_j1_t11 0.021267 0.015208 0.025637 hazard_j1_t12 0.021106 0.015092 0.025444 hazard_j1_t13 0.018764 0.013408 0.022632 hazard_j1_t14 0.020110 0.014376 0.024248 hazard_j1_t15 0.017570 0.012551 0.021198 hazard_j1_t16 0.018835 0.013460 0.022717 hazard_j1_t17 0.018086 0.012922 0.021818 hazard_j1_t18 0.017025 0.012160 0.020542 hazard_j1_t19 0.018930 0.013528 0.022831 hazard_j1_t2 0.034478 0.024750 0.041448 hazard_j1_t20 0.017476 0.012484 0.021085 hazard_j1_t21 0.016565 0.011830 0.019989 hazard_j1_t22 0.017291 0.012351 0.020862 hazard_j1_t23 0.019019 0.013592 0.022938 hazard_j1_t24 0.015144 0.010811 0.018280 hazard_j1_t25 0.016275 0.011622 0.019640 hazard_j1_t26 0.017487 0.012491 0.021097 hazard_j1_t27 0.016094 0.011491 0.019422 hazard_j1_t28 0.014374 0.010258 0.017352 hazard_j1_t29 0.015702 0.011211 0.018951 hazard_j1_t3 0.030175 0.021634 0.036308 hazard_j1_t30 0.012799 0.009130 0.015457 hazard_j1_t4 0.028709 0.020575 0.034555 hazard_j1_t5 0.025486 0.018248 0.030696 hazard_j1_t6 0.025969 0.018596 0.031275 hazard_j1_t7 0.023589 0.016880 0.028423 hazard_j1_t8 0.023506 0.016820 0.028323 hazard_j1_t9 0.021298 0.015231 0.025675 hazard_j2_t1 0.014218 0.008355 0.015345 hazard_j2_t10 0.009761 0.005725 0.010538 hazard_j2_t11 0.008387 0.004917 0.009056 hazard_j2_t12 0.009438 0.005535 0.010190 hazard_j2_t13 0.009528 0.005588 0.010287 hazard_j2_t14 0.009146 0.005363 0.009875 hazard_j2_t15 0.008657 0.005076 0.009348 hazard_j2_t16 0.009020 0.005289 0.009739 hazard_j2_t17 0.009006 0.005281 0.009724 hazard_j2_t18 0.007749 0.004542 0.008368 hazard_j2_t19 0.009260 0.005430 0.009998 hazard_j2_t2 0.011188 0.006566 0.012077 hazard_j2_t20 0.009647 0.005658 0.010415 hazard_j2_t21 0.007461 0.004372 0.008057 hazard_j2_t22 0.008819 0.005171 0.009522 hazard_j2_t23 0.007121 0.004172 0.007690 hazard_j2_t24 0.007273 0.004261 0.007853 hazard_j2_t25 0.007961 0.004666 0.008596 hazard_j2_t26 0.008182 0.004796 0.008835 hazard_j2_t27 0.007490 0.004389 0.008088 hazard_j2_t28 0.007527 0.004411 0.008128 hazard_j2_t29 0.009730 0.005707 0.010505 hazard_j2_t3 0.012239 0.007186 0.013211 hazard_j2_t30 0.009745 0.005716 0.010521 hazard_j2_t4 0.011081 0.006503 0.011962 hazard_j2_t5 0.011003 0.006457 0.011878 hazard_j2_t6 0.010379 0.006089 0.011205 hazard_j2_t7 0.009917 0.005817 0.010707 hazard_j2_t8 0.010762 0.006315 0.011618 hazard_j2_t9 0.009384 0.005504 0.010132 prob_j1_at_t1 0.043097 0.031017 0.051717 prob_j1_at_t2 0.032501 0.023776 0.038668 prob_j1_at_t3 0.027146 0.020132 0.032060 prob_j1_at_t4 0.024733 0.018594 0.029001 prob_j1_at_t5 0.021082 0.016044 0.024564 prob_j1_at_t6 0.020698 0.015947 0.023962 prob_j1_at_t7 0.018118 0.014118 0.020852 prob_j1_at_t8 0.017449 0.013749 0.019965 prob_j1_at_t9 0.015268 0.012161 0.017376 prob_j1_at_t10 0.014857 0.011956 0.016819 prob_j1_at_t11 0.014318 0.011641 0.016121 prob_j1_at_t12 0.013788 0.011321 0.015445 prob_j1_at_t13 0.011884 0.009850 0.013248 prob_j1_at_t14 0.012376 0.010360 0.013727 prob_j1_at_t15 0.010497 0.008867 0.011591 prob_j1_at_t16 0.010957 0.009341 0.012042 prob_j1_at_t17 0.010228 0.008799 0.011190 prob_j1_at_t18 0.009367 0.008130 0.010204 prob_j1_at_t19 0.010157 0.008893 0.011013 prob_j1_at_t20 0.009113 0.008051 0.009836 prob_j1_at_t21 0.008404 0.007491 0.009032 prob_j1_at_t22 0.008561 0.007694 0.009162 prob_j1_at_t23 0.009171 0.008319 0.009767 prob_j1_at_t24 0.007112 0.006499 0.007545 prob_j1_at_t25 0.007471 0.006881 0.007895 prob_j1_at_t26 0.007833 0.007276 0.008241 prob_j1_at_t27 0.007024 0.006578 0.007360 prob_j1_at_t28 0.006125 0.005779 0.006395 prob_j1_at_t29 0.006545 0.006223 0.006806 prob_j1_at_t30 0.005199 0.004982 0.005387 prob_j2_at_t1 0.014218 0.008355 0.015345 prob_j2_at_t2 0.010546 0.006308 0.011267 prob_j2_at_t3 0.011010 0.006687 0.011665 prob_j2_at_t4 0.009546 0.005877 0.010040 prob_j2_at_t5 0.009101 0.005677 0.009505 prob_j2_at_t6 0.008272 0.005222 0.008585 prob_j2_at_t7 0.007617 0.004865 0.007855 prob_j2_at_t8 0.007989 0.005162 0.008190 prob_j2_at_t9 0.006727 0.004394 0.006857 prob_j2_at_t10 0.006783 0.004477 0.006877 prob_j2_at_t11 0.005647 0.003764 0.005695 prob_j2_at_t12 0.006165 0.004152 0.006185 prob_j2_at_t13 0.006035 0.004105 0.006022 prob_j2_at_t14 0.005628 0.003865 0.005590 prob_j2_at_t15 0.005172 0.003586 0.005111 prob_j2_at_t16 0.005247 0.003670 0.005162 prob_j2_at_t17 0.005093 0.003596 0.004987 prob_j2_at_t18 0.004264 0.003036 0.004156 prob_j2_at_t19 0.004969 0.003570 0.004822 prob_j2_at_t20 0.005030 0.003649 0.004859 prob_j2_at_t21 0.003785 0.002769 0.003640 prob_j2_at_t22 0.004366 0.003221 0.004181 prob_j2_at_t23 0.003434 0.002554 0.003274 prob_j2_at_t24 0.003415 0.002562 0.003242 prob_j2_at_t25 0.003655 0.002763 0.003456 prob_j2_at_t26 0.003665 0.002794 0.003451 prob_j2_at_t27 0.003269 0.002513 0.003065 prob_j2_at_t28 0.003208 0.002485 0.002995 prob_j2_at_t29 0.004056 0.003168 0.003773 prob_j2_at_t30 0.003958 0.003119 0.003667 cif_j1_at_t1 0.043097 0.031017 0.051717 cif_j1_at_t2 0.075599 0.054792 0.090385 cif_j1_at_t3 0.102745 0.074924 0.122445 cif_j1_at_t4 0.127478 0.093518 0.151447 cif_j1_at_t5 0.148560 0.109563 0.176011 cif_j1_at_t6 0.169258 0.125510 0.199972 cif_j1_at_t7 0.187375 0.139628 0.220824 cif_j1_at_t8 0.204824 0.153376 0.240789 cif_j1_at_t9 0.220092 0.165537 0.258165 cif_j1_at_t10 0.234950 0.177493 0.274983 cif_j1_at_t11 0.249267 0.189135 0.291105 cif_j1_at_t12 0.263055 0.200455 0.306550 cif_j1_at_t13 0.274939 0.210305 0.319798 cif_j1_at_t14 0.287314 0.220665 0.333525 cif_j1_at_t15 0.297811 0.229531 0.345116 cif_j1_at_t16 0.308768 0.238872 0.357158 cif_j1_at_t17 0.318996 0.247671 0.368348 cif_j1_at_t18 0.328364 0.255801 0.378552 cif_j1_at_t19 0.338521 0.264694 0.389565 cif_j1_at_t20 0.347634 0.272745 0.399401 cif_j1_at_t21 0.356038 0.280236 0.408432 cif_j1_at_t22 0.364599 0.287931 0.417594 cif_j1_at_t23 0.373770 0.296250 0.427361 cif_j1_at_t24 0.380881 0.302749 0.434907 cif_j1_at_t25 0.388352 0.309630 0.442802 cif_j1_at_t26 0.396185 0.316906 0.451043 cif_j1_at_t27 0.403209 0.323484 0.458403 cif_j1_at_t28 0.409334 0.329263 0.464797 cif_j1_at_t29 0.415879 0.335485 0.471603 cif_j1_at_t30 0.421078 0.340468 0.476990 cif_j2_at_t1 0.014218 0.008355 0.015345 cif_j2_at_t2 0.024765 0.014663 0.026612 cif_j2_at_t3 0.035775 0.021350 0.038278 cif_j2_at_t4 0.045321 0.027227 0.048317 cif_j2_at_t5 0.054422 0.032905 0.057822 cif_j2_at_t6 0.062695 0.038126 0.066407 cif_j2_at_t7 0.070311 0.042992 0.074262 cif_j2_at_t8 0.078300 0.048154 0.082451 cif_j2_at_t9 0.085027 0.052548 0.089308 cif_j2_at_t10 0.091810 0.057025 0.096185 cif_j2_at_t11 0.097457 0.060789 0.101880 cif_j2_at_t12 0.103622 0.064940 0.108065 cif_j2_at_t13 0.109657 0.069046 0.114087 cif_j2_at_t14 0.115285 0.072911 0.119677 cif_j2_at_t15 0.120457 0.076496 0.124789 cif_j2_at_t16 0.125704 0.080167 0.129951 cif_j2_at_t17 0.130797 0.083763 0.134938 cif_j2_at_t18 0.135061 0.086799 0.139095 cif_j2_at_t19 0.140029 0.090369 0.143917 cif_j2_at_t20 0.145060 0.094018 0.148776 cif_j2_at_t21 0.148845 0.096787 0.152416 cif_j2_at_t22 0.153211 0.100008 0.156598 cif_j2_at_t23 0.156645 0.102561 0.159872 cif_j2_at_t24 0.160060 0.105123 0.163114 cif_j2_at_t25 0.163714 0.107886 0.166569 cif_j2_at_t26 0.167379 0.110680 0.170020 cif_j2_at_t27 0.170648 0.113192 0.173085 cif_j2_at_t28 0.173856 0.115677 0.176081 cif_j2_at_t29 0.177912 0.118845 0.179853 cif_j2_at_t30 0.181870 0.121964 0.183520","title":"Estimation with DataExpansionFitter"},{"location":"UsageExample-FittingDataExpansionFitter/#estimating-with-dataexpansionfitter","text":"","title":"Estimating with DataExpansionFitter"},{"location":"UsageExample-FittingDataExpansionFitter/#estimation","text":"In the following we apply the estimation method of Lee et al. (2018). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) fitter . print_summary () Model summary for event: 1 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_1 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -78272. Date: Tue, 02 Aug 2022 Deviance: 1.5654e+05 Time: 16:47:21 Pearson chi2: 5.35e+05 No. Iterations: 7 Pseudo R-squ. (CS): 0.01509 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -0.9459 0.033 -28.924 0.000 -1.010 -0.882 C(X)[2] -1.1780 0.035 -33.675 0.000 -1.247 -1.109 C(X)[3] -1.3158 0.037 -35.614 0.000 -1.388 -1.243 C(X)[4] -1.3671 0.039 -35.452 0.000 -1.443 -1.291 C(X)[5] -1.4895 0.041 -36.429 0.000 -1.570 -1.409 C(X)[6] -1.4702 0.042 -35.004 0.000 -1.553 -1.388 C(X)[7] -1.5688 0.044 -35.325 0.000 -1.656 -1.482 C(X)[8] -1.5724 0.046 -34.301 0.000 -1.662 -1.483 C(X)[9] -1.6733 0.049 -34.334 0.000 -1.769 -1.578 C(X)[10] -1.6693 0.050 -33.240 0.000 -1.768 -1.571 C(X)[11] -1.6748 0.052 -32.246 0.000 -1.777 -1.573 C(X)[12] -1.6825 0.054 -31.287 0.000 -1.788 -1.577 C(X)[13] -1.8026 0.058 -31.121 0.000 -1.916 -1.689 C(X)[14] -1.7319 0.058 -29.610 0.000 -1.847 -1.617 C(X)[15] -1.8695 0.064 -29.319 0.000 -1.994 -1.745 C(X)[16] -1.7987 0.064 -27.960 0.000 -1.925 -1.673 C(X)[17] -1.8400 0.068 -27.122 0.000 -1.973 -1.707 C(X)[18] -1.9016 0.072 -26.333 0.000 -2.043 -1.760 C(X)[19] -1.7936 0.072 -24.918 0.000 -1.935 -1.653 C(X)[20] -1.8749 0.077 -24.232 0.000 -2.027 -1.723 C(X)[21] -1.9294 0.082 -23.424 0.000 -2.091 -1.768 C(X)[22] -1.8858 0.084 -22.362 0.000 -2.051 -1.721 C(X)[23] -1.7888 0.085 -21.123 0.000 -1.955 -1.623 C(X)[24] -2.0205 0.098 -20.568 0.000 -2.213 -1.828 C(X)[25] -1.9474 0.100 -19.500 0.000 -2.143 -1.752 C(X)[26] -1.8743 0.102 -18.373 0.000 -2.074 -1.674 C(X)[27] -1.9588 0.112 -17.518 0.000 -2.178 -1.740 C(X)[28] -2.0736 0.125 -16.608 0.000 -2.318 -1.829 C(X)[29] -1.9838 0.128 -15.552 0.000 -2.234 -1.734 C(X)[30] -2.1912 0.151 -14.550 0.000 -2.486 -1.896 Z1 0.1930 0.026 7.495 0.000 0.143 0.244 Z2 -1.1306 0.026 -42.971 0.000 -1.182 -1.079 Z3 -1.1237 0.026 -42.515 0.000 -1.176 -1.072 Z4 -0.8986 0.026 -34.377 0.000 -0.950 -0.847 Z5 -0.6720 0.026 -25.869 0.000 -0.723 -0.621 ============================================================================== Model summary for event: 2 Generalized Linear Model Regression Results ============================================================================== Dep. Variable: j_2 No. Observations: 536780 Model: GLM Df Residuals: 536745 Model Family: Binomial Df Model: 34 Link Function: Logit Scale: 1.0000 Method: IRLS Log-Likelihood: -41269. Date: Tue, 02 Aug 2022 Deviance: 82537. Time: 16:47:22 Pearson chi2: 5.39e+05 No. Iterations: 8 Pseudo R-squ. (CS): 0.006763 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ C(X)[1] -1.7207 0.049 -35.253 0.000 -1.816 -1.625 C(X)[2] -1.9635 0.053 -36.941 0.000 -2.068 -1.859 C(X)[3] -1.8726 0.054 -34.671 0.000 -1.978 -1.767 C(X)[4] -1.9732 0.057 -34.515 0.000 -2.085 -1.861 C(X)[5] -1.9804 0.059 -33.427 0.000 -2.096 -1.864 C(X)[6] -2.0393 0.062 -32.819 0.000 -2.161 -1.918 C(X)[7] -2.0853 0.065 -32.085 0.000 -2.213 -1.958 C(X)[8] -2.0027 0.066 -30.546 0.000 -2.131 -1.874 C(X)[9] -2.1411 0.071 -30.347 0.000 -2.279 -2.003 C(X)[10] -2.1014 0.072 -29.209 0.000 -2.242 -1.960 C(X)[11] -2.2544 0.078 -28.862 0.000 -2.408 -2.101 C(X)[12] -2.1354 0.078 -27.505 0.000 -2.288 -1.983 C(X)[13] -2.1257 0.080 -26.538 0.000 -2.283 -1.969 C(X)[14] -2.1671 0.084 -25.786 0.000 -2.332 -2.002 C(X)[15] -2.2224 0.089 -24.964 0.000 -2.397 -2.048 C(X)[16] -2.1811 0.091 -24.026 0.000 -2.359 -2.003 C(X)[17] -2.1826 0.094 -23.134 0.000 -2.368 -1.998 C(X)[18] -2.3342 0.104 -22.438 0.000 -2.538 -2.130 C(X)[19] -2.1546 0.101 -21.382 0.000 -2.352 -1.957 C(X)[20] -2.1133 0.103 -20.467 0.000 -2.316 -1.911 C(X)[21] -2.3724 0.119 -19.867 0.000 -2.606 -2.138 C(X)[22] -2.2038 0.116 -18.983 0.000 -2.431 -1.976 C(X)[23] -2.4194 0.133 -18.207 0.000 -2.680 -2.159 C(X)[24] -2.3982 0.139 -17.275 0.000 -2.670 -2.126 C(X)[25] -2.3070 0.140 -16.480 0.000 -2.581 -2.033 C(X)[26] -2.2794 0.146 -15.630 0.000 -2.565 -1.994 C(X)[27] -2.3684 0.160 -14.774 0.000 -2.683 -2.054 C(X)[28] -2.3635 0.170 -13.926 0.000 -2.696 -2.031 C(X)[29] -2.1045 0.161 -13.103 0.000 -2.419 -1.790 C(X)[30] -2.1030 0.172 -12.215 0.000 -2.440 -1.766 Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 ==============================================================================","title":"Estimation"},{"location":"UsageExample-FittingDataExpansionFitter/#standard-errors","text":"summary = fitter . event_models [ 1 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta1_summary = summary_df . iloc [ - 5 :] summary = fitter . event_models [ 2 ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) beta2_summary = summary_df . iloc [ - 5 :] beta2_summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 from pydts.examples_utils.plots import plot_first_model_coefs plot_first_model_coefs ( models = fitter . event_models , times = fitter . times , train_df = patients_df , n_cov = 5 )","title":"Standard Errors"},{"location":"UsageExample-FittingDataExpansionFitter/#prediction","text":"Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.942684 0.960628 0.932938 overall_survival_t2 0.899636 0.930545 0.883002 overall_survival_t3 0.861480 0.903726 0.839277 overall_survival_t4 0.827201 0.879254 0.800236 overall_survival_t5 0.797018 0.857533 0.766167 overall_survival_t6 0.768048 0.836364 0.733620 overall_survival_t7 0.742313 0.817381 0.704914 overall_survival_t8 0.716876 0.798470 0.676759 overall_survival_t9 0.694881 0.781915 0.652527 overall_survival_t10 0.673241 0.765482 0.628832 overall_survival_t11 0.653276 0.750077 0.607015 overall_survival_t12 0.633323 0.734605 0.585385 overall_survival_t13 0.615405 0.720649 0.566115 overall_survival_t14 0.597401 0.706425 0.546797 overall_survival_t15 0.581732 0.693972 0.530095 overall_survival_t16 0.565528 0.680961 0.512891 overall_survival_t17 0.550207 0.668566 0.496713 overall_survival_t18 0.536576 0.657400 0.482353 overall_survival_t19 0.521450 0.644937 0.466518 overall_survival_t20 0.507307 0.633237 0.451823 overall_survival_t21 0.495118 0.622977 0.439151 overall_survival_t22 0.482190 0.612062 0.425808 overall_survival_t23 0.469586 0.601189 0.412767 overall_survival_t24 0.459059 0.592128 0.401980 overall_survival_t25 0.447933 0.582483 0.390629 overall_survival_t26 0.436435 0.572414 0.378937 overall_survival_t27 0.426143 0.563323 0.368512 overall_survival_t28 0.416810 0.555060 0.359122 overall_survival_t29 0.406209 0.545669 0.348543 overall_survival_t30 0.397051 0.537568 0.339489 hazard_j1_t1 0.043097 0.031017 0.051717 hazard_j1_t10 0.021381 0.015290 0.025774 hazard_j1_t11 0.021267 0.015208 0.025637 hazard_j1_t12 0.021106 0.015092 0.025444 hazard_j1_t13 0.018764 0.013408 0.022632 hazard_j1_t14 0.020110 0.014376 0.024248 hazard_j1_t15 0.017570 0.012551 0.021198 hazard_j1_t16 0.018835 0.013460 0.022717 hazard_j1_t17 0.018086 0.012922 0.021818 hazard_j1_t18 0.017025 0.012160 0.020542 hazard_j1_t19 0.018930 0.013528 0.022831 hazard_j1_t2 0.034478 0.024750 0.041448 hazard_j1_t20 0.017476 0.012484 0.021085 hazard_j1_t21 0.016565 0.011830 0.019989 hazard_j1_t22 0.017291 0.012351 0.020862 hazard_j1_t23 0.019019 0.013592 0.022938 hazard_j1_t24 0.015144 0.010811 0.018280 hazard_j1_t25 0.016275 0.011622 0.019640 hazard_j1_t26 0.017487 0.012491 0.021097 hazard_j1_t27 0.016094 0.011491 0.019422 hazard_j1_t28 0.014374 0.010258 0.017352 hazard_j1_t29 0.015702 0.011211 0.018951 hazard_j1_t3 0.030175 0.021634 0.036308 hazard_j1_t30 0.012799 0.009130 0.015457 hazard_j1_t4 0.028709 0.020575 0.034555 hazard_j1_t5 0.025486 0.018248 0.030696 hazard_j1_t6 0.025969 0.018596 0.031275 hazard_j1_t7 0.023589 0.016880 0.028423 hazard_j1_t8 0.023506 0.016820 0.028323 hazard_j1_t9 0.021298 0.015231 0.025675 hazard_j2_t1 0.014218 0.008355 0.015345 hazard_j2_t10 0.009761 0.005725 0.010538 hazard_j2_t11 0.008387 0.004917 0.009056 hazard_j2_t12 0.009438 0.005535 0.010190 hazard_j2_t13 0.009528 0.005588 0.010287 hazard_j2_t14 0.009146 0.005363 0.009875 hazard_j2_t15 0.008657 0.005076 0.009348 hazard_j2_t16 0.009020 0.005289 0.009739 hazard_j2_t17 0.009006 0.005281 0.009724 hazard_j2_t18 0.007749 0.004542 0.008368 hazard_j2_t19 0.009260 0.005430 0.009998 hazard_j2_t2 0.011188 0.006566 0.012077 hazard_j2_t20 0.009647 0.005658 0.010415 hazard_j2_t21 0.007461 0.004372 0.008057 hazard_j2_t22 0.008819 0.005171 0.009522 hazard_j2_t23 0.007121 0.004172 0.007690 hazard_j2_t24 0.007273 0.004261 0.007853 hazard_j2_t25 0.007961 0.004666 0.008596 hazard_j2_t26 0.008182 0.004796 0.008835 hazard_j2_t27 0.007490 0.004389 0.008088 hazard_j2_t28 0.007527 0.004411 0.008128 hazard_j2_t29 0.009730 0.005707 0.010505 hazard_j2_t3 0.012239 0.007186 0.013211 hazard_j2_t30 0.009745 0.005716 0.010521 hazard_j2_t4 0.011081 0.006503 0.011962 hazard_j2_t5 0.011003 0.006457 0.011878 hazard_j2_t6 0.010379 0.006089 0.011205 hazard_j2_t7 0.009917 0.005817 0.010707 hazard_j2_t8 0.010762 0.006315 0.011618 hazard_j2_t9 0.009384 0.005504 0.010132 prob_j1_at_t1 0.043097 0.031017 0.051717 prob_j1_at_t2 0.032501 0.023776 0.038668 prob_j1_at_t3 0.027146 0.020132 0.032060 prob_j1_at_t4 0.024733 0.018594 0.029001 prob_j1_at_t5 0.021082 0.016044 0.024564 prob_j1_at_t6 0.020698 0.015947 0.023962 prob_j1_at_t7 0.018118 0.014118 0.020852 prob_j1_at_t8 0.017449 0.013749 0.019965 prob_j1_at_t9 0.015268 0.012161 0.017376 prob_j1_at_t10 0.014857 0.011956 0.016819 prob_j1_at_t11 0.014318 0.011641 0.016121 prob_j1_at_t12 0.013788 0.011321 0.015445 prob_j1_at_t13 0.011884 0.009850 0.013248 prob_j1_at_t14 0.012376 0.010360 0.013727 prob_j1_at_t15 0.010497 0.008867 0.011591 prob_j1_at_t16 0.010957 0.009341 0.012042 prob_j1_at_t17 0.010228 0.008799 0.011190 prob_j1_at_t18 0.009367 0.008130 0.010204 prob_j1_at_t19 0.010157 0.008893 0.011013 prob_j1_at_t20 0.009113 0.008051 0.009836 prob_j1_at_t21 0.008404 0.007491 0.009032 prob_j1_at_t22 0.008561 0.007694 0.009162 prob_j1_at_t23 0.009171 0.008319 0.009767 prob_j1_at_t24 0.007112 0.006499 0.007545 prob_j1_at_t25 0.007471 0.006881 0.007895 prob_j1_at_t26 0.007833 0.007276 0.008241 prob_j1_at_t27 0.007024 0.006578 0.007360 prob_j1_at_t28 0.006125 0.005779 0.006395 prob_j1_at_t29 0.006545 0.006223 0.006806 prob_j1_at_t30 0.005199 0.004982 0.005387 prob_j2_at_t1 0.014218 0.008355 0.015345 prob_j2_at_t2 0.010546 0.006308 0.011267 prob_j2_at_t3 0.011010 0.006687 0.011665 prob_j2_at_t4 0.009546 0.005877 0.010040 prob_j2_at_t5 0.009101 0.005677 0.009505 prob_j2_at_t6 0.008272 0.005222 0.008585 prob_j2_at_t7 0.007617 0.004865 0.007855 prob_j2_at_t8 0.007989 0.005162 0.008190 prob_j2_at_t9 0.006727 0.004394 0.006857 prob_j2_at_t10 0.006783 0.004477 0.006877 prob_j2_at_t11 0.005647 0.003764 0.005695 prob_j2_at_t12 0.006165 0.004152 0.006185 prob_j2_at_t13 0.006035 0.004105 0.006022 prob_j2_at_t14 0.005628 0.003865 0.005590 prob_j2_at_t15 0.005172 0.003586 0.005111 prob_j2_at_t16 0.005247 0.003670 0.005162 prob_j2_at_t17 0.005093 0.003596 0.004987 prob_j2_at_t18 0.004264 0.003036 0.004156 prob_j2_at_t19 0.004969 0.003570 0.004822 prob_j2_at_t20 0.005030 0.003649 0.004859 prob_j2_at_t21 0.003785 0.002769 0.003640 prob_j2_at_t22 0.004366 0.003221 0.004181 prob_j2_at_t23 0.003434 0.002554 0.003274 prob_j2_at_t24 0.003415 0.002562 0.003242 prob_j2_at_t25 0.003655 0.002763 0.003456 prob_j2_at_t26 0.003665 0.002794 0.003451 prob_j2_at_t27 0.003269 0.002513 0.003065 prob_j2_at_t28 0.003208 0.002485 0.002995 prob_j2_at_t29 0.004056 0.003168 0.003773 prob_j2_at_t30 0.003958 0.003119 0.003667 cif_j1_at_t1 0.043097 0.031017 0.051717 cif_j1_at_t2 0.075599 0.054792 0.090385 cif_j1_at_t3 0.102745 0.074924 0.122445 cif_j1_at_t4 0.127478 0.093518 0.151447 cif_j1_at_t5 0.148560 0.109563 0.176011 cif_j1_at_t6 0.169258 0.125510 0.199972 cif_j1_at_t7 0.187375 0.139628 0.220824 cif_j1_at_t8 0.204824 0.153376 0.240789 cif_j1_at_t9 0.220092 0.165537 0.258165 cif_j1_at_t10 0.234950 0.177493 0.274983 cif_j1_at_t11 0.249267 0.189135 0.291105 cif_j1_at_t12 0.263055 0.200455 0.306550 cif_j1_at_t13 0.274939 0.210305 0.319798 cif_j1_at_t14 0.287314 0.220665 0.333525 cif_j1_at_t15 0.297811 0.229531 0.345116 cif_j1_at_t16 0.308768 0.238872 0.357158 cif_j1_at_t17 0.318996 0.247671 0.368348 cif_j1_at_t18 0.328364 0.255801 0.378552 cif_j1_at_t19 0.338521 0.264694 0.389565 cif_j1_at_t20 0.347634 0.272745 0.399401 cif_j1_at_t21 0.356038 0.280236 0.408432 cif_j1_at_t22 0.364599 0.287931 0.417594 cif_j1_at_t23 0.373770 0.296250 0.427361 cif_j1_at_t24 0.380881 0.302749 0.434907 cif_j1_at_t25 0.388352 0.309630 0.442802 cif_j1_at_t26 0.396185 0.316906 0.451043 cif_j1_at_t27 0.403209 0.323484 0.458403 cif_j1_at_t28 0.409334 0.329263 0.464797 cif_j1_at_t29 0.415879 0.335485 0.471603 cif_j1_at_t30 0.421078 0.340468 0.476990 cif_j2_at_t1 0.014218 0.008355 0.015345 cif_j2_at_t2 0.024765 0.014663 0.026612 cif_j2_at_t3 0.035775 0.021350 0.038278 cif_j2_at_t4 0.045321 0.027227 0.048317 cif_j2_at_t5 0.054422 0.032905 0.057822 cif_j2_at_t6 0.062695 0.038126 0.066407 cif_j2_at_t7 0.070311 0.042992 0.074262 cif_j2_at_t8 0.078300 0.048154 0.082451 cif_j2_at_t9 0.085027 0.052548 0.089308 cif_j2_at_t10 0.091810 0.057025 0.096185 cif_j2_at_t11 0.097457 0.060789 0.101880 cif_j2_at_t12 0.103622 0.064940 0.108065 cif_j2_at_t13 0.109657 0.069046 0.114087 cif_j2_at_t14 0.115285 0.072911 0.119677 cif_j2_at_t15 0.120457 0.076496 0.124789 cif_j2_at_t16 0.125704 0.080167 0.129951 cif_j2_at_t17 0.130797 0.083763 0.134938 cif_j2_at_t18 0.135061 0.086799 0.139095 cif_j2_at_t19 0.140029 0.090369 0.143917 cif_j2_at_t20 0.145060 0.094018 0.148776 cif_j2_at_t21 0.148845 0.096787 0.152416 cif_j2_at_t22 0.153211 0.100008 0.156598 cif_j2_at_t23 0.156645 0.102561 0.159872 cif_j2_at_t24 0.160060 0.105123 0.163114 cif_j2_at_t25 0.163714 0.107886 0.166569 cif_j2_at_t26 0.167379 0.110680 0.170020 cif_j2_at_t27 0.170648 0.113192 0.173085 cif_j2_at_t28 0.173856 0.115677 0.176081 cif_j2_at_t29 0.177912 0.118845 0.179853 cif_j2_at_t30 0.181870 0.121964 0.183520","title":"Prediction"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Estimating with TwoStagesFitter \u00a4 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.examples_utils.plots import plot_example_pred_output import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 Estimation \u00a4 In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) new_fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 3374 True -0.987702 2 2328 True -1.220809 3 1805 True -1.358580 4 1524 True -1.409997 5 1214 True -1.530437 6 1114 True -1.511889 7 916 True -1.614043 8 830 True -1.618019 9 683 True -1.718359 10 626 True -1.714668 11 569 True -1.720344 12 516 True -1.728207 13 419 True -1.845399 14 410 True -1.776981 15 326 True -1.909345 16 320 True -1.841848 17 280 True -1.881339 18 240 True -1.950204 19 243 True -1.837087 20 204 True -1.914093 21 176 True -1.978425 22 167 True -1.935467 23 166 True -1.832599 24 118 True -2.068397 25 114 True -1.996911 26 109 True -1.925090 27 89 True -2.008449 28 70 True -2.120056 29 67 True -2.033129 30 47 True -2.231271 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 1250 True -1.737087 2 839 True -1.981763 3 805 True -1.881945 4 644 True -1.991485 5 570 True -1.998569 6 483 True -2.055976 7 416 True -2.099660 8 409 True -2.019652 9 323 True -2.150486 10 306 True -2.112509 11 240 True -2.250577 12 246 True -2.142076 13 226 True -2.132065 14 198 True -2.168557 15 170 True -2.215715 16 162 True -2.178298 17 147 True -2.178342 18 115 True -2.346988 19 125 True -2.151499 20 118 True -2.113865 21 83 True -2.380588 22 89 True -2.190208 23 65 True -2.421944 24 59 True -2.401785 25 58 True -2.318061 26 53 True -2.291874 27 43 True -2.373117 28 38 True -2.368179 29 43 True -2.115566 30 37 True -2.113986 from pydts.examples_utils.plots import plot_second_model_coefs plot_second_model_coefs ( new_fitter . alpha_df , new_fitter . beta_models , new_fitter . times , n_cov = 5 ) Standard Error of the Regression Coefficients \u00a4 new_fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 Regularization \u00a4 It is possible to add regularization when estimating the Beta coefficients. It is done by using the CoxPHFitter (Lifelines) penalizer and l1_ratio arguments, which can be passed using the fit_beta_kwargs argument to the fit() method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_ratio}}{2}||\\beta||_{2}^{2} + \\mbox{L1_ratio} ||\\beta||_1 \\Bigg) $$ Examples for adding L1, L2 and Elastic Net regularization are followed. L1 \u00a4 L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 1 } } L1_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L1_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000102 5.690226e-08 0.000041 Z2 -0.774487 0.025401 -3.574822e-01 0.038251 Z3 -0.762942 0.025533 -6.516077e-01 0.038510 Z4 -0.552172 0.025318 -3.590965e-01 0.038235 Z5 -0.340120 0.025211 -1.435430e-06 0.000132 L2 \u00a4 L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.172957 0.024069 0.032774 0.034626 Z2 -1.007262 0.024506 -0.903957 0.035205 Z3 -1.000509 0.024629 -1.162132 0.035589 Z4 -0.799488 0.024384 -0.903531 0.035177 Z5 -0.597079 0.024255 -0.537159 0.034911 Elastic Net \u00a4 EN_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0.5 } } EN_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) EN_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.039322 0.024542 0.000001 0.000190 Z2 -0.895581 0.024938 -0.654614 0.036595 Z3 -0.886332 0.025065 -0.928867 0.036941 Z4 -0.680998 0.024832 -0.655263 0.036573 Z5 -0.473818 0.024711 -0.265356 0.036382 Separated Penalty Coefficients \u00a4 The above methods can be applied with a separate penalty coefficient to each of the covariates by passing a vector (with same length as the number of covariates) to the penalizer keyword instead of a scalar. For example, applying L2 regularization only to covariates Z1, Z2 can be done as follows: L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : np . array ([ 0.04 , 0.04 , 0 , 0 , 0 ]), 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.088314 0.017178 0.011120 0.020019 Z2 -0.515292 0.017378 -0.306194 0.020269 Z3 -1.069182 0.025695 -1.374391 0.039205 Z4 -0.853807 0.025419 -1.066715 0.038602 Z5 -0.641989 0.025272 -0.637811 0.038161 Prediction \u00a4 Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = new_fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.941845 0.959704 0.932244 overall_survival_t2 0.898252 0.928975 0.881883 overall_survival_t3 0.859546 0.901548 0.837705 overall_survival_t4 0.824888 0.876595 0.798379 overall_survival_t5 0.794348 0.854430 0.764029 overall_survival_t6 0.765050 0.832843 0.731231 overall_survival_t7 0.739087 0.813536 0.702371 overall_survival_t8 0.713461 0.794332 0.674101 overall_survival_t9 0.691252 0.777486 0.649717 overall_survival_t10 0.669427 0.760785 0.625897 overall_survival_t11 0.649221 0.745080 0.603897 overall_survival_t12 0.629094 0.729356 0.582149 overall_survival_t13 0.610989 0.715148 0.562735 overall_survival_t14 0.592808 0.700678 0.543291 overall_survival_t15 0.576893 0.687938 0.526379 overall_survival_t16 0.560503 0.674682 0.509031 overall_survival_t17 0.544986 0.662036 0.492696 overall_survival_t18 0.531334 0.650765 0.478355 overall_survival_t19 0.516058 0.638087 0.462409 overall_survival_t20 0.501756 0.626164 0.447587 overall_survival_t21 0.489552 0.615813 0.434935 overall_survival_t22 0.476519 0.604735 0.421524 overall_survival_t23 0.463848 0.593720 0.408451 overall_survival_t24 0.453297 0.584569 0.397667 overall_survival_t25 0.442188 0.574864 0.386360 overall_survival_t26 0.430726 0.564748 0.374731 overall_survival_t27 0.420435 0.555592 0.364334 overall_survival_t28 0.411084 0.547251 0.354946 overall_survival_t29 0.400506 0.537811 0.344410 overall_survival_t30 0.391319 0.529622 0.335339 hazard_j1_t1 0.043775 0.031795 0.052250 hazard_j1_t10 0.021649 0.015626 0.025957 hazard_j1_t11 0.021529 0.015539 0.025814 hazard_j1_t12 0.021364 0.015419 0.025617 hazard_j1_t13 0.019047 0.013737 0.022849 hazard_j1_t14 0.020368 0.014696 0.024427 hazard_j1_t15 0.017888 0.012897 0.021464 hazard_j1_t16 0.019113 0.013785 0.022928 hazard_j1_t17 0.018387 0.013259 0.022060 hazard_j1_t18 0.017184 0.012387 0.020622 hazard_j1_t19 0.019202 0.013850 0.023035 hazard_j1_t2 0.034991 0.025352 0.041840 hazard_j1_t20 0.017805 0.012837 0.021364 hazard_j1_t21 0.016714 0.012047 0.020060 hazard_j1_t22 0.017435 0.012569 0.020922 hazard_j1_t23 0.019287 0.013912 0.023136 hazard_j1_t24 0.015298 0.011022 0.018365 hazard_j1_t25 0.016413 0.011829 0.019700 hazard_j1_t26 0.017613 0.012698 0.021135 hazard_j1_t27 0.016227 0.011695 0.019478 hazard_j1_t28 0.014539 0.010472 0.017457 hazard_j1_t29 0.015838 0.011413 0.019012 hazard_j1_t3 0.030625 0.022161 0.036652 hazard_j1_t30 0.013028 0.009381 0.015648 hazard_j1_t4 0.029135 0.021074 0.034880 hazard_j1_t5 0.025915 0.018728 0.031045 hazard_j1_t6 0.026387 0.019071 0.031608 hazard_j1_t7 0.023886 0.017251 0.028626 hazard_j1_t8 0.023794 0.017184 0.028516 hazard_j1_t9 0.021571 0.015569 0.025864 hazard_j2_t1 0.014380 0.008500 0.015506 hazard_j2_t10 0.009924 0.005855 0.010704 hazard_j2_t11 0.008655 0.005104 0.009337 hazard_j2_t12 0.009637 0.005686 0.010396 hazard_j2_t13 0.009733 0.005743 0.010499 hazard_j2_t14 0.009388 0.005538 0.010127 hazard_j2_t15 0.008959 0.005284 0.009665 hazard_j2_t16 0.009298 0.005485 0.010030 hazard_j2_t17 0.009297 0.005484 0.010029 hazard_j2_t18 0.007866 0.004637 0.008486 hazard_j2_t19 0.009548 0.005633 0.010299 hazard_j2_t2 0.011294 0.006668 0.012181 hazard_j2_t20 0.009910 0.005848 0.010690 hazard_j2_t21 0.007608 0.004485 0.008208 hazard_j2_t22 0.009189 0.005420 0.009912 hazard_j2_t23 0.007302 0.004304 0.007878 hazard_j2_t24 0.007450 0.004391 0.008037 hazard_j2_t25 0.008095 0.004773 0.008733 hazard_j2_t26 0.008308 0.004899 0.008963 hazard_j2_t27 0.007665 0.004518 0.008269 hazard_j2_t28 0.007702 0.004540 0.008310 hazard_j2_t29 0.009894 0.005838 0.010672 hazard_j2_t3 0.012465 0.007363 0.013443 hazard_j2_t30 0.009909 0.005847 0.010689 hazard_j2_t4 0.011186 0.006604 0.012065 hazard_j2_t5 0.011108 0.006557 0.011981 hazard_j2_t6 0.010495 0.006194 0.011320 hazard_j2_t7 0.010051 0.005931 0.010841 hazard_j2_t8 0.010879 0.006422 0.011734 hazard_j2_t9 0.009558 0.005638 0.010310 prob_j1_at_t1 0.043775 0.031795 0.052250 prob_j1_at_t2 0.032956 0.024330 0.039005 prob_j1_at_t3 0.027509 0.020587 0.032323 prob_j1_at_t4 0.025043 0.018999 0.029219 prob_j1_at_t5 0.021377 0.016416 0.024785 prob_j1_at_t6 0.020961 0.016295 0.024149 prob_j1_at_t7 0.018274 0.014368 0.020932 prob_j1_at_t8 0.017586 0.013980 0.020029 prob_j1_at_t9 0.015390 0.012367 0.017435 prob_j1_at_t10 0.014965 0.012149 0.016865 prob_j1_at_t11 0.014412 0.011822 0.016157 prob_j1_at_t12 0.013870 0.011488 0.015470 prob_j1_at_t13 0.011982 0.010019 0.013301 prob_j1_at_t14 0.012445 0.010510 0.013746 prob_j1_at_t15 0.010604 0.009037 0.011661 prob_j1_at_t16 0.011026 0.009483 0.012069 prob_j1_at_t17 0.010306 0.008945 0.011229 prob_j1_at_t18 0.009365 0.008201 0.010160 prob_j1_at_t19 0.010203 0.009013 0.011019 prob_j1_at_t20 0.009188 0.008191 0.009879 prob_j1_at_t21 0.008386 0.007543 0.008978 prob_j1_at_t22 0.008535 0.007740 0.009100 prob_j1_at_t23 0.009191 0.008413 0.009752 prob_j1_at_t24 0.007096 0.006544 0.007501 prob_j1_at_t25 0.007440 0.006915 0.007834 prob_j1_at_t26 0.007788 0.007300 0.008166 prob_j1_at_t27 0.006990 0.006604 0.007299 prob_j1_at_t28 0.006113 0.005818 0.006360 prob_j1_at_t29 0.006511 0.006246 0.006748 prob_j1_at_t30 0.005218 0.005045 0.005389 prob_j2_at_t1 0.014380 0.008500 0.015506 prob_j2_at_t2 0.010637 0.006399 0.011356 prob_j2_at_t3 0.011197 0.006840 0.011855 prob_j2_at_t4 0.009615 0.005954 0.010107 prob_j2_at_t5 0.009163 0.005748 0.009565 prob_j2_at_t6 0.008337 0.005292 0.008649 prob_j2_at_t7 0.007689 0.004939 0.007928 prob_j2_at_t8 0.008040 0.005224 0.008241 prob_j2_at_t9 0.006819 0.004479 0.006950 prob_j2_at_t10 0.006860 0.004552 0.006955 prob_j2_at_t11 0.005794 0.003883 0.005844 prob_j2_at_t12 0.006257 0.004236 0.006278 prob_j2_at_t13 0.006123 0.004188 0.006112 prob_j2_at_t14 0.005736 0.003961 0.005699 prob_j2_at_t15 0.005311 0.003703 0.005251 prob_j2_at_t16 0.005364 0.003773 0.005279 prob_j2_at_t17 0.005211 0.003700 0.005105 prob_j2_at_t18 0.004287 0.003070 0.004181 prob_j2_at_t19 0.005073 0.003666 0.004927 prob_j2_at_t20 0.005114 0.003731 0.004943 prob_j2_at_t21 0.003817 0.002808 0.003674 prob_j2_at_t22 0.004498 0.003338 0.004311 prob_j2_at_t23 0.003480 0.002603 0.003321 prob_j2_at_t24 0.003455 0.002607 0.003283 prob_j2_at_t25 0.003669 0.002790 0.003473 prob_j2_at_t26 0.003674 0.002816 0.003463 prob_j2_at_t27 0.003301 0.002552 0.003099 prob_j2_at_t28 0.003238 0.002523 0.003027 prob_j2_at_t29 0.004067 0.003195 0.003788 prob_j2_at_t30 0.003969 0.003144 0.003681 cif_j1_at_t1 0.043775 0.031795 0.052250 cif_j1_at_t2 0.076731 0.056126 0.091255 cif_j1_at_t3 0.104240 0.076713 0.123578 cif_j1_at_t4 0.129283 0.095712 0.152797 cif_j1_at_t5 0.150660 0.112129 0.177582 cif_j1_at_t6 0.171621 0.128424 0.201731 cif_j1_at_t7 0.189895 0.142791 0.222664 cif_j1_at_t8 0.207480 0.156771 0.242692 cif_j1_at_t9 0.222870 0.169138 0.260127 cif_j1_at_t10 0.237835 0.181287 0.276992 cif_j1_at_t11 0.252248 0.193109 0.293148 cif_j1_at_t12 0.266118 0.204597 0.308618 cif_j1_at_t13 0.278100 0.214616 0.321919 cif_j1_at_t14 0.290544 0.225126 0.335665 cif_j1_at_t15 0.301148 0.234163 0.347326 cif_j1_at_t16 0.312174 0.243646 0.359395 cif_j1_at_t17 0.322480 0.252591 0.370624 cif_j1_at_t18 0.331845 0.260792 0.380785 cif_j1_at_t19 0.342048 0.269805 0.391804 cif_j1_at_t20 0.351236 0.277996 0.401683 cif_j1_at_t21 0.359623 0.285540 0.410661 cif_j1_at_t22 0.368158 0.293280 0.419761 cif_j1_at_t23 0.377348 0.301693 0.429513 cif_j1_at_t24 0.384444 0.308236 0.437014 cif_j1_at_t25 0.391884 0.315151 0.444848 cif_j1_at_t26 0.399673 0.322451 0.453014 cif_j1_at_t27 0.406662 0.329055 0.460313 cif_j1_at_t28 0.412775 0.334874 0.466673 cif_j1_at_t29 0.419285 0.341119 0.473422 cif_j1_at_t30 0.424503 0.346164 0.478811 cif_j2_at_t1 0.014380 0.008500 0.015506 cif_j2_at_t2 0.025018 0.014900 0.026862 cif_j2_at_t3 0.036214 0.021739 0.038717 cif_j2_at_t4 0.045829 0.027693 0.048824 cif_j2_at_t5 0.054992 0.033441 0.058389 cif_j2_at_t6 0.063329 0.038733 0.067038 cif_j2_at_t7 0.071018 0.043673 0.074965 cif_j2_at_t8 0.079059 0.048897 0.083207 cif_j2_at_t9 0.085878 0.053376 0.090156 cif_j2_at_t10 0.092737 0.057928 0.097111 cif_j2_at_t11 0.098531 0.061811 0.102955 cif_j2_at_t12 0.104788 0.066048 0.109233 cif_j2_at_t13 0.110912 0.070236 0.115345 cif_j2_at_t14 0.116647 0.074197 0.121044 cif_j2_at_t15 0.121959 0.077899 0.126295 cif_j2_at_t16 0.127322 0.081672 0.131574 cif_j2_at_t17 0.132534 0.085372 0.136679 cif_j2_at_t18 0.136820 0.088442 0.140860 cif_j2_at_t19 0.141894 0.092108 0.145787 cif_j2_at_t20 0.147008 0.095839 0.150730 cif_j2_at_t21 0.150825 0.098647 0.154404 cif_j2_at_t22 0.155324 0.101985 0.158715 cif_j2_at_t23 0.158803 0.104588 0.162036 cif_j2_at_t24 0.162259 0.107195 0.165319 cif_j2_at_t25 0.165928 0.109985 0.168792 cif_j2_at_t26 0.169602 0.112801 0.172254 cif_j2_at_t27 0.172903 0.115352 0.175353 cif_j2_at_t28 0.176141 0.117875 0.178380 cif_j2_at_t29 0.180208 0.121070 0.182168 cif_j2_at_t30 0.184177 0.124214 0.185850","title":"UsageExample FittingTwoStagesFitter FULL"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#estimating-with-twostagesfitter","text":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.examples_utils.plots import plot_example_pred_output import warnings pd . set_option ( \"display.max_rows\" , 500 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } n_patients = 50000 n_cov = 5 patients_df = generate_quick_start_df ( n_patients = n_patients , n_cov = n_cov , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , censoring_prob = 0.8 , real_coef_dict = real_coef_dict ) patients_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14","title":"Estimating with TwoStagesFitter"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#estimation","text":"In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) new_fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 3374 True -0.987702 2 2328 True -1.220809 3 1805 True -1.358580 4 1524 True -1.409997 5 1214 True -1.530437 6 1114 True -1.511889 7 916 True -1.614043 8 830 True -1.618019 9 683 True -1.718359 10 626 True -1.714668 11 569 True -1.720344 12 516 True -1.728207 13 419 True -1.845399 14 410 True -1.776981 15 326 True -1.909345 16 320 True -1.841848 17 280 True -1.881339 18 240 True -1.950204 19 243 True -1.837087 20 204 True -1.914093 21 176 True -1.978425 22 167 True -1.935467 23 166 True -1.832599 24 118 True -2.068397 25 114 True -1.996911 26 109 True -1.925090 27 89 True -2.008449 28 70 True -2.120056 29 67 True -2.033129 30 47 True -2.231271 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 1250 True -1.737087 2 839 True -1.981763 3 805 True -1.881945 4 644 True -1.991485 5 570 True -1.998569 6 483 True -2.055976 7 416 True -2.099660 8 409 True -2.019652 9 323 True -2.150486 10 306 True -2.112509 11 240 True -2.250577 12 246 True -2.142076 13 226 True -2.132065 14 198 True -2.168557 15 170 True -2.215715 16 162 True -2.178298 17 147 True -2.178342 18 115 True -2.346988 19 125 True -2.151499 20 118 True -2.113865 21 83 True -2.380588 22 89 True -2.190208 23 65 True -2.421944 24 59 True -2.401785 25 58 True -2.318061 26 53 True -2.291874 27 43 True -2.373117 28 38 True -2.368179 29 43 True -2.115566 30 37 True -2.113986 from pydts.examples_utils.plots import plot_second_model_coefs plot_second_model_coefs ( new_fitter . alpha_df , new_fitter . beta_models , new_fitter . times , n_cov = 5 )","title":"Estimation"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#standard-error-of-the-regression-coefficients","text":"new_fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179","title":"Standard Error of the Regression Coefficients"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#regularization","text":"It is possible to add regularization when estimating the Beta coefficients. It is done by using the CoxPHFitter (Lifelines) penalizer and l1_ratio arguments, which can be passed using the fit_beta_kwargs argument to the fit() method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_ratio}}{2}||\\beta||_{2}^{2} + \\mbox{L1_ratio} ||\\beta||_1 \\Bigg) $$ Examples for adding L1, L2 and Elastic Net regularization are followed.","title":"Regularization"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#l1","text":"L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 1 } } L1_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L1_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000102 5.690226e-08 0.000041 Z2 -0.774487 0.025401 -3.574822e-01 0.038251 Z3 -0.762942 0.025533 -6.516077e-01 0.038510 Z4 -0.552172 0.025318 -3.590965e-01 0.038235 Z5 -0.340120 0.025211 -1.435430e-06 0.000132","title":"L1"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#l2","text":"L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.172957 0.024069 0.032774 0.034626 Z2 -1.007262 0.024506 -0.903957 0.035205 Z3 -1.000509 0.024629 -1.162132 0.035589 Z4 -0.799488 0.024384 -0.903531 0.035177 Z5 -0.597079 0.024255 -0.537159 0.034911","title":"L2"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#elastic-net","text":"EN_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0.5 } } EN_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) EN_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.039322 0.024542 0.000001 0.000190 Z2 -0.895581 0.024938 -0.654614 0.036595 Z3 -0.886332 0.025065 -0.928867 0.036941 Z4 -0.680998 0.024832 -0.655263 0.036573 Z5 -0.473818 0.024711 -0.265356 0.036382","title":"Elastic Net"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#separated-penalty-coefficients","text":"The above methods can be applied with a separate penalty coefficient to each of the covariates by passing a vector (with same length as the number of covariates) to the penalizer keyword instead of a scalar. For example, applying L2 regularization only to covariates Z1, Z2 can be done as follows: L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : np . array ([ 0.04 , 0.04 , 0 , 0 , 0 ]), 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.088314 0.017178 0.011120 0.020019 Z2 -0.515292 0.017378 -0.306194 0.020269 Z3 -1.069182 0.025695 -1.374391 0.039205 Z4 -0.853807 0.025419 -1.066715 0.038602 Z5 -0.641989 0.025272 -0.637811 0.038161","title":"Separated Penalty Coefficients"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#prediction","text":"Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = new_fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.941845 0.959704 0.932244 overall_survival_t2 0.898252 0.928975 0.881883 overall_survival_t3 0.859546 0.901548 0.837705 overall_survival_t4 0.824888 0.876595 0.798379 overall_survival_t5 0.794348 0.854430 0.764029 overall_survival_t6 0.765050 0.832843 0.731231 overall_survival_t7 0.739087 0.813536 0.702371 overall_survival_t8 0.713461 0.794332 0.674101 overall_survival_t9 0.691252 0.777486 0.649717 overall_survival_t10 0.669427 0.760785 0.625897 overall_survival_t11 0.649221 0.745080 0.603897 overall_survival_t12 0.629094 0.729356 0.582149 overall_survival_t13 0.610989 0.715148 0.562735 overall_survival_t14 0.592808 0.700678 0.543291 overall_survival_t15 0.576893 0.687938 0.526379 overall_survival_t16 0.560503 0.674682 0.509031 overall_survival_t17 0.544986 0.662036 0.492696 overall_survival_t18 0.531334 0.650765 0.478355 overall_survival_t19 0.516058 0.638087 0.462409 overall_survival_t20 0.501756 0.626164 0.447587 overall_survival_t21 0.489552 0.615813 0.434935 overall_survival_t22 0.476519 0.604735 0.421524 overall_survival_t23 0.463848 0.593720 0.408451 overall_survival_t24 0.453297 0.584569 0.397667 overall_survival_t25 0.442188 0.574864 0.386360 overall_survival_t26 0.430726 0.564748 0.374731 overall_survival_t27 0.420435 0.555592 0.364334 overall_survival_t28 0.411084 0.547251 0.354946 overall_survival_t29 0.400506 0.537811 0.344410 overall_survival_t30 0.391319 0.529622 0.335339 hazard_j1_t1 0.043775 0.031795 0.052250 hazard_j1_t10 0.021649 0.015626 0.025957 hazard_j1_t11 0.021529 0.015539 0.025814 hazard_j1_t12 0.021364 0.015419 0.025617 hazard_j1_t13 0.019047 0.013737 0.022849 hazard_j1_t14 0.020368 0.014696 0.024427 hazard_j1_t15 0.017888 0.012897 0.021464 hazard_j1_t16 0.019113 0.013785 0.022928 hazard_j1_t17 0.018387 0.013259 0.022060 hazard_j1_t18 0.017184 0.012387 0.020622 hazard_j1_t19 0.019202 0.013850 0.023035 hazard_j1_t2 0.034991 0.025352 0.041840 hazard_j1_t20 0.017805 0.012837 0.021364 hazard_j1_t21 0.016714 0.012047 0.020060 hazard_j1_t22 0.017435 0.012569 0.020922 hazard_j1_t23 0.019287 0.013912 0.023136 hazard_j1_t24 0.015298 0.011022 0.018365 hazard_j1_t25 0.016413 0.011829 0.019700 hazard_j1_t26 0.017613 0.012698 0.021135 hazard_j1_t27 0.016227 0.011695 0.019478 hazard_j1_t28 0.014539 0.010472 0.017457 hazard_j1_t29 0.015838 0.011413 0.019012 hazard_j1_t3 0.030625 0.022161 0.036652 hazard_j1_t30 0.013028 0.009381 0.015648 hazard_j1_t4 0.029135 0.021074 0.034880 hazard_j1_t5 0.025915 0.018728 0.031045 hazard_j1_t6 0.026387 0.019071 0.031608 hazard_j1_t7 0.023886 0.017251 0.028626 hazard_j1_t8 0.023794 0.017184 0.028516 hazard_j1_t9 0.021571 0.015569 0.025864 hazard_j2_t1 0.014380 0.008500 0.015506 hazard_j2_t10 0.009924 0.005855 0.010704 hazard_j2_t11 0.008655 0.005104 0.009337 hazard_j2_t12 0.009637 0.005686 0.010396 hazard_j2_t13 0.009733 0.005743 0.010499 hazard_j2_t14 0.009388 0.005538 0.010127 hazard_j2_t15 0.008959 0.005284 0.009665 hazard_j2_t16 0.009298 0.005485 0.010030 hazard_j2_t17 0.009297 0.005484 0.010029 hazard_j2_t18 0.007866 0.004637 0.008486 hazard_j2_t19 0.009548 0.005633 0.010299 hazard_j2_t2 0.011294 0.006668 0.012181 hazard_j2_t20 0.009910 0.005848 0.010690 hazard_j2_t21 0.007608 0.004485 0.008208 hazard_j2_t22 0.009189 0.005420 0.009912 hazard_j2_t23 0.007302 0.004304 0.007878 hazard_j2_t24 0.007450 0.004391 0.008037 hazard_j2_t25 0.008095 0.004773 0.008733 hazard_j2_t26 0.008308 0.004899 0.008963 hazard_j2_t27 0.007665 0.004518 0.008269 hazard_j2_t28 0.007702 0.004540 0.008310 hazard_j2_t29 0.009894 0.005838 0.010672 hazard_j2_t3 0.012465 0.007363 0.013443 hazard_j2_t30 0.009909 0.005847 0.010689 hazard_j2_t4 0.011186 0.006604 0.012065 hazard_j2_t5 0.011108 0.006557 0.011981 hazard_j2_t6 0.010495 0.006194 0.011320 hazard_j2_t7 0.010051 0.005931 0.010841 hazard_j2_t8 0.010879 0.006422 0.011734 hazard_j2_t9 0.009558 0.005638 0.010310 prob_j1_at_t1 0.043775 0.031795 0.052250 prob_j1_at_t2 0.032956 0.024330 0.039005 prob_j1_at_t3 0.027509 0.020587 0.032323 prob_j1_at_t4 0.025043 0.018999 0.029219 prob_j1_at_t5 0.021377 0.016416 0.024785 prob_j1_at_t6 0.020961 0.016295 0.024149 prob_j1_at_t7 0.018274 0.014368 0.020932 prob_j1_at_t8 0.017586 0.013980 0.020029 prob_j1_at_t9 0.015390 0.012367 0.017435 prob_j1_at_t10 0.014965 0.012149 0.016865 prob_j1_at_t11 0.014412 0.011822 0.016157 prob_j1_at_t12 0.013870 0.011488 0.015470 prob_j1_at_t13 0.011982 0.010019 0.013301 prob_j1_at_t14 0.012445 0.010510 0.013746 prob_j1_at_t15 0.010604 0.009037 0.011661 prob_j1_at_t16 0.011026 0.009483 0.012069 prob_j1_at_t17 0.010306 0.008945 0.011229 prob_j1_at_t18 0.009365 0.008201 0.010160 prob_j1_at_t19 0.010203 0.009013 0.011019 prob_j1_at_t20 0.009188 0.008191 0.009879 prob_j1_at_t21 0.008386 0.007543 0.008978 prob_j1_at_t22 0.008535 0.007740 0.009100 prob_j1_at_t23 0.009191 0.008413 0.009752 prob_j1_at_t24 0.007096 0.006544 0.007501 prob_j1_at_t25 0.007440 0.006915 0.007834 prob_j1_at_t26 0.007788 0.007300 0.008166 prob_j1_at_t27 0.006990 0.006604 0.007299 prob_j1_at_t28 0.006113 0.005818 0.006360 prob_j1_at_t29 0.006511 0.006246 0.006748 prob_j1_at_t30 0.005218 0.005045 0.005389 prob_j2_at_t1 0.014380 0.008500 0.015506 prob_j2_at_t2 0.010637 0.006399 0.011356 prob_j2_at_t3 0.011197 0.006840 0.011855 prob_j2_at_t4 0.009615 0.005954 0.010107 prob_j2_at_t5 0.009163 0.005748 0.009565 prob_j2_at_t6 0.008337 0.005292 0.008649 prob_j2_at_t7 0.007689 0.004939 0.007928 prob_j2_at_t8 0.008040 0.005224 0.008241 prob_j2_at_t9 0.006819 0.004479 0.006950 prob_j2_at_t10 0.006860 0.004552 0.006955 prob_j2_at_t11 0.005794 0.003883 0.005844 prob_j2_at_t12 0.006257 0.004236 0.006278 prob_j2_at_t13 0.006123 0.004188 0.006112 prob_j2_at_t14 0.005736 0.003961 0.005699 prob_j2_at_t15 0.005311 0.003703 0.005251 prob_j2_at_t16 0.005364 0.003773 0.005279 prob_j2_at_t17 0.005211 0.003700 0.005105 prob_j2_at_t18 0.004287 0.003070 0.004181 prob_j2_at_t19 0.005073 0.003666 0.004927 prob_j2_at_t20 0.005114 0.003731 0.004943 prob_j2_at_t21 0.003817 0.002808 0.003674 prob_j2_at_t22 0.004498 0.003338 0.004311 prob_j2_at_t23 0.003480 0.002603 0.003321 prob_j2_at_t24 0.003455 0.002607 0.003283 prob_j2_at_t25 0.003669 0.002790 0.003473 prob_j2_at_t26 0.003674 0.002816 0.003463 prob_j2_at_t27 0.003301 0.002552 0.003099 prob_j2_at_t28 0.003238 0.002523 0.003027 prob_j2_at_t29 0.004067 0.003195 0.003788 prob_j2_at_t30 0.003969 0.003144 0.003681 cif_j1_at_t1 0.043775 0.031795 0.052250 cif_j1_at_t2 0.076731 0.056126 0.091255 cif_j1_at_t3 0.104240 0.076713 0.123578 cif_j1_at_t4 0.129283 0.095712 0.152797 cif_j1_at_t5 0.150660 0.112129 0.177582 cif_j1_at_t6 0.171621 0.128424 0.201731 cif_j1_at_t7 0.189895 0.142791 0.222664 cif_j1_at_t8 0.207480 0.156771 0.242692 cif_j1_at_t9 0.222870 0.169138 0.260127 cif_j1_at_t10 0.237835 0.181287 0.276992 cif_j1_at_t11 0.252248 0.193109 0.293148 cif_j1_at_t12 0.266118 0.204597 0.308618 cif_j1_at_t13 0.278100 0.214616 0.321919 cif_j1_at_t14 0.290544 0.225126 0.335665 cif_j1_at_t15 0.301148 0.234163 0.347326 cif_j1_at_t16 0.312174 0.243646 0.359395 cif_j1_at_t17 0.322480 0.252591 0.370624 cif_j1_at_t18 0.331845 0.260792 0.380785 cif_j1_at_t19 0.342048 0.269805 0.391804 cif_j1_at_t20 0.351236 0.277996 0.401683 cif_j1_at_t21 0.359623 0.285540 0.410661 cif_j1_at_t22 0.368158 0.293280 0.419761 cif_j1_at_t23 0.377348 0.301693 0.429513 cif_j1_at_t24 0.384444 0.308236 0.437014 cif_j1_at_t25 0.391884 0.315151 0.444848 cif_j1_at_t26 0.399673 0.322451 0.453014 cif_j1_at_t27 0.406662 0.329055 0.460313 cif_j1_at_t28 0.412775 0.334874 0.466673 cif_j1_at_t29 0.419285 0.341119 0.473422 cif_j1_at_t30 0.424503 0.346164 0.478811 cif_j2_at_t1 0.014380 0.008500 0.015506 cif_j2_at_t2 0.025018 0.014900 0.026862 cif_j2_at_t3 0.036214 0.021739 0.038717 cif_j2_at_t4 0.045829 0.027693 0.048824 cif_j2_at_t5 0.054992 0.033441 0.058389 cif_j2_at_t6 0.063329 0.038733 0.067038 cif_j2_at_t7 0.071018 0.043673 0.074965 cif_j2_at_t8 0.079059 0.048897 0.083207 cif_j2_at_t9 0.085878 0.053376 0.090156 cif_j2_at_t10 0.092737 0.057928 0.097111 cif_j2_at_t11 0.098531 0.061811 0.102955 cif_j2_at_t12 0.104788 0.066048 0.109233 cif_j2_at_t13 0.110912 0.070236 0.115345 cif_j2_at_t14 0.116647 0.074197 0.121044 cif_j2_at_t15 0.121959 0.077899 0.126295 cif_j2_at_t16 0.127322 0.081672 0.131574 cif_j2_at_t17 0.132534 0.085372 0.136679 cif_j2_at_t18 0.136820 0.088442 0.140860 cif_j2_at_t19 0.141894 0.092108 0.145787 cif_j2_at_t20 0.147008 0.095839 0.150730 cif_j2_at_t21 0.150825 0.098647 0.154404 cif_j2_at_t22 0.155324 0.101985 0.158715 cif_j2_at_t23 0.158803 0.104588 0.162036 cif_j2_at_t24 0.162259 0.107195 0.165319 cif_j2_at_t25 0.165928 0.109985 0.168792 cif_j2_at_t26 0.169602 0.112801 0.172254 cif_j2_at_t27 0.172903 0.115352 0.175353 cif_j2_at_t28 0.176141 0.117875 0.178380 cif_j2_at_t29 0.180208 0.121070 0.182168 cif_j2_at_t30 0.184177 0.124214 0.185850","title":"Prediction"},{"location":"UsageExample-FittingTwoStagesFitter/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Estimating with TwoStagesFitter \u00a4 Estimation \u00a4 In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) new_fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 3374 True -0.987702 2 2328 True -1.220809 3 1805 True -1.358580 4 1524 True -1.409997 5 1214 True -1.530437 6 1114 True -1.511889 7 916 True -1.614043 8 830 True -1.618019 9 683 True -1.718359 10 626 True -1.714668 11 569 True -1.720344 12 516 True -1.728207 13 419 True -1.845399 14 410 True -1.776981 15 326 True -1.909345 16 320 True -1.841848 17 280 True -1.881339 18 240 True -1.950204 19 243 True -1.837087 20 204 True -1.914093 21 176 True -1.978425 22 167 True -1.935467 23 166 True -1.832599 24 118 True -2.068397 25 114 True -1.996911 26 109 True -1.925090 27 89 True -2.008449 28 70 True -2.120056 29 67 True -2.033129 30 47 True -2.231271 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 1250 True -1.737087 2 839 True -1.981763 3 805 True -1.881945 4 644 True -1.991485 5 570 True -1.998569 6 483 True -2.055976 7 416 True -2.099660 8 409 True -2.019652 9 323 True -2.150486 10 306 True -2.112509 11 240 True -2.250577 12 246 True -2.142076 13 226 True -2.132065 14 198 True -2.168557 15 170 True -2.215715 16 162 True -2.178298 17 147 True -2.178342 18 115 True -2.346988 19 125 True -2.151499 20 118 True -2.113865 21 83 True -2.380588 22 89 True -2.190208 23 65 True -2.421944 24 59 True -2.401785 25 58 True -2.318061 26 53 True -2.291874 27 43 True -2.373117 28 38 True -2.368179 29 43 True -2.115566 30 37 True -2.113986 from pydts.examples_utils.plots import plot_second_model_coefs plot_second_model_coefs ( new_fitter . alpha_df , new_fitter . beta_models , new_fitter . times , n_cov = 5 ) Standard Error of the Regression Coefficients \u00a4 new_fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 Regularization \u00a4 It is possible to add regularization when estimating the Beta coefficients. It is done by using the CoxPHFitter (Lifelines) penalizer and l1_ratio arguments, which can be passed using the fit_beta_kwargs argument to the fit() method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_ratio}}{2}||\\beta||_{2}^{2} + \\mbox{L1_ratio} ||\\beta||_1 \\Bigg) $$ Examples for adding L1, L2 and Elastic Net regularization are followed. L1 \u00a4 L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 1 } } L1_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L1_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000102 5.690226e-08 0.000041 Z2 -0.774487 0.025401 -3.574822e-01 0.038251 Z3 -0.762942 0.025533 -6.516077e-01 0.038510 Z4 -0.552172 0.025318 -3.590965e-01 0.038235 Z5 -0.340120 0.025211 -1.435430e-06 0.000132 L2 \u00a4 L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.172957 0.024069 0.032774 0.034626 Z2 -1.007262 0.024506 -0.903957 0.035205 Z3 -1.000509 0.024629 -1.162132 0.035589 Z4 -0.799488 0.024384 -0.903531 0.035177 Z5 -0.597079 0.024255 -0.537159 0.034911 Elastic Net \u00a4 EN_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0.5 } } EN_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) EN_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.039322 0.024542 0.000001 0.000190 Z2 -0.895581 0.024938 -0.654614 0.036595 Z3 -0.886332 0.025065 -0.928867 0.036941 Z4 -0.680998 0.024832 -0.655263 0.036573 Z5 -0.473818 0.024711 -0.265356 0.036382 Separated Penalty Coefficients \u00a4 The above methods can be applied with a separate penalty coefficient to each of the covariates by passing a vector (with same length as the number of covariates) to the penalizer keyword instead of a scalar. For example, applying L2 regularization only to covariates Z1, Z2 can be done as follows: L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : np . array ([ 0.04 , 0.04 , 0 , 0 , 0 ]), 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.088314 0.017178 0.011120 0.020019 Z2 -0.515292 0.017378 -0.306194 0.020269 Z3 -1.069182 0.025695 -1.374391 0.039205 Z4 -0.853807 0.025419 -1.066715 0.038602 Z5 -0.641989 0.025272 -0.637811 0.038161 Prediction \u00a4 Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = new_fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.941845 0.959704 0.932244 overall_survival_t2 0.898252 0.928975 0.881883 overall_survival_t3 0.859546 0.901548 0.837705 overall_survival_t4 0.824888 0.876595 0.798379 overall_survival_t5 0.794348 0.854430 0.764029 overall_survival_t6 0.765050 0.832843 0.731231 overall_survival_t7 0.739087 0.813536 0.702371 overall_survival_t8 0.713461 0.794332 0.674101 overall_survival_t9 0.691252 0.777486 0.649717 overall_survival_t10 0.669427 0.760785 0.625897 overall_survival_t11 0.649221 0.745080 0.603897 overall_survival_t12 0.629094 0.729356 0.582149 overall_survival_t13 0.610989 0.715148 0.562735 overall_survival_t14 0.592808 0.700678 0.543291 overall_survival_t15 0.576893 0.687938 0.526379 overall_survival_t16 0.560503 0.674682 0.509031 overall_survival_t17 0.544986 0.662036 0.492696 overall_survival_t18 0.531334 0.650765 0.478355 overall_survival_t19 0.516058 0.638087 0.462409 overall_survival_t20 0.501756 0.626164 0.447587 overall_survival_t21 0.489552 0.615813 0.434935 overall_survival_t22 0.476519 0.604735 0.421524 overall_survival_t23 0.463848 0.593720 0.408451 overall_survival_t24 0.453297 0.584569 0.397667 overall_survival_t25 0.442188 0.574864 0.386360 overall_survival_t26 0.430726 0.564748 0.374731 overall_survival_t27 0.420435 0.555592 0.364334 overall_survival_t28 0.411084 0.547251 0.354946 overall_survival_t29 0.400506 0.537811 0.344410 overall_survival_t30 0.391319 0.529622 0.335339 hazard_j1_t1 0.043775 0.031795 0.052250 hazard_j1_t10 0.021649 0.015626 0.025957 hazard_j1_t11 0.021529 0.015539 0.025814 hazard_j1_t12 0.021364 0.015419 0.025617 hazard_j1_t13 0.019047 0.013737 0.022849 hazard_j1_t14 0.020368 0.014696 0.024427 hazard_j1_t15 0.017888 0.012897 0.021464 hazard_j1_t16 0.019113 0.013785 0.022928 hazard_j1_t17 0.018387 0.013259 0.022060 hazard_j1_t18 0.017184 0.012387 0.020622 hazard_j1_t19 0.019202 0.013850 0.023035 hazard_j1_t2 0.034991 0.025352 0.041840 hazard_j1_t20 0.017805 0.012837 0.021364 hazard_j1_t21 0.016714 0.012047 0.020060 hazard_j1_t22 0.017435 0.012569 0.020922 hazard_j1_t23 0.019287 0.013912 0.023136 hazard_j1_t24 0.015298 0.011022 0.018365 hazard_j1_t25 0.016413 0.011829 0.019700 hazard_j1_t26 0.017613 0.012698 0.021135 hazard_j1_t27 0.016227 0.011695 0.019478 hazard_j1_t28 0.014539 0.010472 0.017457 hazard_j1_t29 0.015838 0.011413 0.019012 hazard_j1_t3 0.030625 0.022161 0.036652 hazard_j1_t30 0.013028 0.009381 0.015648 hazard_j1_t4 0.029135 0.021074 0.034880 hazard_j1_t5 0.025915 0.018728 0.031045 hazard_j1_t6 0.026387 0.019071 0.031608 hazard_j1_t7 0.023886 0.017251 0.028626 hazard_j1_t8 0.023794 0.017184 0.028516 hazard_j1_t9 0.021571 0.015569 0.025864 hazard_j2_t1 0.014380 0.008500 0.015506 hazard_j2_t10 0.009924 0.005855 0.010704 hazard_j2_t11 0.008655 0.005104 0.009337 hazard_j2_t12 0.009637 0.005686 0.010396 hazard_j2_t13 0.009733 0.005743 0.010499 hazard_j2_t14 0.009388 0.005538 0.010127 hazard_j2_t15 0.008959 0.005284 0.009665 hazard_j2_t16 0.009298 0.005485 0.010030 hazard_j2_t17 0.009297 0.005484 0.010029 hazard_j2_t18 0.007866 0.004637 0.008486 hazard_j2_t19 0.009548 0.005633 0.010299 hazard_j2_t2 0.011294 0.006668 0.012181 hazard_j2_t20 0.009910 0.005848 0.010690 hazard_j2_t21 0.007608 0.004485 0.008208 hazard_j2_t22 0.009189 0.005420 0.009912 hazard_j2_t23 0.007302 0.004304 0.007878 hazard_j2_t24 0.007450 0.004391 0.008037 hazard_j2_t25 0.008095 0.004773 0.008733 hazard_j2_t26 0.008308 0.004899 0.008963 hazard_j2_t27 0.007665 0.004518 0.008269 hazard_j2_t28 0.007702 0.004540 0.008310 hazard_j2_t29 0.009894 0.005838 0.010672 hazard_j2_t3 0.012465 0.007363 0.013443 hazard_j2_t30 0.009909 0.005847 0.010689 hazard_j2_t4 0.011186 0.006604 0.012065 hazard_j2_t5 0.011108 0.006557 0.011981 hazard_j2_t6 0.010495 0.006194 0.011320 hazard_j2_t7 0.010051 0.005931 0.010841 hazard_j2_t8 0.010879 0.006422 0.011734 hazard_j2_t9 0.009558 0.005638 0.010310 prob_j1_at_t1 0.043775 0.031795 0.052250 prob_j1_at_t2 0.032956 0.024330 0.039005 prob_j1_at_t3 0.027509 0.020587 0.032323 prob_j1_at_t4 0.025043 0.018999 0.029219 prob_j1_at_t5 0.021377 0.016416 0.024785 prob_j1_at_t6 0.020961 0.016295 0.024149 prob_j1_at_t7 0.018274 0.014368 0.020932 prob_j1_at_t8 0.017586 0.013980 0.020029 prob_j1_at_t9 0.015390 0.012367 0.017435 prob_j1_at_t10 0.014965 0.012149 0.016865 prob_j1_at_t11 0.014412 0.011822 0.016157 prob_j1_at_t12 0.013870 0.011488 0.015470 prob_j1_at_t13 0.011982 0.010019 0.013301 prob_j1_at_t14 0.012445 0.010510 0.013746 prob_j1_at_t15 0.010604 0.009037 0.011661 prob_j1_at_t16 0.011026 0.009483 0.012069 prob_j1_at_t17 0.010306 0.008945 0.011229 prob_j1_at_t18 0.009365 0.008201 0.010160 prob_j1_at_t19 0.010203 0.009013 0.011019 prob_j1_at_t20 0.009188 0.008191 0.009879 prob_j1_at_t21 0.008386 0.007543 0.008978 prob_j1_at_t22 0.008535 0.007740 0.009100 prob_j1_at_t23 0.009191 0.008413 0.009752 prob_j1_at_t24 0.007096 0.006544 0.007501 prob_j1_at_t25 0.007440 0.006915 0.007834 prob_j1_at_t26 0.007788 0.007300 0.008166 prob_j1_at_t27 0.006990 0.006604 0.007299 prob_j1_at_t28 0.006113 0.005818 0.006360 prob_j1_at_t29 0.006511 0.006246 0.006748 prob_j1_at_t30 0.005218 0.005045 0.005389 prob_j2_at_t1 0.014380 0.008500 0.015506 prob_j2_at_t2 0.010637 0.006399 0.011356 prob_j2_at_t3 0.011197 0.006840 0.011855 prob_j2_at_t4 0.009615 0.005954 0.010107 prob_j2_at_t5 0.009163 0.005748 0.009565 prob_j2_at_t6 0.008337 0.005292 0.008649 prob_j2_at_t7 0.007689 0.004939 0.007928 prob_j2_at_t8 0.008040 0.005224 0.008241 prob_j2_at_t9 0.006819 0.004479 0.006950 prob_j2_at_t10 0.006860 0.004552 0.006955 prob_j2_at_t11 0.005794 0.003883 0.005844 prob_j2_at_t12 0.006257 0.004236 0.006278 prob_j2_at_t13 0.006123 0.004188 0.006112 prob_j2_at_t14 0.005736 0.003961 0.005699 prob_j2_at_t15 0.005311 0.003703 0.005251 prob_j2_at_t16 0.005364 0.003773 0.005279 prob_j2_at_t17 0.005211 0.003700 0.005105 prob_j2_at_t18 0.004287 0.003070 0.004181 prob_j2_at_t19 0.005073 0.003666 0.004927 prob_j2_at_t20 0.005114 0.003731 0.004943 prob_j2_at_t21 0.003817 0.002808 0.003674 prob_j2_at_t22 0.004498 0.003338 0.004311 prob_j2_at_t23 0.003480 0.002603 0.003321 prob_j2_at_t24 0.003455 0.002607 0.003283 prob_j2_at_t25 0.003669 0.002790 0.003473 prob_j2_at_t26 0.003674 0.002816 0.003463 prob_j2_at_t27 0.003301 0.002552 0.003099 prob_j2_at_t28 0.003238 0.002523 0.003027 prob_j2_at_t29 0.004067 0.003195 0.003788 prob_j2_at_t30 0.003969 0.003144 0.003681 cif_j1_at_t1 0.043775 0.031795 0.052250 cif_j1_at_t2 0.076731 0.056126 0.091255 cif_j1_at_t3 0.104240 0.076713 0.123578 cif_j1_at_t4 0.129283 0.095712 0.152797 cif_j1_at_t5 0.150660 0.112129 0.177582 cif_j1_at_t6 0.171621 0.128424 0.201731 cif_j1_at_t7 0.189895 0.142791 0.222664 cif_j1_at_t8 0.207480 0.156771 0.242692 cif_j1_at_t9 0.222870 0.169138 0.260127 cif_j1_at_t10 0.237835 0.181287 0.276992 cif_j1_at_t11 0.252248 0.193109 0.293148 cif_j1_at_t12 0.266118 0.204597 0.308618 cif_j1_at_t13 0.278100 0.214616 0.321919 cif_j1_at_t14 0.290544 0.225126 0.335665 cif_j1_at_t15 0.301148 0.234163 0.347326 cif_j1_at_t16 0.312174 0.243646 0.359395 cif_j1_at_t17 0.322480 0.252591 0.370624 cif_j1_at_t18 0.331845 0.260792 0.380785 cif_j1_at_t19 0.342048 0.269805 0.391804 cif_j1_at_t20 0.351236 0.277996 0.401683 cif_j1_at_t21 0.359623 0.285540 0.410661 cif_j1_at_t22 0.368158 0.293280 0.419761 cif_j1_at_t23 0.377348 0.301693 0.429513 cif_j1_at_t24 0.384444 0.308236 0.437014 cif_j1_at_t25 0.391884 0.315151 0.444848 cif_j1_at_t26 0.399673 0.322451 0.453014 cif_j1_at_t27 0.406662 0.329055 0.460313 cif_j1_at_t28 0.412775 0.334874 0.466673 cif_j1_at_t29 0.419285 0.341119 0.473422 cif_j1_at_t30 0.424503 0.346164 0.478811 cif_j2_at_t1 0.014380 0.008500 0.015506 cif_j2_at_t2 0.025018 0.014900 0.026862 cif_j2_at_t3 0.036214 0.021739 0.038717 cif_j2_at_t4 0.045829 0.027693 0.048824 cif_j2_at_t5 0.054992 0.033441 0.058389 cif_j2_at_t6 0.063329 0.038733 0.067038 cif_j2_at_t7 0.071018 0.043673 0.074965 cif_j2_at_t8 0.079059 0.048897 0.083207 cif_j2_at_t9 0.085878 0.053376 0.090156 cif_j2_at_t10 0.092737 0.057928 0.097111 cif_j2_at_t11 0.098531 0.061811 0.102955 cif_j2_at_t12 0.104788 0.066048 0.109233 cif_j2_at_t13 0.110912 0.070236 0.115345 cif_j2_at_t14 0.116647 0.074197 0.121044 cif_j2_at_t15 0.121959 0.077899 0.126295 cif_j2_at_t16 0.127322 0.081672 0.131574 cif_j2_at_t17 0.132534 0.085372 0.136679 cif_j2_at_t18 0.136820 0.088442 0.140860 cif_j2_at_t19 0.141894 0.092108 0.145787 cif_j2_at_t20 0.147008 0.095839 0.150730 cif_j2_at_t21 0.150825 0.098647 0.154404 cif_j2_at_t22 0.155324 0.101985 0.158715 cif_j2_at_t23 0.158803 0.104588 0.162036 cif_j2_at_t24 0.162259 0.107195 0.165319 cif_j2_at_t25 0.165928 0.109985 0.168792 cif_j2_at_t26 0.169602 0.112801 0.172254 cif_j2_at_t27 0.172903 0.115352 0.175353 cif_j2_at_t28 0.176141 0.117875 0.178380 cif_j2_at_t29 0.180208 0.121070 0.182168 cif_j2_at_t30 0.184177 0.124214 0.185850","title":"Estimation with TwoStagesFitter"},{"location":"UsageExample-FittingTwoStagesFitter/#estimating-with-twostagesfitter","text":"","title":"Estimating with TwoStagesFitter"},{"location":"UsageExample-FittingTwoStagesFitter/#estimation","text":"In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'. from pydts.fitters import TwoStagesFitter new_fitter = TwoStagesFitter () new_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 )) new_fitter . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 3374 True -0.987702 2 2328 True -1.220809 3 1805 True -1.358580 4 1524 True -1.409997 5 1214 True -1.530437 6 1114 True -1.511889 7 916 True -1.614043 8 830 True -1.618019 9 683 True -1.718359 10 626 True -1.714668 11 569 True -1.720344 12 516 True -1.728207 13 419 True -1.845399 14 410 True -1.776981 15 326 True -1.909345 16 320 True -1.841848 17 280 True -1.881339 18 240 True -1.950204 19 243 True -1.837087 20 204 True -1.914093 21 176 True -1.978425 22 167 True -1.935467 23 166 True -1.832599 24 118 True -2.068397 25 114 True -1.996911 26 109 True -1.925090 27 89 True -2.008449 28 70 True -2.120056 29 67 True -2.033129 30 47 True -2.231271 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 1250 True -1.737087 2 839 True -1.981763 3 805 True -1.881945 4 644 True -1.991485 5 570 True -1.998569 6 483 True -2.055976 7 416 True -2.099660 8 409 True -2.019652 9 323 True -2.150486 10 306 True -2.112509 11 240 True -2.250577 12 246 True -2.142076 13 226 True -2.132065 14 198 True -2.168557 15 170 True -2.215715 16 162 True -2.178298 17 147 True -2.178342 18 115 True -2.346988 19 125 True -2.151499 20 118 True -2.113865 21 83 True -2.380588 22 89 True -2.190208 23 65 True -2.421944 24 59 True -2.401785 25 58 True -2.318061 26 53 True -2.291874 27 43 True -2.373117 28 38 True -2.368179 29 43 True -2.115566 30 37 True -2.113986 from pydts.examples_utils.plots import plot_second_model_coefs plot_second_model_coefs ( new_fitter . alpha_df , new_fitter . beta_models , new_fitter . times , n_cov = 5 )","title":"Estimation"},{"location":"UsageExample-FittingTwoStagesFitter/#standard-error-of-the-regression-coefficients","text":"new_fitter . get_beta_SE () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179","title":"Standard Error of the Regression Coefficients"},{"location":"UsageExample-FittingTwoStagesFitter/#regularization","text":"It is possible to add regularization when estimating the Beta coefficients. It is done by using the CoxPHFitter (Lifelines) penalizer and l1_ratio arguments, which can be passed using the fit_beta_kwargs argument to the fit() method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_ratio}}{2}||\\beta||_{2}^{2} + \\mbox{L1_ratio} ||\\beta||_1 \\Bigg) $$ Examples for adding L1, L2 and Elastic Net regularization are followed.","title":"Regularization"},{"location":"UsageExample-FittingTwoStagesFitter/#l1","text":"L1_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 1 } } L1_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L1_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000102 5.690226e-08 0.000041 Z2 -0.774487 0.025401 -3.574822e-01 0.038251 Z3 -0.762942 0.025533 -6.516077e-01 0.038510 Z4 -0.552172 0.025318 -3.590965e-01 0.038235 Z5 -0.340120 0.025211 -1.435430e-06 0.000132","title":"L1"},{"location":"UsageExample-FittingTwoStagesFitter/#l2","text":"L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.172957 0.024069 0.032774 0.034626 Z2 -1.007262 0.024506 -0.903957 0.035205 Z3 -1.000509 0.024629 -1.162132 0.035589 Z4 -0.799488 0.024384 -0.903531 0.035177 Z5 -0.597079 0.024255 -0.537159 0.034911","title":"L2"},{"location":"UsageExample-FittingTwoStagesFitter/#elastic-net","text":"EN_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : 0.003 , 'l1_ratio' : 0.5 } } EN_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) EN_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.039322 0.024542 0.000001 0.000190 Z2 -0.895581 0.024938 -0.654614 0.036595 Z3 -0.886332 0.025065 -0.928867 0.036941 Z4 -0.680998 0.024832 -0.655263 0.036573 Z5 -0.473818 0.024711 -0.265356 0.036382","title":"Elastic Net"},{"location":"UsageExample-FittingTwoStagesFitter/#separated-penalty-coefficients","text":"The above methods can be applied with a separate penalty coefficient to each of the covariates by passing a vector (with same length as the number of covariates) to the penalizer keyword instead of a scalar. For example, applying L2 regularization only to covariates Z1, Z2 can be done as follows: L2_regularized_fitter = TwoStagesFitter () fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : np . array ([ 0.04 , 0.04 , 0 , 0 , 0 ]), 'l1_ratio' : 0 } } L2_regularized_fitter . fit ( df = patients_df . drop ([ 'C' , 'T' ], axis = 1 ), fit_beta_kwargs = fit_beta_kwargs ) L2_regularized_fitter . get_beta_SE () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.088314 0.017178 0.011120 0.020019 Z2 -0.515292 0.017378 -0.306194 0.020269 Z3 -1.069182 0.025695 -1.374391 0.039205 Z4 -0.853807 0.025419 -1.066715 0.038602 Z5 -0.641989 0.025272 -0.637811 0.038161","title":"Separated Penalty Coefficients"},{"location":"UsageExample-FittingTwoStagesFitter/#prediction","text":"Full prediction is given by the method predict_cumulative_incident_function() The input is a pandas.DataFrame() containing for each observation the covariates columns which were used in the fit() method (Z1-Z5 in our example). The following columns will be added: The overall survival at each time point t The hazard for each failure type \\(j\\) at each time point t The probability of event type \\(j\\) at time t The Cumulative Incident Function (CIF) of event type \\(j\\) at time t In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view. pred_df = new_fitter . predict_cumulative_incident_function ( patients_df . drop ([ 'J' , 'T' , 'C' , 'X' ], axis = 1 ) . head ( 3 )) . set_index ( 'pid' ) . T pred_df . index . name = '' pred_df . columns = [ 'ID=0' , 'ID=1' , 'ID=2' ] plot_example_pred_output ( pred_df ) pred_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.941845 0.959704 0.932244 overall_survival_t2 0.898252 0.928975 0.881883 overall_survival_t3 0.859546 0.901548 0.837705 overall_survival_t4 0.824888 0.876595 0.798379 overall_survival_t5 0.794348 0.854430 0.764029 overall_survival_t6 0.765050 0.832843 0.731231 overall_survival_t7 0.739087 0.813536 0.702371 overall_survival_t8 0.713461 0.794332 0.674101 overall_survival_t9 0.691252 0.777486 0.649717 overall_survival_t10 0.669427 0.760785 0.625897 overall_survival_t11 0.649221 0.745080 0.603897 overall_survival_t12 0.629094 0.729356 0.582149 overall_survival_t13 0.610989 0.715148 0.562735 overall_survival_t14 0.592808 0.700678 0.543291 overall_survival_t15 0.576893 0.687938 0.526379 overall_survival_t16 0.560503 0.674682 0.509031 overall_survival_t17 0.544986 0.662036 0.492696 overall_survival_t18 0.531334 0.650765 0.478355 overall_survival_t19 0.516058 0.638087 0.462409 overall_survival_t20 0.501756 0.626164 0.447587 overall_survival_t21 0.489552 0.615813 0.434935 overall_survival_t22 0.476519 0.604735 0.421524 overall_survival_t23 0.463848 0.593720 0.408451 overall_survival_t24 0.453297 0.584569 0.397667 overall_survival_t25 0.442188 0.574864 0.386360 overall_survival_t26 0.430726 0.564748 0.374731 overall_survival_t27 0.420435 0.555592 0.364334 overall_survival_t28 0.411084 0.547251 0.354946 overall_survival_t29 0.400506 0.537811 0.344410 overall_survival_t30 0.391319 0.529622 0.335339 hazard_j1_t1 0.043775 0.031795 0.052250 hazard_j1_t10 0.021649 0.015626 0.025957 hazard_j1_t11 0.021529 0.015539 0.025814 hazard_j1_t12 0.021364 0.015419 0.025617 hazard_j1_t13 0.019047 0.013737 0.022849 hazard_j1_t14 0.020368 0.014696 0.024427 hazard_j1_t15 0.017888 0.012897 0.021464 hazard_j1_t16 0.019113 0.013785 0.022928 hazard_j1_t17 0.018387 0.013259 0.022060 hazard_j1_t18 0.017184 0.012387 0.020622 hazard_j1_t19 0.019202 0.013850 0.023035 hazard_j1_t2 0.034991 0.025352 0.041840 hazard_j1_t20 0.017805 0.012837 0.021364 hazard_j1_t21 0.016714 0.012047 0.020060 hazard_j1_t22 0.017435 0.012569 0.020922 hazard_j1_t23 0.019287 0.013912 0.023136 hazard_j1_t24 0.015298 0.011022 0.018365 hazard_j1_t25 0.016413 0.011829 0.019700 hazard_j1_t26 0.017613 0.012698 0.021135 hazard_j1_t27 0.016227 0.011695 0.019478 hazard_j1_t28 0.014539 0.010472 0.017457 hazard_j1_t29 0.015838 0.011413 0.019012 hazard_j1_t3 0.030625 0.022161 0.036652 hazard_j1_t30 0.013028 0.009381 0.015648 hazard_j1_t4 0.029135 0.021074 0.034880 hazard_j1_t5 0.025915 0.018728 0.031045 hazard_j1_t6 0.026387 0.019071 0.031608 hazard_j1_t7 0.023886 0.017251 0.028626 hazard_j1_t8 0.023794 0.017184 0.028516 hazard_j1_t9 0.021571 0.015569 0.025864 hazard_j2_t1 0.014380 0.008500 0.015506 hazard_j2_t10 0.009924 0.005855 0.010704 hazard_j2_t11 0.008655 0.005104 0.009337 hazard_j2_t12 0.009637 0.005686 0.010396 hazard_j2_t13 0.009733 0.005743 0.010499 hazard_j2_t14 0.009388 0.005538 0.010127 hazard_j2_t15 0.008959 0.005284 0.009665 hazard_j2_t16 0.009298 0.005485 0.010030 hazard_j2_t17 0.009297 0.005484 0.010029 hazard_j2_t18 0.007866 0.004637 0.008486 hazard_j2_t19 0.009548 0.005633 0.010299 hazard_j2_t2 0.011294 0.006668 0.012181 hazard_j2_t20 0.009910 0.005848 0.010690 hazard_j2_t21 0.007608 0.004485 0.008208 hazard_j2_t22 0.009189 0.005420 0.009912 hazard_j2_t23 0.007302 0.004304 0.007878 hazard_j2_t24 0.007450 0.004391 0.008037 hazard_j2_t25 0.008095 0.004773 0.008733 hazard_j2_t26 0.008308 0.004899 0.008963 hazard_j2_t27 0.007665 0.004518 0.008269 hazard_j2_t28 0.007702 0.004540 0.008310 hazard_j2_t29 0.009894 0.005838 0.010672 hazard_j2_t3 0.012465 0.007363 0.013443 hazard_j2_t30 0.009909 0.005847 0.010689 hazard_j2_t4 0.011186 0.006604 0.012065 hazard_j2_t5 0.011108 0.006557 0.011981 hazard_j2_t6 0.010495 0.006194 0.011320 hazard_j2_t7 0.010051 0.005931 0.010841 hazard_j2_t8 0.010879 0.006422 0.011734 hazard_j2_t9 0.009558 0.005638 0.010310 prob_j1_at_t1 0.043775 0.031795 0.052250 prob_j1_at_t2 0.032956 0.024330 0.039005 prob_j1_at_t3 0.027509 0.020587 0.032323 prob_j1_at_t4 0.025043 0.018999 0.029219 prob_j1_at_t5 0.021377 0.016416 0.024785 prob_j1_at_t6 0.020961 0.016295 0.024149 prob_j1_at_t7 0.018274 0.014368 0.020932 prob_j1_at_t8 0.017586 0.013980 0.020029 prob_j1_at_t9 0.015390 0.012367 0.017435 prob_j1_at_t10 0.014965 0.012149 0.016865 prob_j1_at_t11 0.014412 0.011822 0.016157 prob_j1_at_t12 0.013870 0.011488 0.015470 prob_j1_at_t13 0.011982 0.010019 0.013301 prob_j1_at_t14 0.012445 0.010510 0.013746 prob_j1_at_t15 0.010604 0.009037 0.011661 prob_j1_at_t16 0.011026 0.009483 0.012069 prob_j1_at_t17 0.010306 0.008945 0.011229 prob_j1_at_t18 0.009365 0.008201 0.010160 prob_j1_at_t19 0.010203 0.009013 0.011019 prob_j1_at_t20 0.009188 0.008191 0.009879 prob_j1_at_t21 0.008386 0.007543 0.008978 prob_j1_at_t22 0.008535 0.007740 0.009100 prob_j1_at_t23 0.009191 0.008413 0.009752 prob_j1_at_t24 0.007096 0.006544 0.007501 prob_j1_at_t25 0.007440 0.006915 0.007834 prob_j1_at_t26 0.007788 0.007300 0.008166 prob_j1_at_t27 0.006990 0.006604 0.007299 prob_j1_at_t28 0.006113 0.005818 0.006360 prob_j1_at_t29 0.006511 0.006246 0.006748 prob_j1_at_t30 0.005218 0.005045 0.005389 prob_j2_at_t1 0.014380 0.008500 0.015506 prob_j2_at_t2 0.010637 0.006399 0.011356 prob_j2_at_t3 0.011197 0.006840 0.011855 prob_j2_at_t4 0.009615 0.005954 0.010107 prob_j2_at_t5 0.009163 0.005748 0.009565 prob_j2_at_t6 0.008337 0.005292 0.008649 prob_j2_at_t7 0.007689 0.004939 0.007928 prob_j2_at_t8 0.008040 0.005224 0.008241 prob_j2_at_t9 0.006819 0.004479 0.006950 prob_j2_at_t10 0.006860 0.004552 0.006955 prob_j2_at_t11 0.005794 0.003883 0.005844 prob_j2_at_t12 0.006257 0.004236 0.006278 prob_j2_at_t13 0.006123 0.004188 0.006112 prob_j2_at_t14 0.005736 0.003961 0.005699 prob_j2_at_t15 0.005311 0.003703 0.005251 prob_j2_at_t16 0.005364 0.003773 0.005279 prob_j2_at_t17 0.005211 0.003700 0.005105 prob_j2_at_t18 0.004287 0.003070 0.004181 prob_j2_at_t19 0.005073 0.003666 0.004927 prob_j2_at_t20 0.005114 0.003731 0.004943 prob_j2_at_t21 0.003817 0.002808 0.003674 prob_j2_at_t22 0.004498 0.003338 0.004311 prob_j2_at_t23 0.003480 0.002603 0.003321 prob_j2_at_t24 0.003455 0.002607 0.003283 prob_j2_at_t25 0.003669 0.002790 0.003473 prob_j2_at_t26 0.003674 0.002816 0.003463 prob_j2_at_t27 0.003301 0.002552 0.003099 prob_j2_at_t28 0.003238 0.002523 0.003027 prob_j2_at_t29 0.004067 0.003195 0.003788 prob_j2_at_t30 0.003969 0.003144 0.003681 cif_j1_at_t1 0.043775 0.031795 0.052250 cif_j1_at_t2 0.076731 0.056126 0.091255 cif_j1_at_t3 0.104240 0.076713 0.123578 cif_j1_at_t4 0.129283 0.095712 0.152797 cif_j1_at_t5 0.150660 0.112129 0.177582 cif_j1_at_t6 0.171621 0.128424 0.201731 cif_j1_at_t7 0.189895 0.142791 0.222664 cif_j1_at_t8 0.207480 0.156771 0.242692 cif_j1_at_t9 0.222870 0.169138 0.260127 cif_j1_at_t10 0.237835 0.181287 0.276992 cif_j1_at_t11 0.252248 0.193109 0.293148 cif_j1_at_t12 0.266118 0.204597 0.308618 cif_j1_at_t13 0.278100 0.214616 0.321919 cif_j1_at_t14 0.290544 0.225126 0.335665 cif_j1_at_t15 0.301148 0.234163 0.347326 cif_j1_at_t16 0.312174 0.243646 0.359395 cif_j1_at_t17 0.322480 0.252591 0.370624 cif_j1_at_t18 0.331845 0.260792 0.380785 cif_j1_at_t19 0.342048 0.269805 0.391804 cif_j1_at_t20 0.351236 0.277996 0.401683 cif_j1_at_t21 0.359623 0.285540 0.410661 cif_j1_at_t22 0.368158 0.293280 0.419761 cif_j1_at_t23 0.377348 0.301693 0.429513 cif_j1_at_t24 0.384444 0.308236 0.437014 cif_j1_at_t25 0.391884 0.315151 0.444848 cif_j1_at_t26 0.399673 0.322451 0.453014 cif_j1_at_t27 0.406662 0.329055 0.460313 cif_j1_at_t28 0.412775 0.334874 0.466673 cif_j1_at_t29 0.419285 0.341119 0.473422 cif_j1_at_t30 0.424503 0.346164 0.478811 cif_j2_at_t1 0.014380 0.008500 0.015506 cif_j2_at_t2 0.025018 0.014900 0.026862 cif_j2_at_t3 0.036214 0.021739 0.038717 cif_j2_at_t4 0.045829 0.027693 0.048824 cif_j2_at_t5 0.054992 0.033441 0.058389 cif_j2_at_t6 0.063329 0.038733 0.067038 cif_j2_at_t7 0.071018 0.043673 0.074965 cif_j2_at_t8 0.079059 0.048897 0.083207 cif_j2_at_t9 0.085878 0.053376 0.090156 cif_j2_at_t10 0.092737 0.057928 0.097111 cif_j2_at_t11 0.098531 0.061811 0.102955 cif_j2_at_t12 0.104788 0.066048 0.109233 cif_j2_at_t13 0.110912 0.070236 0.115345 cif_j2_at_t14 0.116647 0.074197 0.121044 cif_j2_at_t15 0.121959 0.077899 0.126295 cif_j2_at_t16 0.127322 0.081672 0.131574 cif_j2_at_t17 0.132534 0.085372 0.136679 cif_j2_at_t18 0.136820 0.088442 0.140860 cif_j2_at_t19 0.141894 0.092108 0.145787 cif_j2_at_t20 0.147008 0.095839 0.150730 cif_j2_at_t21 0.150825 0.098647 0.154404 cif_j2_at_t22 0.155324 0.101985 0.158715 cif_j2_at_t23 0.158803 0.104588 0.162036 cif_j2_at_t24 0.162259 0.107195 0.165319 cif_j2_at_t25 0.165928 0.109985 0.168792 cif_j2_at_t26 0.169602 0.112801 0.172254 cif_j2_at_t27 0.172903 0.115352 0.175353 cif_j2_at_t28 0.176141 0.117875 0.178380 cif_j2_at_t29 0.180208 0.121070 0.182168 cif_j2_at_t30 0.184177 0.124214 0.185850","title":"Prediction"},{"location":"UsageExample-Intro/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction \u00a4 PyDTS currently includes two estimation procedures: TwoStagesFitter - a python implementation of our estimation method [1]. DataExpansionFitter - a python implementation of the estimation method of Lee et al. (2018) [2]. The following example demonstrates the required data preparation steps, and applies our estimation procedure (Meir et. al., 2022) and that of Lee et al. (2018). As expected, both methods perform similarly in terms of point estimation and standard errors, while that of Meir et al (2022) is much faster. The following sections includes: Data Preparation Estimating with TwoStagesFitter TwoStagesFitter Prediction Estimating with DataExpansionFitter DataExpansionFitter Prediction For details about the estimation methods, please see Methods section. For a detailed comparison between the two estimation techniques, based on as simulation study, see Meir et al. (2022). from IPython.display import Image Image ( filename = 'models_params.png' ) References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881","title":"Introduction"},{"location":"UsageExample-Intro/#introduction","text":"PyDTS currently includes two estimation procedures: TwoStagesFitter - a python implementation of our estimation method [1]. DataExpansionFitter - a python implementation of the estimation method of Lee et al. (2018) [2]. The following example demonstrates the required data preparation steps, and applies our estimation procedure (Meir et. al., 2022) and that of Lee et al. (2018). As expected, both methods perform similarly in terms of point estimation and standard errors, while that of Meir et al (2022) is much faster. The following sections includes: Data Preparation Estimating with TwoStagesFitter TwoStagesFitter Prediction Estimating with DataExpansionFitter DataExpansionFitter Prediction For details about the estimation methods, please see Methods section. For a detailed comparison between the two estimation techniques, based on as simulation study, see Meir et al. (2022). from IPython.display import Image Image ( filename = 'models_params.png' )","title":"Introduction"},{"location":"UsageExample-Intro/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881","title":"References"},{"location":"UsageExample-RegroupingData/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Data Regrouping Example \u00a4 As previously discussed, both estimators require enough observed failures for each (j, t). Sometimes, the data do not comply with this requirement. For example, when dealing with hospitalization length of stay, patients are more likely to be released after a few days rather than after a month, and releases can be less frequent on weekends. In this example we demonstrate data regrouping that can be part of the preprocessing stage, which will allow a successful estimation. import warnings import sys import pandas as pd import numpy as np from matplotlib import pyplot as plt from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.examples_utils.plots import plot_events_occurrence , add_panel_text , plot_example_estimated_params from pydts.fitters import TwoStagesFitter pd . set_option ( \"display.max_rows\" , 500 ) pd . set_option ( \"display.max_columns\" , 25 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline Not enough observed failures in later times \u00a4 We consider a setting in which the observed events become less frequent in later times by simply reducing the sample size to \\(n=1000\\) . real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } df = generate_quick_start_df ( n_patients = 1000 , n_cov = 5 , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , real_coef_dict = real_coef_dict ) Evidently, we see that we do not observe enough events in later times. For example, \\(n_{j=1,t=25} = 1\\) and \\(n_{j=2,t=25} = 0\\) ax = plot_events_occurrence ( df ) add_panel_text ( ax , 'a' ) Trying to fit the model with such data will result in the following error message: m2 = TwoStagesFitter () try : m2 . fit ( df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [4] , in <cell line: 2> () 3 m2 . fit(df . drop(columns = [ ' C ' , ' T ' ]), verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : Number of observed events at some time points are too small. Consider collapsing neighbor time points. See https://tomer1812.github.io/pydts/UsageExample-RegroupingData/ for more details. For fixing zero events and the tail of the distribution, events occured later than the 21st day (either \\(J=1\\) or \\(J=2\\) ) are considered to be in a 21+ event time. regrouped_df = df . copy () regrouped_df [ 'X' ] . clip ( upper = 21 , inplace = True ) ax = plot_events_occurrence ( regrouped_df ) add_panel_text ( ax , 'b' ) Now, we can successfully estimate the parameters: fig , axes = plt . subplots ( 2 , 1 , figsize = ( 10 , 8 )) ax = axes [ 0 ] ax = plot_events_occurrence ( df , ax = ax ) add_panel_text ( ax , 'a' ) ax = axes [ 1 ] ax = plot_events_occurrence ( regrouped_df , ax = ax ) labels = [ item . get_text () for item in ax . get_xticklabels ()] labels [ - 1 ] = '21+' ax . set_xticklabels ( labels ) add_panel_text ( ax , 'b' ) fig . tight_layout () m2 = TwoStagesFitter () m2 . fit ( regrouped_df . drop ( columns = [ 'C' , 'T' ])) plot_example_estimated_params ( m2 ) INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. m2 . print_summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.059593 0.187268 0.433584 0.286376 Z2 -0.977635 0.192579 -0.651536 0.285630 Z3 -1.012224 0.191226 -1.147630 0.290300 Z4 -1.048233 0.180891 -0.221056 0.272420 Z5 -0.817598 0.180479 -0.475688 0.272666 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 63 True -0.947684 2 49 True -1.051086 3 34 True -1.295866 4 34 True -1.178560 5 15 True -1.903136 6 25 True -1.298924 7 20 True -1.401847 8 17 True -1.451159 9 11 True -1.788478 10 12 True -1.564731 11 15 True -1.247966 12 12 True -1.373390 13 7 True -1.834668 14 5 True -2.060652 15 14 True -0.884086 16 5 True -1.761146 17 5 True -1.645269 18 4 True -1.729781 19 1 True -2.928615 20 3 True -1.769298 21 8 True -0.566276 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 24 True -2.770174 2 24 True -2.619309 3 13 True -3.105049 4 11 True -3.164241 5 9 True -3.269706 6 12 True -2.900518 7 14 True -2.616379 8 6 True -3.361561 9 13 True -2.468053 10 3 True -3.827000 11 1 True -4.497481 12 3 True -3.627228 13 4 True -3.288580 14 3 True -3.455462 15 3 True -3.369194 16 3 True -3.193872 17 1 True -4.094674 18 2 True -3.325382 19 2 True -3.218786 20 1 True -3.768195 21 4 True -2.222326 Not enough observed events at specific times \u00a4 Consider the case of almost no discharge events during the weekends. In the following we resample the data to reflect this setting: from random import random def map_days ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ] and row [ 'J' ] in [ 1 ]: if ( random () > 0.1 ) or ( row [ 'X' ] == 21 ): row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row regrouped_df = regrouped_df . apply ( map_days , axis = 1 ) regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] = regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) ( regrouped_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 J 0 30.0 20.0 28.0 21.0 22.0 22.0 23.0 21.0 20.0 11.0 18.0 15.0 15.0 13.0 21.0 16.0 19.0 14.0 14.0 14.0 108.0 1 63.0 49.0 34.0 34.0 15.0 43.0 2.0 17.0 11.0 12.0 15.0 12.0 11.0 1.0 14.0 5.0 5.0 4.0 1.0 11.0 NaN 2 24.0 24.0 13.0 11.0 9.0 12.0 14.0 6.0 13.0 3.0 1.0 3.0 4.0 3.0 3.0 3.0 1.0 2.0 2.0 1.0 4.0 df = regrouped_df . copy () plot_events_occurrence ( regrouped_df ) <AxesSubplot:xlabel='Time', ylabel='Number of Observations'> Trying to fit the model with such data will result in the following error message: m2 = TwoStagesFitter () try : m2 . fit ( regrouped_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [11] , in <cell line: 2> () 3 m2 . fit(regrouped_df . drop(columns = [ ' C ' , ' T ' ]), verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : Number of observed events at some time points are too small. Consider collapsing neighbor time points. See https://tomer1812.github.io/pydts/UsageExample-RegroupingData/ for more details. We suggest to regroup empty times with the preceding days: def map_days_second_try ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ]: row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row regrouped_df = regrouped_df . apply ( map_days_second_try , axis = 1 ) regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] = regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) ( regrouped_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 8 9 10 11 12 13 15 16 17 18 19 20 J 0 30 20 28 21 22 45 21 20 11 18 15 28 21 16 19 14 14 122 1 63 49 34 34 15 45 17 11 12 15 12 12 14 5 5 4 1 11 2 24 24 13 11 9 26 6 13 3 1 3 7 3 3 1 2 2 5 plot_events_occurrence ( regrouped_df ) <AxesSubplot:xlabel='Time', ylabel='Number of Observations'> fig , axes = plt . subplots ( 2 , 1 , figsize = ( 10 , 8 )) ax = axes [ 0 ] ax = plot_events_occurrence ( df , ax = ax ) add_panel_text ( ax , 'a' ) ax = axes [ 1 ] ax = plot_events_occurrence ( regrouped_df , ax = ax ) labels = [ item . get_text () for item in ax . get_xticklabels ()] labels [ 5 ] = '6-7' labels [ 11 ] = '13-14' labels [ 17 ] = '20-21' ax . set_xticklabels ( labels ) add_panel_text ( ax , 'b' ) fig . tight_layout () Now, we can estimate the parameters, while the interpretation of the parameters related to the grouped time points should be interpreted with care. m2 = TwoStagesFitter () m2 . fit ( regrouped_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) plot_example_estimated_params ( m2 ) m2 . print_summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.125497 0.226571 0.559011 0.344835 Z2 -0.590350 0.230030 -0.146264 0.338669 Z3 -0.577012 0.230431 -0.601869 0.343531 Z4 -0.813756 0.221557 0.119530 0.328842 Z5 -0.586253 0.231495 -0.254760 0.345008 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 63 True -1.544779 2 49 True -1.658131 3 34 True -1.911357 4 34 True -1.795520 5 15 True -2.530238 6 45 True -1.296992 8 17 True -2.090432 9 11 True -2.431846 10 12 True -2.211441 11 15 True -1.904531 12 12 True -2.032937 13 12 True -1.938316 15 14 True -1.565972 16 5 True -2.453997 17 5 True -2.335441 18 4 True -2.424637 19 1 True -3.647561 20 11 True -1.103130 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 24 True -3.574037 2 24 True -3.442172 3 13 True -3.946532 4 11 True -4.010704 5 9 True -4.104630 6 26 True -2.947698 8 6 True -4.205070 9 13 True -3.331808 10 3 True -4.678181 11 1 True -5.489276 12 3 True -4.531819 13 7 True -3.595795 15 3 True -4.254731 16 3 True -4.099612 17 1 True -4.963138 18 2 True -4.249112 19 2 True -4.146457 20 5 True -3.099039","title":"Data Regrouping Example"},{"location":"UsageExample-RegroupingData/#data-regrouping-example","text":"As previously discussed, both estimators require enough observed failures for each (j, t). Sometimes, the data do not comply with this requirement. For example, when dealing with hospitalization length of stay, patients are more likely to be released after a few days rather than after a month, and releases can be less frequent on weekends. In this example we demonstrate data regrouping that can be part of the preprocessing stage, which will allow a successful estimation. import warnings import sys import pandas as pd import numpy as np from matplotlib import pyplot as plt from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.examples_utils.plots import plot_events_occurrence , add_panel_text , plot_example_estimated_params from pydts.fitters import TwoStagesFitter pd . set_option ( \"display.max_rows\" , 500 ) pd . set_option ( \"display.max_columns\" , 25 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline","title":"Data Regrouping Example"},{"location":"UsageExample-RegroupingData/#not-enough-observed-failures-in-later-times","text":"We consider a setting in which the observed events become less frequent in later times by simply reducing the sample size to \\(n=1000\\) . real_coef_dict = { \"alpha\" : { 1 : lambda t : - 1 - 0.3 * np . log ( t ), 2 : lambda t : - 1.75 - 0.15 * np . log ( t ) }, \"beta\" : { 1 : - np . log ([ 0.8 , 3 , 3 , 2.5 , 2 ]), 2 : - np . log ([ 1 , 3 , 4 , 3 , 2 ]) } } df = generate_quick_start_df ( n_patients = 1000 , n_cov = 5 , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 , real_coef_dict = real_coef_dict ) Evidently, we see that we do not observe enough events in later times. For example, \\(n_{j=1,t=25} = 1\\) and \\(n_{j=2,t=25} = 0\\) ax = plot_events_occurrence ( df ) add_panel_text ( ax , 'a' ) Trying to fit the model with such data will result in the following error message: m2 = TwoStagesFitter () try : m2 . fit ( df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [4] , in <cell line: 2> () 3 m2 . fit(df . drop(columns = [ ' C ' , ' T ' ]), verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : Number of observed events at some time points are too small. Consider collapsing neighbor time points. See https://tomer1812.github.io/pydts/UsageExample-RegroupingData/ for more details. For fixing zero events and the tail of the distribution, events occured later than the 21st day (either \\(J=1\\) or \\(J=2\\) ) are considered to be in a 21+ event time. regrouped_df = df . copy () regrouped_df [ 'X' ] . clip ( upper = 21 , inplace = True ) ax = plot_events_occurrence ( regrouped_df ) add_panel_text ( ax , 'b' ) Now, we can successfully estimate the parameters: fig , axes = plt . subplots ( 2 , 1 , figsize = ( 10 , 8 )) ax = axes [ 0 ] ax = plot_events_occurrence ( df , ax = ax ) add_panel_text ( ax , 'a' ) ax = axes [ 1 ] ax = plot_events_occurrence ( regrouped_df , ax = ax ) labels = [ item . get_text () for item in ax . get_xticklabels ()] labels [ - 1 ] = '21+' ax . set_xticklabels ( labels ) add_panel_text ( ax , 'b' ) fig . tight_layout () m2 = TwoStagesFitter () m2 . fit ( regrouped_df . drop ( columns = [ 'C' , 'T' ])) plot_example_estimated_params ( m2 ) INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. m2 . print_summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.059593 0.187268 0.433584 0.286376 Z2 -0.977635 0.192579 -0.651536 0.285630 Z3 -1.012224 0.191226 -1.147630 0.290300 Z4 -1.048233 0.180891 -0.221056 0.272420 Z5 -0.817598 0.180479 -0.475688 0.272666 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 63 True -0.947684 2 49 True -1.051086 3 34 True -1.295866 4 34 True -1.178560 5 15 True -1.903136 6 25 True -1.298924 7 20 True -1.401847 8 17 True -1.451159 9 11 True -1.788478 10 12 True -1.564731 11 15 True -1.247966 12 12 True -1.373390 13 7 True -1.834668 14 5 True -2.060652 15 14 True -0.884086 16 5 True -1.761146 17 5 True -1.645269 18 4 True -1.729781 19 1 True -2.928615 20 3 True -1.769298 21 8 True -0.566276 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 24 True -2.770174 2 24 True -2.619309 3 13 True -3.105049 4 11 True -3.164241 5 9 True -3.269706 6 12 True -2.900518 7 14 True -2.616379 8 6 True -3.361561 9 13 True -2.468053 10 3 True -3.827000 11 1 True -4.497481 12 3 True -3.627228 13 4 True -3.288580 14 3 True -3.455462 15 3 True -3.369194 16 3 True -3.193872 17 1 True -4.094674 18 2 True -3.325382 19 2 True -3.218786 20 1 True -3.768195 21 4 True -2.222326","title":"Not enough observed failures in later times"},{"location":"UsageExample-RegroupingData/#not-enough-observed-events-at-specific-times","text":"Consider the case of almost no discharge events during the weekends. In the following we resample the data to reflect this setting: from random import random def map_days ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ] and row [ 'J' ] in [ 1 ]: if ( random () > 0.1 ) or ( row [ 'X' ] == 21 ): row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row regrouped_df = regrouped_df . apply ( map_days , axis = 1 ) regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] = regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) ( regrouped_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 J 0 30.0 20.0 28.0 21.0 22.0 22.0 23.0 21.0 20.0 11.0 18.0 15.0 15.0 13.0 21.0 16.0 19.0 14.0 14.0 14.0 108.0 1 63.0 49.0 34.0 34.0 15.0 43.0 2.0 17.0 11.0 12.0 15.0 12.0 11.0 1.0 14.0 5.0 5.0 4.0 1.0 11.0 NaN 2 24.0 24.0 13.0 11.0 9.0 12.0 14.0 6.0 13.0 3.0 1.0 3.0 4.0 3.0 3.0 3.0 1.0 2.0 2.0 1.0 4.0 df = regrouped_df . copy () plot_events_occurrence ( regrouped_df ) <AxesSubplot:xlabel='Time', ylabel='Number of Observations'> Trying to fit the model with such data will result in the following error message: m2 = TwoStagesFitter () try : m2 . fit ( regrouped_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [11] , in <cell line: 2> () 3 m2 . fit(regrouped_df . drop(columns = [ ' C ' , ' T ' ]), verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : Number of observed events at some time points are too small. Consider collapsing neighbor time points. See https://tomer1812.github.io/pydts/UsageExample-RegroupingData/ for more details. We suggest to regroup empty times with the preceding days: def map_days_second_try ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ]: row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row regrouped_df = regrouped_df . apply ( map_days_second_try , axis = 1 ) regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] = regrouped_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) ( regrouped_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 8 9 10 11 12 13 15 16 17 18 19 20 J 0 30 20 28 21 22 45 21 20 11 18 15 28 21 16 19 14 14 122 1 63 49 34 34 15 45 17 11 12 15 12 12 14 5 5 4 1 11 2 24 24 13 11 9 26 6 13 3 1 3 7 3 3 1 2 2 5 plot_events_occurrence ( regrouped_df ) <AxesSubplot:xlabel='Time', ylabel='Number of Observations'> fig , axes = plt . subplots ( 2 , 1 , figsize = ( 10 , 8 )) ax = axes [ 0 ] ax = plot_events_occurrence ( df , ax = ax ) add_panel_text ( ax , 'a' ) ax = axes [ 1 ] ax = plot_events_occurrence ( regrouped_df , ax = ax ) labels = [ item . get_text () for item in ax . get_xticklabels ()] labels [ 5 ] = '6-7' labels [ 11 ] = '13-14' labels [ 17 ] = '20-21' ax . set_xticklabels ( labels ) add_panel_text ( ax , 'b' ) fig . tight_layout () Now, we can estimate the parameters, while the interpretation of the parameters related to the grouped time points should be interpreted with care. m2 = TwoStagesFitter () m2 . fit ( regrouped_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) plot_example_estimated_params ( m2 ) m2 . print_summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } j1_params j1_SE j2_params j2_SE covariate Z1 0.125497 0.226571 0.559011 0.344835 Z2 -0.590350 0.230030 -0.146264 0.338669 Z3 -0.577012 0.230431 -0.601869 0.343531 Z4 -0.813756 0.221557 0.119530 0.328842 Z5 -0.586253 0.231495 -0.254760 0.345008 Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 63 True -1.544779 2 49 True -1.658131 3 34 True -1.911357 4 34 True -1.795520 5 15 True -2.530238 6 45 True -1.296992 8 17 True -2.090432 9 11 True -2.431846 10 12 True -2.211441 11 15 True -1.904531 12 12 True -2.032937 13 12 True -1.938316 15 14 True -1.565972 16 5 True -2.453997 17 5 True -2.335441 18 4 True -2.424637 19 1 True -3.647561 20 11 True -1.103130 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 24 True -3.574037 2 24 True -3.442172 3 13 True -3.946532 4 11 True -4.010704 5 9 True -4.104630 6 26 True -2.947698 8 6 True -4.205070 9 13 True -3.331808 10 3 True -4.678181 11 1 True -5.489276 12 3 True -4.531819 13 7 True -3.595795 15 3 True -4.254731 16 3 True -4.099612 17 1 True -4.963138 18 2 True -4.249112 19 2 True -4.146457 20 5 True -3.099039","title":"Not enough observed events at specific times"},{"location":"User%20Story/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Guide for dealing with problem definition \u00a4 import warnings import sys import pandas as pd from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.fitters import TwoStagesFitter pd . set_option ( \"display.max_rows\" , 500 ) pd . set_option ( \"display.max_columns\" , 25 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline Case study #1: Not enough cases in the tail \u00a4 For the first example we would consider settings in which the cases are becoming less frequent in the tail end of the data df = generate_quick_start_df ( n_patients = 1000 , n_cov = 5 , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 ) ( df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 ... 19 20 21 22 23 24 25 26 27 28 29 30 J 0 24.0 25.0 25.0 24.0 19.0 19.0 22.0 17.0 23.0 20.0 15.0 16.0 ... 16.0 13.0 20.0 13.0 14.0 14.0 12.0 9.0 7.0 11.0 6.0 9.0 1 74.0 41.0 36.0 33.0 21.0 25.0 20.0 13.0 11.0 13.0 10.0 10.0 ... 2.0 4.0 2.0 5.0 1.0 6.0 1.0 2.0 1.0 2.0 NaN 1.0 2 29.0 11.0 19.0 15.0 17.0 4.0 7.0 6.0 8.0 7.0 11.0 3.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 1.0 3 rows \u00d7 30 columns m2 = TwoStagesFitter () try : m2 . fit ( df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [3] , in <cell line: 2> () 3 m2 . fit(df . drop(columns = [ ' C ' , ' T ' ]),verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : In event J=1, The method didn't have events D=[29]. Consider changing the problem definition. See TBD for more details. In this case we see that the method doesn't have events for D=29. let's see the distrubtion: axes = df . hist ( column = [ 'X' ], by = [ 'J' ], bins = 30 , figsize = ( 10 , 10 )) for ax in axes . flatten (): ax . set_xlim ( 0 , 30 ) As one can easily see from the data, We don't have events during the 25-30 days. \\ for example - we can think about a patients length of stay in hospital - patints are more likely to stay in hospital or to leave hospital soon after hospitalization, but only few are leaving after it.\\ To tackle this challenge, we can induce adminstrative censorship, such that patients that had event (either \\(J=1\\) or \\(J=2\\) ) after the 25th day (25+), is considered to be similar to patient that had event in the 25th day. temp_df = df . copy () # so we don't change the original data temp_df [ 'X' ] . clip ( upper = 25 , inplace = True ) # we are clipping all the patients over 25 days to 25 m2 = TwoStagesFitter () m2 . fit ( temp_df . drop ( columns = [ 'C' , 'T' ])) m2 . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_1' strata X_copy baseline estimation breslow number of observations 9560 number of events observed 375 partial log-likelihood -2313.33 time fit was run 2022-03-22 16:13:49 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.09 1.09 0.18 -0.26 0.44 0.77 1.56 0.50 0.62 0.70 Z2 -1.04 0.35 0.18 -1.41 -0.68 0.24 0.51 -5.65 <0.005 25.93 Z3 -1.07 0.34 0.19 -1.44 -0.71 0.24 0.49 -5.81 <0.005 27.26 Z4 -0.53 0.59 0.17 -0.88 -0.19 0.42 0.82 -3.08 <0.005 8.91 Z5 -0.52 0.59 0.18 -0.87 -0.18 0.42 0.84 -2.98 <0.005 8.46 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.63 Partial AIC 4636.66 log-likelihood ratio test 81.75 on 5 df -log2(p) of ll-ratio test 51.30 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 74 True -1.095820 2 41 True -1.564119 3 36 True -1.580492 4 33 True -1.542081 5 21 True -1.896471 6 25 True -1.598934 7 20 True -1.737360 8 13 True -2.066961 9 11 True -2.135509 10 13 True -1.859094 11 10 True -2.002466 12 10 True -1.879609 13 5 True -2.486454 14 12 True -1.501572 15 9 True -1.703977 16 7 True -1.856787 17 5 True -2.067510 18 3 True -2.482490 19 2 True -2.806679 20 4 True -1.998939 21 2 True -2.568323 22 5 True -1.487019 23 1 True -2.914662 24 6 True -0.859760 25 7 True -0.335299 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_2' strata X_copy baseline estimation breslow number of observations 9560 number of events observed 161 partial log-likelihood -987.17 time fit was run 2022-03-22 16:13:50 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.17 1.18 0.27 -0.37 0.71 0.69 2.02 0.61 0.54 0.89 Z2 -1.08 0.34 0.28 -1.63 -0.53 0.20 0.59 -3.85 <0.005 13.03 Z3 -1.67 0.19 0.29 -2.24 -1.10 0.11 0.33 -5.73 <0.005 26.59 Z4 -0.67 0.51 0.27 -1.19 -0.15 0.30 0.86 -2.53 0.01 6.46 Z5 -0.38 0.69 0.27 -0.90 0.15 0.41 1.16 -1.41 0.16 2.66 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.66 Partial AIC 1984.33 log-likelihood ratio test 56.08 on 5 df -log2(p) of ll-ratio test 33.57 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 29 True -1.889441 2 11 True -2.713222 3 19 True -2.027058 4 15 True -2.141383 5 17 True -1.889341 6 4 True -3.162640 7 7 True -2.589820 8 6 True -2.631269 9 8 True -2.229403 10 7 True -2.257762 11 11 True -1.664896 12 3 True -2.836411 13 5 True -2.246092 14 1 True -3.607395 15 3 True -2.582261 16 4 True -2.176043 17 1 True -3.368219 18 2 True -2.626271 19 1 True -3.202843 20 1 True -3.042474 21 1 True -2.958991 22 1 True -2.840443 23 1 True -2.655788 24 1 True -2.408854 25 2 True -1.382297 Now, we can predict the surviavl curves for patients that had events ( \\(J\\in\\{1,2\\}\\) ) in days 1-24 or in day 25 and after. Case study #2: Not enough cases in the middle \u00a4 In this example we examine a case where there is no events during the weekend, but only cencoreship. This use-case shows possible solution of combining those events into stucks of \"weekend\". ( temp_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 J 0 24 25 25 24 19 19 22 17 23 20 15 16 14 12 7 16 11 11 16 13 20 13 14 14 54 1 74 41 36 33 21 25 20 13 11 13 10 10 5 12 9 7 5 3 2 4 2 5 1 6 7 2 29 11 19 15 17 4 7 6 8 7 11 3 5 1 3 4 1 2 1 1 1 1 1 1 2 def map_days ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ] and row [ 'J' ] in [ 1 , 2 ]: row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row temp_df = temp_df . apply ( map_days , axis = 1 ) temp_df [[ 'J' , 'T' , 'C' , 'X' ]] = temp_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) As expected in this case, the fitter is falied to converage, because it lacks events in days 7,14,21 (\"The weekend\") m2 = TwoStagesFitter () try : m2 . fit ( temp_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [8] , in <cell line: 2> () 3 m2 . fit(temp_df . drop(columns = [ ' C ' , ' T ' ]), verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : In event J=1, The method didn't have events D=[7, 14, 21]. Consider changing the problem definition. See TBD for more details. And we get error as expected Further exploring the disribution, we notice that we have a \"hole\" during the weekends, so we need do fix this behavior display (( temp_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack ()) axes = temp_df . hist ( column = [ 'X' ], by = [ 'J' ], bins = 25 , figsize = ( 10 , 10 )) for ax in axes . flatten (): ax . set_xlim ( 0 , 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 J 0 24.0 25.0 25.0 24.0 19.0 19.0 22.0 17.0 23.0 20.0 15.0 16.0 14.0 12.0 7.0 16.0 11.0 11.0 16.0 13.0 20.0 13.0 14.0 14.0 54.0 1 74.0 41.0 36.0 33.0 21.0 45.0 NaN 13.0 11.0 13.0 10.0 10.0 17.0 NaN 9.0 7.0 5.0 3.0 2.0 6.0 NaN 5.0 1.0 6.0 7.0 2 29.0 11.0 19.0 15.0 17.0 11.0 NaN 6.0 8.0 7.0 11.0 3.0 6.0 NaN 3.0 4.0 1.0 2.0 1.0 2.0 NaN 1.0 1.0 1.0 2.0 One of such fixes can be stacking the events which occurs during the weekend, as one \"bucket\". def map_days_second_try ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ] and row [ 'J' ] == 0 : row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row temp_df = temp_df . apply ( map_days_second_try , axis = 1 ) temp_df [[ 'J' , 'T' , 'C' , 'X' ]] = temp_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) display (( temp_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack ()) axes = temp_df . hist ( column = [ 'X' ], by = [ 'J' ], bins = 25 , figsize = ( 10 , 10 )) for ax in axes . flatten (): ax . set_xlim ( 0 , 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 8 9 10 11 12 13 15 16 17 18 19 20 22 23 24 25 J 0 24 25 25 24 19 41 17 23 20 15 16 26 7 16 11 11 16 33 13 14 14 54 1 74 41 36 33 21 45 13 11 13 10 10 17 9 7 5 3 2 6 5 1 6 7 2 29 11 19 15 17 11 6 8 7 11 3 6 3 4 1 2 1 2 1 1 1 2 Now, we can our fit would work as planned, and the model provides \\(\\alpha\\) only for the times we provided. \\ Thus, we should note our model can't predict during those days. m2 = TwoStagesFitter () m2 . fit ( temp_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) m2 . print_summary () Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_1' strata X_copy baseline estimation breslow number of observations 9463 number of events observed 375 partial log-likelihood -2315.15 time fit was run 2022-03-22 16:14:19 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.09 1.09 0.18 -0.27 0.44 0.76 1.55 0.48 0.63 0.66 Z2 -1.04 0.35 0.18 -1.41 -0.68 0.25 0.51 -5.65 <0.005 25.87 Z3 -1.08 0.34 0.18 -1.44 -0.72 0.24 0.49 -5.84 <0.005 27.49 Z4 -0.53 0.59 0.17 -0.87 -0.19 0.42 0.83 -3.06 <0.005 8.82 Z5 -0.52 0.59 0.18 -0.86 -0.18 0.42 0.84 -2.96 <0.005 8.37 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.63 Partial AIC 4640.29 log-likelihood ratio test 81.78 on 5 df -log2(p) of ll-ratio test 51.32 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 74 True -1.095262 2 41 True -1.563535 3 36 True -1.579912 4 33 True -1.541470 5 21 True -1.895841 6 45 True -0.967645 8 13 True -2.066400 9 11 True -2.134931 10 13 True -1.858564 11 10 True -2.001975 12 10 True -1.879093 13 17 True -1.223394 15 9 True -1.703488 16 7 True -1.856321 17 5 True -2.067005 18 3 True -2.481967 19 2 True -2.806130 20 6 True -1.579369 22 5 True -1.486437 23 1 True -2.914180 24 6 True -0.859284 25 7 True -0.334542 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_2' strata X_copy baseline estimation breslow number of observations 9463 number of events observed 161 partial log-likelihood -987.94 time fit was run 2022-03-22 16:14:20 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.16 1.18 0.27 -0.37 0.70 0.69 2.02 0.60 0.55 0.87 Z2 -1.08 0.34 0.28 -1.63 -0.53 0.20 0.59 -3.83 <0.005 12.92 Z3 -1.67 0.19 0.29 -2.24 -1.10 0.11 0.33 -5.74 <0.005 26.68 Z4 -0.67 0.51 0.27 -1.19 -0.15 0.31 0.86 -2.52 0.01 6.39 Z5 -0.38 0.69 0.27 -0.90 0.15 0.41 1.16 -1.40 0.16 2.64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.66 Partial AIC 1985.88 log-likelihood ratio test 55.99 on 5 df -log2(p) of ll-ratio test 33.51 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 29 True -1.891318 2 11 True -2.715119 3 19 True -2.029019 4 15 True -2.143387 5 17 True -1.891328 6 11 True -2.224464 8 6 True -2.633376 9 8 True -2.231573 10 7 True -2.259989 11 11 True -1.667235 12 3 True -2.838670 13 6 True -2.067073 15 3 True -2.584596 16 4 True -2.178489 17 1 True -3.370445 18 2 True -2.628716 19 1 True -3.205144 20 2 True -2.415453 22 1 True -2.842736 23 1 True -2.658322 24 1 True -2.411368 25 2 True -1.384788","title":"User Story"},{"location":"User%20Story/#guide-for-dealing-with-problem-definition","text":"import warnings import sys import pandas as pd from pydts.examples_utils.generate_simulations_data import generate_quick_start_df from pydts.fitters import TwoStagesFitter pd . set_option ( \"display.max_rows\" , 500 ) pd . set_option ( \"display.max_columns\" , 25 ) warnings . filterwarnings ( 'ignore' ) % matplotlib inline","title":"Guide for dealing with problem definition"},{"location":"User%20Story/#case-study-1-not-enough-cases-in-the-tail","text":"For the first example we would consider settings in which the cases are becoming less frequent in the tail end of the data df = generate_quick_start_df ( n_patients = 1000 , n_cov = 5 , d_times = 30 , j_events = 2 , pid_col = 'pid' , seed = 0 ) ( df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 ... 19 20 21 22 23 24 25 26 27 28 29 30 J 0 24.0 25.0 25.0 24.0 19.0 19.0 22.0 17.0 23.0 20.0 15.0 16.0 ... 16.0 13.0 20.0 13.0 14.0 14.0 12.0 9.0 7.0 11.0 6.0 9.0 1 74.0 41.0 36.0 33.0 21.0 25.0 20.0 13.0 11.0 13.0 10.0 10.0 ... 2.0 4.0 2.0 5.0 1.0 6.0 1.0 2.0 1.0 2.0 NaN 1.0 2 29.0 11.0 19.0 15.0 17.0 4.0 7.0 6.0 8.0 7.0 11.0 3.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 1.0 3 rows \u00d7 30 columns m2 = TwoStagesFitter () try : m2 . fit ( df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [3] , in <cell line: 2> () 3 m2 . fit(df . drop(columns = [ ' C ' , ' T ' ]),verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : In event J=1, The method didn't have events D=[29]. Consider changing the problem definition. See TBD for more details. In this case we see that the method doesn't have events for D=29. let's see the distrubtion: axes = df . hist ( column = [ 'X' ], by = [ 'J' ], bins = 30 , figsize = ( 10 , 10 )) for ax in axes . flatten (): ax . set_xlim ( 0 , 30 ) As one can easily see from the data, We don't have events during the 25-30 days. \\ for example - we can think about a patients length of stay in hospital - patints are more likely to stay in hospital or to leave hospital soon after hospitalization, but only few are leaving after it.\\ To tackle this challenge, we can induce adminstrative censorship, such that patients that had event (either \\(J=1\\) or \\(J=2\\) ) after the 25th day (25+), is considered to be similar to patient that had event in the 25th day. temp_df = df . copy () # so we don't change the original data temp_df [ 'X' ] . clip ( upper = 25 , inplace = True ) # we are clipping all the patients over 25 days to 25 m2 = TwoStagesFitter () m2 . fit ( temp_df . drop ( columns = [ 'C' , 'T' ])) m2 . print_summary () INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers. Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_1' strata X_copy baseline estimation breslow number of observations 9560 number of events observed 375 partial log-likelihood -2313.33 time fit was run 2022-03-22 16:13:49 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.09 1.09 0.18 -0.26 0.44 0.77 1.56 0.50 0.62 0.70 Z2 -1.04 0.35 0.18 -1.41 -0.68 0.24 0.51 -5.65 <0.005 25.93 Z3 -1.07 0.34 0.19 -1.44 -0.71 0.24 0.49 -5.81 <0.005 27.26 Z4 -0.53 0.59 0.17 -0.88 -0.19 0.42 0.82 -3.08 <0.005 8.91 Z5 -0.52 0.59 0.18 -0.87 -0.18 0.42 0.84 -2.98 <0.005 8.46 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.63 Partial AIC 4636.66 log-likelihood ratio test 81.75 on 5 df -log2(p) of ll-ratio test 51.30 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 74 True -1.095820 2 41 True -1.564119 3 36 True -1.580492 4 33 True -1.542081 5 21 True -1.896471 6 25 True -1.598934 7 20 True -1.737360 8 13 True -2.066961 9 11 True -2.135509 10 13 True -1.859094 11 10 True -2.002466 12 10 True -1.879609 13 5 True -2.486454 14 12 True -1.501572 15 9 True -1.703977 16 7 True -1.856787 17 5 True -2.067510 18 3 True -2.482490 19 2 True -2.806679 20 4 True -1.998939 21 2 True -2.568323 22 5 True -1.487019 23 1 True -2.914662 24 6 True -0.859760 25 7 True -0.335299 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_2' strata X_copy baseline estimation breslow number of observations 9560 number of events observed 161 partial log-likelihood -987.17 time fit was run 2022-03-22 16:13:50 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.17 1.18 0.27 -0.37 0.71 0.69 2.02 0.61 0.54 0.89 Z2 -1.08 0.34 0.28 -1.63 -0.53 0.20 0.59 -3.85 <0.005 13.03 Z3 -1.67 0.19 0.29 -2.24 -1.10 0.11 0.33 -5.73 <0.005 26.59 Z4 -0.67 0.51 0.27 -1.19 -0.15 0.30 0.86 -2.53 0.01 6.46 Z5 -0.38 0.69 0.27 -0.90 0.15 0.41 1.16 -1.41 0.16 2.66 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.66 Partial AIC 1984.33 log-likelihood ratio test 56.08 on 5 df -log2(p) of ll-ratio test 33.57 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 29 True -1.889441 2 11 True -2.713222 3 19 True -2.027058 4 15 True -2.141383 5 17 True -1.889341 6 4 True -3.162640 7 7 True -2.589820 8 6 True -2.631269 9 8 True -2.229403 10 7 True -2.257762 11 11 True -1.664896 12 3 True -2.836411 13 5 True -2.246092 14 1 True -3.607395 15 3 True -2.582261 16 4 True -2.176043 17 1 True -3.368219 18 2 True -2.626271 19 1 True -3.202843 20 1 True -3.042474 21 1 True -2.958991 22 1 True -2.840443 23 1 True -2.655788 24 1 True -2.408854 25 2 True -1.382297 Now, we can predict the surviavl curves for patients that had events ( \\(J\\in\\{1,2\\}\\) ) in days 1-24 or in day 25 and after.","title":"Case study #1: Not enough cases in the tail"},{"location":"User%20Story/#case-study-2-not-enough-cases-in-the-middle","text":"In this example we examine a case where there is no events during the weekend, but only cencoreship. This use-case shows possible solution of combining those events into stucks of \"weekend\". ( temp_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 J 0 24 25 25 24 19 19 22 17 23 20 15 16 14 12 7 16 11 11 16 13 20 13 14 14 54 1 74 41 36 33 21 25 20 13 11 13 10 10 5 12 9 7 5 3 2 4 2 5 1 6 7 2 29 11 19 15 17 4 7 6 8 7 11 3 5 1 3 4 1 2 1 1 1 1 1 1 2 def map_days ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ] and row [ 'J' ] in [ 1 , 2 ]: row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row temp_df = temp_df . apply ( map_days , axis = 1 ) temp_df [[ 'J' , 'T' , 'C' , 'X' ]] = temp_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) As expected in this case, the fitter is falied to converage, because it lacks events in days 7,14,21 (\"The weekend\") m2 = TwoStagesFitter () try : m2 . fit ( temp_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) except RuntimeError as e : raise e . with_traceback ( None ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Input In [8] , in <cell line: 2> () 3 m2 . fit(temp_df . drop(columns = [ ' C ' , ' T ' ]), verbose = 0 ) 4 except RuntimeError as e: ----> 5 raise e . with_traceback( None ) RuntimeError : In event J=1, The method didn't have events D=[7, 14, 21]. Consider changing the problem definition. See TBD for more details. And we get error as expected Further exploring the disribution, we notice that we have a \"hole\" during the weekends, so we need do fix this behavior display (( temp_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack ()) axes = temp_df . hist ( column = [ 'X' ], by = [ 'J' ], bins = 25 , figsize = ( 10 , 10 )) for ax in axes . flatten (): ax . set_xlim ( 0 , 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 J 0 24.0 25.0 25.0 24.0 19.0 19.0 22.0 17.0 23.0 20.0 15.0 16.0 14.0 12.0 7.0 16.0 11.0 11.0 16.0 13.0 20.0 13.0 14.0 14.0 54.0 1 74.0 41.0 36.0 33.0 21.0 45.0 NaN 13.0 11.0 13.0 10.0 10.0 17.0 NaN 9.0 7.0 5.0 3.0 2.0 6.0 NaN 5.0 1.0 6.0 7.0 2 29.0 11.0 19.0 15.0 17.0 11.0 NaN 6.0 8.0 7.0 11.0 3.0 6.0 NaN 3.0 4.0 1.0 2.0 1.0 2.0 NaN 1.0 1.0 1.0 2.0 One of such fixes can be stacking the events which occurs during the weekend, as one \"bucket\". def map_days_second_try ( row ): if row [ 'X' ] in [ 7 , 14 , 21 ] and row [ 'J' ] == 0 : row [ 'X' ] -= 1 row [ 'X' ] . astype ( int ) return row temp_df = temp_df . apply ( map_days_second_try , axis = 1 ) temp_df [[ 'J' , 'T' , 'C' , 'X' ]] = temp_df [[ 'J' , 'T' , 'C' , 'X' ]] . astype ( 'int64' ) display (( temp_df . groupby ([ 'J' ])[ 'X' ] . value_counts ()) . to_frame () . unstack ()) axes = temp_df . hist ( column = [ 'X' ], by = [ 'J' ], bins = 25 , figsize = ( 10 , 10 )) for ax in axes . flatten (): ax . set_xlim ( 0 , 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } X X 1 2 3 4 5 6 8 9 10 11 12 13 15 16 17 18 19 20 22 23 24 25 J 0 24 25 25 24 19 41 17 23 20 15 16 26 7 16 11 11 16 33 13 14 14 54 1 74 41 36 33 21 45 13 11 13 10 10 17 9 7 5 3 2 6 5 1 6 7 2 29 11 19 15 17 11 6 8 7 11 3 6 3 4 1 2 1 2 1 1 1 2 Now, we can our fit would work as planned, and the model provides \\(\\alpha\\) only for the times we provided. \\ Thus, we should note our model can't predict during those days. m2 = TwoStagesFitter () m2 . fit ( temp_df . drop ( columns = [ 'C' , 'T' ]), verbose = 0 ) m2 . print_summary () Model summary for event: 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_1' strata X_copy baseline estimation breslow number of observations 9463 number of events observed 375 partial log-likelihood -2315.15 time fit was run 2022-03-22 16:14:19 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.09 1.09 0.18 -0.27 0.44 0.76 1.55 0.48 0.63 0.66 Z2 -1.04 0.35 0.18 -1.41 -0.68 0.25 0.51 -5.65 <0.005 25.87 Z3 -1.08 0.34 0.18 -1.44 -0.72 0.24 0.49 -5.84 <0.005 27.49 Z4 -0.53 0.59 0.17 -0.87 -0.19 0.42 0.83 -3.06 <0.005 8.82 Z5 -0.52 0.59 0.18 -0.86 -0.18 0.42 0.84 -2.96 <0.005 8.37 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.63 Partial AIC 4640.29 log-likelihood ratio test 81.78 on 5 df -log2(p) of ll-ratio test 51.32 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 1 1 74 True -1.095262 2 41 True -1.563535 3 36 True -1.579912 4 33 True -1.541470 5 21 True -1.895841 6 45 True -0.967645 8 13 True -2.066400 9 11 True -2.134931 10 13 True -1.858564 11 10 True -2.001975 12 10 True -1.879093 13 17 True -1.223394 15 9 True -1.703488 16 7 True -1.856321 17 5 True -2.067005 18 3 True -2.481967 19 2 True -2.806130 20 6 True -1.579369 22 5 True -1.486437 23 1 True -2.914180 24 6 True -0.859284 25 7 True -0.334542 Model summary for event: 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model lifelines.CoxPHFitter duration col 'X' event col 'j_2' strata X_copy baseline estimation breslow number of observations 9463 number of events observed 161 partial log-likelihood -987.94 time fit was run 2022-03-22 16:14:20 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.16 1.18 0.27 -0.37 0.70 0.69 2.02 0.60 0.55 0.87 Z2 -1.08 0.34 0.28 -1.63 -0.53 0.20 0.59 -3.83 <0.005 12.92 Z3 -1.67 0.19 0.29 -2.24 -1.10 0.11 0.33 -5.74 <0.005 26.68 Z4 -0.67 0.51 0.27 -1.19 -0.15 0.31 0.86 -2.52 0.01 6.39 Z5 -0.38 0.69 0.27 -0.90 0.15 0.41 1.16 -1.40 0.16 2.64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Concordance 0.66 Partial AIC 1985.88 log-likelihood ratio test 55.99 on 5 df -log2(p) of ll-ratio test 33.51 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_jt success alpha_jt J X 2 1 29 True -1.891318 2 11 True -2.715119 3 19 True -2.029019 4 15 True -2.143387 5 17 True -1.891328 6 11 True -2.224464 8 6 True -2.633376 9 8 True -2.231573 10 7 True -2.259989 11 11 True -1.667235 12 3 True -2.838670 13 6 True -2.067073 15 3 True -2.584596 16 4 True -2.178489 17 1 True -3.370445 18 2 True -2.628716 19 1 True -3.205144 20 2 True -2.415453 22 1 True -2.842736 23 1 True -2.658322 24 1 True -2.411368 25 2 True -1.384788","title":"Case study #2: Not enough cases in the middle"},{"location":"intro/","text":"Introduction \u00a4 Based on \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" Tomer Meir*, Rom Gutman*, and Malka Gorfine (2022) [1] . and \"Discrete-time Competing-Risks Regression with or without Penalization\" Tomer Meir and Malka Gorfine (2023) [2] . Discrete-data survival analysis \u00a4 Discrete-data survival analysis refers to the case where data can only take values over a discrete grid. Sometimes, events can only occur at regular, discrete points in time. For example, in the United States a change in party controlling the presidency only occurs quadrennially in the month of January [3] . In other situations events may occur at any point in time, but available data record only the particular interval of time in which each event occurs. For example, death from cancer measured by months since time of diagnosis [4] , or length of stay in hospital recorded on a daily basis. It is well-known that naively using standard continuous-time models (even after correcting for ties) with discrete-time data may result in biased estimators for the discrete time models. Competing events \u00a4 Competing events arise when individuals are susceptible to several types of events but can experience at most one event. For example, competing risks for hospital length of stay are discharge and in-hospital death. Occurrence of one of these events precludes us from observing the other event on this patient. Another classical example of competing risks is cause-specific mortality, such as death from heart disease, death from cancer and death from other causes [5, 6] . PyDTS is an open source Python package which implements tools for discrete-time survival analysis with competing risks. References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\" (2023) [3] Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718 [4] Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881 [5] Kalbfleisch, John D. and Prentice, Ross L. \"The Statistical Analysis of Failure Time Data\" 2nd Ed., Wiley (2011) ISBN: 978-1-118-03123-0 [6] Klein, John P. and Moeschberger, Melvin L. \"Survival Analysis\", Springer (2003) ISBN: 978-0-387-95399-1","title":"Introduction"},{"location":"intro/#introduction","text":"Based on \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" Tomer Meir*, Rom Gutman*, and Malka Gorfine (2022) [1] . and \"Discrete-time Competing-Risks Regression with or without Penalization\" Tomer Meir and Malka Gorfine (2023) [2] .","title":"Introduction"},{"location":"intro/#discrete-data-survival-analysis","text":"Discrete-data survival analysis refers to the case where data can only take values over a discrete grid. Sometimes, events can only occur at regular, discrete points in time. For example, in the United States a change in party controlling the presidency only occurs quadrennially in the month of January [3] . In other situations events may occur at any point in time, but available data record only the particular interval of time in which each event occurs. For example, death from cancer measured by months since time of diagnosis [4] , or length of stay in hospital recorded on a daily basis. It is well-known that naively using standard continuous-time models (even after correcting for ties) with discrete-time data may result in biased estimators for the discrete time models.","title":"Discrete-data survival analysis"},{"location":"intro/#competing-events","text":"Competing events arise when individuals are susceptible to several types of events but can experience at most one event. For example, competing risks for hospital length of stay are discharge and in-hospital death. Occurrence of one of these events precludes us from observing the other event on this patient. Another classical example of competing risks is cause-specific mortality, such as death from heart disease, death from cancer and death from other causes [5, 6] . PyDTS is an open source Python package which implements tools for discrete-time survival analysis with competing risks.","title":"Competing events"},{"location":"intro/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\" (2023) [3] Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718 [4] Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881 [5] Kalbfleisch, John D. and Prentice, Ross L. \"The Statistical Analysis of Failure Time Data\" 2nd Ed., Wiley (2011) ISBN: 978-1-118-03123-0 [6] Klein, John P. and Moeschberger, Melvin L. \"Survival Analysis\", Springer (2003) ISBN: 978-0-387-95399-1","title":"References"},{"location":"methods/","text":"Methods \u00a4 Based on \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" Tomer Meir*, Rom Gutman*, and Malka Gorfine (2022) [1] . Definitions \u00a4 We let \\(T\\) denote a discrete event time that can take on only the values \\(\\{1,2,...,d\\}\\) and \\(J\\) denote the type of event, \\(J \\in \\{1,\\ldots,M\\}\\) . Consider a \\(p \\times 1\\) vector of baseline covariates \\(Z\\) . A general discrete cause-specific hazard function is of the form $$ \\lambda_j(t|Z) = \\Pr(T=t,J=j|T\\geq t, Z) \\hspace{0.3cm} t \\in {1,2,...,d} \\hspace{0.3cm} j=1,\\ldots,M \\, . $$ A popular semi-parametric model of the above hazard function based on a transformation regression model is of the form $$ h(\\lambda_{j}(t|Z)) = \\alpha_{jt} +Z^T \\beta_j \\hspace{0.3cm} t \\in {1,2,...,d} \\hspace{0.3cm} j=1, \\ldots,M $$ such that \\(h\\) is a known function [2, and reference therein] . The total number of parameters in the model is \\(M(d+p)\\) . The logit function \\(h(a)=\\log \\{ a/(1-a) \\}\\) yields \\begin{equation}\\label{eq:logis} \\lambda_j(t|Z)=\\frac{\\exp(\\alpha_{jt}+Z^T\\beta_j)}{1+\\exp(\\alpha_{jt}+Z^T\\beta_j)} \\, . \\end{equation} It should be noted that leaving \\(\\alpha_{jt}\\) unspecified is analogous to having an unspecified baseline hazard function in the Cox proportional hazard model [3] , and thus we consider the above as a semi-parametric model. Let \\(S(t|Z) = \\Pr(T>t|Z)\\) be the overall survival given \\(Z\\) . Then, the probability of experiencing event of type \\(j\\) at time \\(t\\) equals $$ \\Pr(T=t,J=j|Z)=\\lambda_j(t|Z) \\prod_{k=1}^{t-1} \\left\\lbrace 1- \\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace $$ and the cumulative incident function (CIF) of cause \\(j\\) is given by $$ F_j(t|Z) = \\Pr(T \\leq t, J=j|Z) = \\sum_{m=1}^{t} \\lambda_j(m|Z) S(m-1|Z) = \\sum_{m=1}^{t}\\lambda_j(m|Z) \\prod_{k=1}^{m-1} \\left\\lbrace 1-\\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace \\, . $$ Finally, the marginal probability of event type \\(j\\) (marginally with respect to the time of event), given \\(Z\\) , equals $$ \\Pr(J=j|Z) = \\sum_{m=1}^{d} \\lambda_j(m|Z) \\prod_{k=1}^{m-1} \\left\\lbrace 1-\\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace \\, . $$ In the next section we provide a fast estimation technique of the parameters \\(\\{\\alpha_{j1},\\ldots,\\alpha_{jd},\\beta_j^T \\, ; \\, j=1,\\ldots,M\\}\\) . The Collapsed Log-Likelihood Approach and the Proposed Estimators \u00a4 For simplicity of presentation, we assume two competing events, i.e., \\(M=2\\) and our goal is estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T,\\alpha_{21},\\ldots,\\alpha_{2d},\\beta_2^T\\}\\) along with the standard error of the estimators. The data at hand consist of \\(n\\) independent observations, each with \\((X_i,\\delta_i,J_i,Z_i)\\) where \\(X_i=\\min(C_i,T_i)\\) , \\(C_i\\) is a right-censoring time, \\(\\delta_i=I(X_i=T_i)\\) is the event indicator and \\(J_i\\in\\{0,1,2\\}\\) , where \\(J_i=0\\) if and only if \\(\\delta_i=0\\) . Assume that given the covariates, the censoring and failure time are independent and non-informative. Then, the likelihood function is proportional to $$ L = \\prod_{i=1}^n \\left\\lbrace\\frac{\\lambda_1(X_i|Z_i)}{1-\\lambda_1(X_i|Z_i)-\\lambda_2(X_i|Z_i)}\\right\\rbrace^{I(\\delta_{1i}=1)} \\left\\lbrace\\frac{\\lambda_2(X_i|Z_i)}{1-\\lambda_1(X_i|Z_i)-\\lambda_2(X_i|Z_i)}\\right\\rbrace^{I(\\delta_{2i}=1)} \\prod_{k=1}^{X_i}\\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace $$ or, equivalently, $$ L = \\prod_{i=1}^n \\left[ \\prod_{j=1}^2 \\prod_{m=1}^{X_i} \\left\\lbrace \\frac{\\lambda_j(m|Z_i)}{1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)}\\right\\rbrace^{\\delta_{jim}}\\right] \\prod_{k=1}^{X_i}\\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace $$ where \\(\\delta_{jim}\\) equals one if subject \\(i\\) experienced event type \\(j\\) at time \\(m\\) ; and 0 otherwise. Clearly \\(L\\) cannot be decomposed into separate likelihoods for each cause-specific hazard function \\(\\lambda_j\\) . The log likelihood becomes $$ \\log L = \\sum_{i=1}^n \\left[ \\sum_{j=1}^2 \\sum_{m=1}^{X_i} \\left[ \\delta_{jim} \\log \\lambda_j(m|Z_i) - \\delta_{jim}{1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)}\\right] \\right. +\\left.\\sum_{k=1}^{X_i}\\log \\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace \\right] $$ $$ = \\sum_{i=1}^n \\sum_{m=1}^{X_i} \\left[ \\delta_{1im} \\log \\lambda_1(m|Z_i)+\\delta_{2im} \\log \\lambda_2(m|Z_i) \\right. +\\left. \\lbrace 1-\\delta_{1im}-\\delta_{2im}\\rbrace \\log\\lbrace 1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)\\rbrace \\right] \\, . $$ Instead of maximizing the \\(M(d+p)\\) parameters simultaneously based on the above log-likelihood, the collapsed log-likelihood of Lee et al. [4] can be adopted. Specifically, the data are expanded such that for each observation \\(i\\) the expanded dataset includes \\(X_i\\) rows, one row for each time \\(t\\) , \\(t \\leq X_i\\) . At each time point \\(t\\) the expanded data are conditionally multinomial with one of three possible outcomes \\(\\{\\delta_{1it},\\delta_{2it},1-\\delta_{1it}-\\delta_{2it}\\}\\) . Then, for estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T\\}\\) , we combine \\(\\delta_{2it}\\) and \\(1-\\delta_{1it}-\\delta_{2it}\\) , and the collapsed log-likelihood for cause \\(J=1\\) based on a binary regression model with \\(\\delta_{1it}\\) as the outcome is given by $$ \\log L_1 = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{1im} \\log \\lambda_1(m|Z_i)+(1-\\delta_{1im})\\log \\lbrace 1-\\lambda_1(m|Z_i)\\rbrace \\right] \\, . $$ Similarly, the collapsed log-likelihood for cause \\(J=2\\) based on a binary regression model with \\(\\delta_{2it}\\) as the outcome becomes $$ \\log L_2 = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{2im} \\log \\lambda_2(m|Z_i)+(1-\\delta_{2im})\\log \\lbrace 1-\\lambda_2(m|Z_i)\\rbrace \\right] $$ and one can fit the two models, separately. In general, for \\(M\\) competing events, the estimators of \\(\\{\\alpha_{j1},\\ldots,\\alpha_{jd},\\beta_j^T\\}\\) , \\(j=1,\\ldots,M\\) , are the respective values that maximize $$ \\log L_j = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{jim} \\log \\lambda_j(m|Z_i)+(1-\\delta_{jim})\\log {1-\\lambda_j(m|Z_i)} \\right] \\, . $$ Namely, each maximization \\(j\\) , \\(j=1,\\ldots,M\\) , consists of maximizing \\(d + p\\) parameters simultaneously. Proposed Estimators \u00a4 Alternatively, we propose the following simpler and faster estimation procedure, with a negligible efficiency loss, if any. Our idea exploits the close relationship between conditional logistic regression analysis and stratified Cox regression analysis [5] . We propose to estimate each \\(\\beta_j\\) separately, and given \\(\\beta_j\\) , \\(\\alpha_{jt}\\) , \\(t=1\\ldots,d\\) , are separately estimated. In particular, the proposed estimating procedure consists of the following two speedy steps: Step 1. \u00a4 Use the expanded dataset and estimate each vector \\(\\beta_j\\) , \\(j \\in \\{1,\\ldots, M\\}\\) , by a simple conditional logistic regression, conditioning on the event time \\(X\\) , using a stratified Cox analysis. Step 2. \u00a4 Given the estimators \\(\\widehat{\\beta}_j\\) , \\(j \\in \\{1,\\ldots, M\\}\\) , of Step 1, use the original (un-expanded) data and estimate each \\(\\alpha_{jt}\\) , \\(j \\in \\{1,\\ldots,M\\}\\) , \\(t=1,\\ldots,d\\) , separately, by \\[\\widehat{ \\alpha }_{jt} = argmin_{a} \\left\\lbrace \\frac{1}{y_t} \\sum_{i=1}^n I(X_i \\geq t) \\frac{ \\exp(a+Z_i^T \\widehat{\\beta}_j)}{1 + \\exp(a + Z_i^T \\widehat{\\beta}_j)} - \\frac{n_{tj}}{y_t} \\right\\rbrace ^2 \\] where \\(y_t=\\sum_{i=1}^n I(X_i \\geq t)\\) and \\(n_{tj}=\\sum_{i=1}^n I(X_i = t, J_i=j)\\) . The above equation consists minimizing the squared distance between the observed proportion of failures of type \\(j\\) at time \\(t\\) ( \\(n_{tj}/y_t\\) ) and the expected proportion of failures given model defined above for \\(\\lambda_j\\) and \\(\\widehat{\\beta}_j\\) . The simulation results of section Simple Simulation reveals that the above two-step procedure performs well in terms of bias, and provides similar standard error of that of [3] . However, the improvement in computational time, by using our procedure, could be improved by a factor of 1.5-3.5 depending on d. Standard errors of \\(\\widehat{\\beta}_j\\) , \\(j \\in \\{1,\\ldots,M\\}\\) , can be derived directly from the stratified Cox analysis. References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka \"PyDTS: A Python Package for Discrete Time Survival-analysis with Competing Risks\" (2022) [2] Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718 [3] Cox, D. R. \"Regression Models and Life-Tables\" Journal of the Royal Statistical Society: Series B (Methodological) (1972) doi: 10.1111/j.2517-6161.1972.tb00899.x [4] Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881 [5] Prentice, Ross L and Breslow, Norman E \"Retrospective studies and failure time models\" Biometrika (1978) doi: 10.1111/j.2517-6161.1972.tb00899.x","title":"Definitions and Estimation"},{"location":"methods/#methods","text":"Based on \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" Tomer Meir*, Rom Gutman*, and Malka Gorfine (2022) [1] .","title":"Methods"},{"location":"methods/#definitions","text":"We let \\(T\\) denote a discrete event time that can take on only the values \\(\\{1,2,...,d\\}\\) and \\(J\\) denote the type of event, \\(J \\in \\{1,\\ldots,M\\}\\) . Consider a \\(p \\times 1\\) vector of baseline covariates \\(Z\\) . A general discrete cause-specific hazard function is of the form $$ \\lambda_j(t|Z) = \\Pr(T=t,J=j|T\\geq t, Z) \\hspace{0.3cm} t \\in {1,2,...,d} \\hspace{0.3cm} j=1,\\ldots,M \\, . $$ A popular semi-parametric model of the above hazard function based on a transformation regression model is of the form $$ h(\\lambda_{j}(t|Z)) = \\alpha_{jt} +Z^T \\beta_j \\hspace{0.3cm} t \\in {1,2,...,d} \\hspace{0.3cm} j=1, \\ldots,M $$ such that \\(h\\) is a known function [2, and reference therein] . The total number of parameters in the model is \\(M(d+p)\\) . The logit function \\(h(a)=\\log \\{ a/(1-a) \\}\\) yields \\begin{equation}\\label{eq:logis} \\lambda_j(t|Z)=\\frac{\\exp(\\alpha_{jt}+Z^T\\beta_j)}{1+\\exp(\\alpha_{jt}+Z^T\\beta_j)} \\, . \\end{equation} It should be noted that leaving \\(\\alpha_{jt}\\) unspecified is analogous to having an unspecified baseline hazard function in the Cox proportional hazard model [3] , and thus we consider the above as a semi-parametric model. Let \\(S(t|Z) = \\Pr(T>t|Z)\\) be the overall survival given \\(Z\\) . Then, the probability of experiencing event of type \\(j\\) at time \\(t\\) equals $$ \\Pr(T=t,J=j|Z)=\\lambda_j(t|Z) \\prod_{k=1}^{t-1} \\left\\lbrace 1- \\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace $$ and the cumulative incident function (CIF) of cause \\(j\\) is given by $$ F_j(t|Z) = \\Pr(T \\leq t, J=j|Z) = \\sum_{m=1}^{t} \\lambda_j(m|Z) S(m-1|Z) = \\sum_{m=1}^{t}\\lambda_j(m|Z) \\prod_{k=1}^{m-1} \\left\\lbrace 1-\\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace \\, . $$ Finally, the marginal probability of event type \\(j\\) (marginally with respect to the time of event), given \\(Z\\) , equals $$ \\Pr(J=j|Z) = \\sum_{m=1}^{d} \\lambda_j(m|Z) \\prod_{k=1}^{m-1} \\left\\lbrace 1-\\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace \\, . $$ In the next section we provide a fast estimation technique of the parameters \\(\\{\\alpha_{j1},\\ldots,\\alpha_{jd},\\beta_j^T \\, ; \\, j=1,\\ldots,M\\}\\) .","title":"Definitions"},{"location":"methods/#the-collapsed-log-likelihood-approach-and-the-proposed-estimators","text":"For simplicity of presentation, we assume two competing events, i.e., \\(M=2\\) and our goal is estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T,\\alpha_{21},\\ldots,\\alpha_{2d},\\beta_2^T\\}\\) along with the standard error of the estimators. The data at hand consist of \\(n\\) independent observations, each with \\((X_i,\\delta_i,J_i,Z_i)\\) where \\(X_i=\\min(C_i,T_i)\\) , \\(C_i\\) is a right-censoring time, \\(\\delta_i=I(X_i=T_i)\\) is the event indicator and \\(J_i\\in\\{0,1,2\\}\\) , where \\(J_i=0\\) if and only if \\(\\delta_i=0\\) . Assume that given the covariates, the censoring and failure time are independent and non-informative. Then, the likelihood function is proportional to $$ L = \\prod_{i=1}^n \\left\\lbrace\\frac{\\lambda_1(X_i|Z_i)}{1-\\lambda_1(X_i|Z_i)-\\lambda_2(X_i|Z_i)}\\right\\rbrace^{I(\\delta_{1i}=1)} \\left\\lbrace\\frac{\\lambda_2(X_i|Z_i)}{1-\\lambda_1(X_i|Z_i)-\\lambda_2(X_i|Z_i)}\\right\\rbrace^{I(\\delta_{2i}=1)} \\prod_{k=1}^{X_i}\\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace $$ or, equivalently, $$ L = \\prod_{i=1}^n \\left[ \\prod_{j=1}^2 \\prod_{m=1}^{X_i} \\left\\lbrace \\frac{\\lambda_j(m|Z_i)}{1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)}\\right\\rbrace^{\\delta_{jim}}\\right] \\prod_{k=1}^{X_i}\\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace $$ where \\(\\delta_{jim}\\) equals one if subject \\(i\\) experienced event type \\(j\\) at time \\(m\\) ; and 0 otherwise. Clearly \\(L\\) cannot be decomposed into separate likelihoods for each cause-specific hazard function \\(\\lambda_j\\) . The log likelihood becomes $$ \\log L = \\sum_{i=1}^n \\left[ \\sum_{j=1}^2 \\sum_{m=1}^{X_i} \\left[ \\delta_{jim} \\log \\lambda_j(m|Z_i) - \\delta_{jim}{1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)}\\right] \\right. +\\left.\\sum_{k=1}^{X_i}\\log \\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace \\right] $$ $$ = \\sum_{i=1}^n \\sum_{m=1}^{X_i} \\left[ \\delta_{1im} \\log \\lambda_1(m|Z_i)+\\delta_{2im} \\log \\lambda_2(m|Z_i) \\right. +\\left. \\lbrace 1-\\delta_{1im}-\\delta_{2im}\\rbrace \\log\\lbrace 1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)\\rbrace \\right] \\, . $$ Instead of maximizing the \\(M(d+p)\\) parameters simultaneously based on the above log-likelihood, the collapsed log-likelihood of Lee et al. [4] can be adopted. Specifically, the data are expanded such that for each observation \\(i\\) the expanded dataset includes \\(X_i\\) rows, one row for each time \\(t\\) , \\(t \\leq X_i\\) . At each time point \\(t\\) the expanded data are conditionally multinomial with one of three possible outcomes \\(\\{\\delta_{1it},\\delta_{2it},1-\\delta_{1it}-\\delta_{2it}\\}\\) . Then, for estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T\\}\\) , we combine \\(\\delta_{2it}\\) and \\(1-\\delta_{1it}-\\delta_{2it}\\) , and the collapsed log-likelihood for cause \\(J=1\\) based on a binary regression model with \\(\\delta_{1it}\\) as the outcome is given by $$ \\log L_1 = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{1im} \\log \\lambda_1(m|Z_i)+(1-\\delta_{1im})\\log \\lbrace 1-\\lambda_1(m|Z_i)\\rbrace \\right] \\, . $$ Similarly, the collapsed log-likelihood for cause \\(J=2\\) based on a binary regression model with \\(\\delta_{2it}\\) as the outcome becomes $$ \\log L_2 = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{2im} \\log \\lambda_2(m|Z_i)+(1-\\delta_{2im})\\log \\lbrace 1-\\lambda_2(m|Z_i)\\rbrace \\right] $$ and one can fit the two models, separately. In general, for \\(M\\) competing events, the estimators of \\(\\{\\alpha_{j1},\\ldots,\\alpha_{jd},\\beta_j^T\\}\\) , \\(j=1,\\ldots,M\\) , are the respective values that maximize $$ \\log L_j = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{jim} \\log \\lambda_j(m|Z_i)+(1-\\delta_{jim})\\log {1-\\lambda_j(m|Z_i)} \\right] \\, . $$ Namely, each maximization \\(j\\) , \\(j=1,\\ldots,M\\) , consists of maximizing \\(d + p\\) parameters simultaneously.","title":"The Collapsed Log-Likelihood Approach and the Proposed Estimators"},{"location":"methods/#proposed-estimators","text":"Alternatively, we propose the following simpler and faster estimation procedure, with a negligible efficiency loss, if any. Our idea exploits the close relationship between conditional logistic regression analysis and stratified Cox regression analysis [5] . We propose to estimate each \\(\\beta_j\\) separately, and given \\(\\beta_j\\) , \\(\\alpha_{jt}\\) , \\(t=1\\ldots,d\\) , are separately estimated. In particular, the proposed estimating procedure consists of the following two speedy steps:","title":"Proposed Estimators"},{"location":"methods/#step-1","text":"Use the expanded dataset and estimate each vector \\(\\beta_j\\) , \\(j \\in \\{1,\\ldots, M\\}\\) , by a simple conditional logistic regression, conditioning on the event time \\(X\\) , using a stratified Cox analysis.","title":"Step 1."},{"location":"methods/#step-2","text":"Given the estimators \\(\\widehat{\\beta}_j\\) , \\(j \\in \\{1,\\ldots, M\\}\\) , of Step 1, use the original (un-expanded) data and estimate each \\(\\alpha_{jt}\\) , \\(j \\in \\{1,\\ldots,M\\}\\) , \\(t=1,\\ldots,d\\) , separately, by \\[\\widehat{ \\alpha }_{jt} = argmin_{a} \\left\\lbrace \\frac{1}{y_t} \\sum_{i=1}^n I(X_i \\geq t) \\frac{ \\exp(a+Z_i^T \\widehat{\\beta}_j)}{1 + \\exp(a + Z_i^T \\widehat{\\beta}_j)} - \\frac{n_{tj}}{y_t} \\right\\rbrace ^2 \\] where \\(y_t=\\sum_{i=1}^n I(X_i \\geq t)\\) and \\(n_{tj}=\\sum_{i=1}^n I(X_i = t, J_i=j)\\) . The above equation consists minimizing the squared distance between the observed proportion of failures of type \\(j\\) at time \\(t\\) ( \\(n_{tj}/y_t\\) ) and the expected proportion of failures given model defined above for \\(\\lambda_j\\) and \\(\\widehat{\\beta}_j\\) . The simulation results of section Simple Simulation reveals that the above two-step procedure performs well in terms of bias, and provides similar standard error of that of [3] . However, the improvement in computational time, by using our procedure, could be improved by a factor of 1.5-3.5 depending on d. Standard errors of \\(\\widehat{\\beta}_j\\) , \\(j \\in \\{1,\\ldots,M\\}\\) , can be derived directly from the stratified Cox analysis.","title":"Step 2."},{"location":"methods/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka \"PyDTS: A Python Package for Discrete Time Survival-analysis with Competing Risks\" (2022) [2] Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718 [3] Cox, D. R. \"Regression Models and Life-Tables\" Journal of the Royal Statistical Society: Series B (Methodological) (1972) doi: 10.1111/j.2517-6161.1972.tb00899.x [4] Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881 [5] Prentice, Ross L and Breslow, Norman E \"Retrospective studies and failure time models\" Biometrika (1978) doi: 10.1111/j.2517-6161.1972.tb00899.x","title":"References"},{"location":"methodsevaluation/","text":"Evaluation Measures \u00a4 Let \\[ \\pi_{ij}(t) = \\widehat{\\Pr}(T_i=t, J_i=j \\mid Z_i) = \\widehat{\\lambda}_j (t \\mid Z_i) \\widehat{S}(t-1 \\mid Z_i) \\] and \\[ D_{ij} (t) = I(T_i = t, J_i = j) \\] The cause-specific incidence/dynamic area under the receiver operating characteristics curve (AUC) is defined and estimated in the spirit of Heagerty and Zheng (2005) and Blanche et al. (2015) as the probability of a random observation with observed event \\(j\\) at time \\(t\\) having a higher risk prediction for cause \\(j\\) than a randomly selected observation \\(m\\) , at risk at time \\(t\\) , without the observed event \\(j\\) at time \\(t\\) . Namely, \\[ \\mbox{AUC}_j(t) = \\Pr (\\pi_{ij}(t) > \\pi_{mj}(t) \\mid D_{ij} (t) = 1, D_{mj} (t) = 0, T_m \\geq t) \\] In the presence of censored data and under the assumption that the censoring is independent of the failure time and observed covariates, an inverse probability censoring weighting (IPCW) estimator of \\(\\mbox{AUC}_j(t)\\) becomes \\[ \\widehat{\\mbox{AUC}}_j (t) = \\frac{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) W_{ij}(t) W_{mj}(t) \\{I(\\pi_{ij}(t) > \\pi_{mj}(t))+0.5I(\\pi_{ij}(t)=\\pi_{mj}(t))\\}}{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) W_{ij}(t) W_{mj}(t)} \\] And can be simplified as: \\[ \\widehat{\\mbox{AUC}}_j (t) = \\frac{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) \\{I(\\pi_{ij}(t) > \\pi_{mj}(t))+0.5I(\\pi_{ij}(t)=\\pi_{mj}(t))\\}}{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t)} \\] where \\[ W_{ij}(t) = \\frac{D_{ij}(t)}{\\widehat{G}_C(T_i)} + I(X_i \\geq t)\\frac{1-D_{ij}(t)}{\\widehat{G}_C(t)} = \\frac{D_{ij}(t)}{\\widehat{G}_C(t)} + I(X_i \\geq t)\\frac{1-D_{ij}(t)}{\\widehat{G}_C(t)} = I(X_i \\geq t) / \\widehat{G}_C(t) \\] and \\(\\widehat{G}_C(\\cdot)\\) is the estimated survival function of the censoring (e.g., the Kaplan-Meier estimator). Interestingly, the IPCWs have no effect on \\(\\widehat{\\mbox{AUC}}_j (t)\\) . An integrated cause-specific AUC can be estimated as a weighted sum by \\[ \\widehat{\\mbox{AUC}}_j = \\sum_t \\widehat{\\mbox{AUC}}_j (t) w_j (t) \\] and we adopt a simple data-driven weight function of the form \\[ w_j(t) = \\frac{N_j(t)}{\\sum_t N_j(t)} \\] A global AUC can be defined as \\[ \\widehat{\\mbox{AUC}} = \\sum_j \\widehat{\\mbox{AUC}}_j v_j \\] where \\[ v_j = \\frac{\\sum_{t} N_j(t)}{ \\sum_{j=1}^M \\sum_{t} N_j(t) } \\] Another well-known performance measure is the Brier Score (BS). In the spirit of Blanche et al. (2015) we define \\[ \\widehat{\\mbox{BS}}_{j}(t) = \\frac{1}{Y_{\\cdot}(t)} {\\sum_{i=1}^n W_{ij}(t) \\left( D_{ij}(t) - \\pi_{ij}(t)\\right)^2} \\, . \\] An integrated cause-specific BS can be estimated by the weighted sum \\[ \\widehat{\\mbox{BS}}_{j} = \\sum_t \\widehat{\\mbox{BS}}_{j}(t) w_j(t) \\] and an estimated global BS is given by \\[ \\widehat{\\mbox{BS}} = \\sum_j \\widehat{\\mbox{BS}}_{j} v_j \\, . \\]","title":"Evaluation Metrics"},{"location":"methodsevaluation/#evaluation-measures","text":"Let \\[ \\pi_{ij}(t) = \\widehat{\\Pr}(T_i=t, J_i=j \\mid Z_i) = \\widehat{\\lambda}_j (t \\mid Z_i) \\widehat{S}(t-1 \\mid Z_i) \\] and \\[ D_{ij} (t) = I(T_i = t, J_i = j) \\] The cause-specific incidence/dynamic area under the receiver operating characteristics curve (AUC) is defined and estimated in the spirit of Heagerty and Zheng (2005) and Blanche et al. (2015) as the probability of a random observation with observed event \\(j\\) at time \\(t\\) having a higher risk prediction for cause \\(j\\) than a randomly selected observation \\(m\\) , at risk at time \\(t\\) , without the observed event \\(j\\) at time \\(t\\) . Namely, \\[ \\mbox{AUC}_j(t) = \\Pr (\\pi_{ij}(t) > \\pi_{mj}(t) \\mid D_{ij} (t) = 1, D_{mj} (t) = 0, T_m \\geq t) \\] In the presence of censored data and under the assumption that the censoring is independent of the failure time and observed covariates, an inverse probability censoring weighting (IPCW) estimator of \\(\\mbox{AUC}_j(t)\\) becomes \\[ \\widehat{\\mbox{AUC}}_j (t) = \\frac{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) W_{ij}(t) W_{mj}(t) \\{I(\\pi_{ij}(t) > \\pi_{mj}(t))+0.5I(\\pi_{ij}(t)=\\pi_{mj}(t))\\}}{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) W_{ij}(t) W_{mj}(t)} \\] And can be simplified as: \\[ \\widehat{\\mbox{AUC}}_j (t) = \\frac{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) \\{I(\\pi_{ij}(t) > \\pi_{mj}(t))+0.5I(\\pi_{ij}(t)=\\pi_{mj}(t))\\}}{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t)} \\] where \\[ W_{ij}(t) = \\frac{D_{ij}(t)}{\\widehat{G}_C(T_i)} + I(X_i \\geq t)\\frac{1-D_{ij}(t)}{\\widehat{G}_C(t)} = \\frac{D_{ij}(t)}{\\widehat{G}_C(t)} + I(X_i \\geq t)\\frac{1-D_{ij}(t)}{\\widehat{G}_C(t)} = I(X_i \\geq t) / \\widehat{G}_C(t) \\] and \\(\\widehat{G}_C(\\cdot)\\) is the estimated survival function of the censoring (e.g., the Kaplan-Meier estimator). Interestingly, the IPCWs have no effect on \\(\\widehat{\\mbox{AUC}}_j (t)\\) . An integrated cause-specific AUC can be estimated as a weighted sum by \\[ \\widehat{\\mbox{AUC}}_j = \\sum_t \\widehat{\\mbox{AUC}}_j (t) w_j (t) \\] and we adopt a simple data-driven weight function of the form \\[ w_j(t) = \\frac{N_j(t)}{\\sum_t N_j(t)} \\] A global AUC can be defined as \\[ \\widehat{\\mbox{AUC}} = \\sum_j \\widehat{\\mbox{AUC}}_j v_j \\] where \\[ v_j = \\frac{\\sum_{t} N_j(t)}{ \\sum_{j=1}^M \\sum_{t} N_j(t) } \\] Another well-known performance measure is the Brier Score (BS). In the spirit of Blanche et al. (2015) we define \\[ \\widehat{\\mbox{BS}}_{j}(t) = \\frac{1}{Y_{\\cdot}(t)} {\\sum_{i=1}^n W_{ij}(t) \\left( D_{ij}(t) - \\pi_{ij}(t)\\right)^2} \\, . \\] An integrated cause-specific BS can be estimated by the weighted sum \\[ \\widehat{\\mbox{BS}}_{j} = \\sum_t \\widehat{\\mbox{BS}}_{j}(t) w_j(t) \\] and an estimated global BS is given by \\[ \\widehat{\\mbox{BS}} = \\sum_j \\widehat{\\mbox{BS}}_{j} v_j \\, . \\]","title":"Evaluation Measures"},{"location":"methodsintro/","text":"Methods \u00a4 In this section, we outline the statistical background for the tools incorporated in PyDTS. We commence with some definitions, present the collapsed log-likelihood approach and the estimation procedure of Lee et al. (2018) [4] , introduce our estimation method [1] - [2] , and conclude with evaluation metrics. For additional details, check out the references. References \u00a4 [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\" (2023) [3] Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718 [4] Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881 [5] Kalbfleisch, John D. and Prentice, Ross L. \"The Statistical Analysis of Failure Time Data\" 2nd Ed., Wiley (2011) ISBN: 978-1-118-03123-0 [6] Klein, John P. and Moeschberger, Melvin L. \"Survival Analysis\", Springer (2003) ISBN: 978-0-387-95399-1","title":"Introduction"},{"location":"methodsintro/#methods","text":"In this section, we outline the statistical background for the tools incorporated in PyDTS. We commence with some definitions, present the collapsed log-likelihood approach and the estimation procedure of Lee et al. (2018) [4] , introduce our estimation method [1] - [2] , and conclude with evaluation metrics. For additional details, check out the references.","title":"Methods"},{"location":"methodsintro/#references","text":"[1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) [2] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\" (2023) [3] Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718 [4] Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881 [5] Kalbfleisch, John D. and Prentice, Ross L. \"The Statistical Analysis of Failure Time Data\" 2nd Ed., Wiley (2011) ISBN: 978-1-118-03123-0 [6] Klein, John P. and Moeschberger, Melvin L. \"Survival Analysis\", Springer (2003) ISBN: 978-0-387-95399-1","title":"References"},{"location":"api/cross_validation/","text":"BasePenaltyGridSearchCV \u00a4 This class implements K-fold cross-validation of the PenaltyGridSearch Source code in pydts/cross_validation.py class BasePenaltyGridSearchCV ( object ): \"\"\" This class implements K-fold cross-validation of the PenaltyGridSearch \"\"\" def __init__ ( self ): self . folds_grids = {} self . test_pids = {} self . global_auc = {} self . integrated_auc = {} self . global_bs = {} self . integrated_bs = {} self . TwoStagesFitter_type = 'CoxPHFitter' def cross_validate ( self , full_df : pd . DataFrame , l1_ratio : float , penalizers : list , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = { 'nb_workers' : WORKERS }, metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) -> pd . DataFrame : \"\"\" This method implements K-fold cross-validation using PenaltyGridSearch and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). twostages_fit_kwargs (dict): keyword arguments to pass to each TwoStagesFitter. metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearchCV.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearchCV.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearchCV.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearchCV.global_bs). Returns: gauc_output_df (pd.DataFrame): Global AUC k-fold mean and standard error for all possible combination of the penalizers. \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . folds_grids = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): print ( f 'Starting fold { i_fold + 1 } / { n_splits } ' ) start = time () self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_pgs = PenaltyGridSearchExact () else : fold_pgs = PenaltyGridSearch () fold_pgs . evaluate ( train_df = train_df , test_df = test_df , l1_ratio = l1_ratio , penalizers = penalizers , metrics = metrics , seed = seed , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , twostages_fit_kwargs = twostages_fit_kwargs ) self . folds_grids [ i_fold ] = fold_pgs for metric in metrics : if metric == 'GAUC' : self . global_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_auc ) elif metric == 'IAUC' : self . integrated_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_auc ) elif metric == 'GBS' : self . global_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_bs ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_bs ) end = time () print ( f 'Finished fold { i_fold + 1 } / { n_splits } , { int ( end - start ) } seconds' ) if 'GAUC' in metrics : res = [ v for k , v in self . global_auc . items ()] gauc_output_df = pd . concat ([ pd . concat ( res , axis = 1 ) . mean ( axis = 1 ), pd . concat ( res , axis = 1 ) . std ( axis = 1 )], keys = [ 'Mean' , 'SE' ], axis = 1 ) else : gauc_output_df = pd . DataFrame () return gauc_output_df cross_validate ( self , full_df , l1_ratio , penalizers , n_splits = 5 , shuffle = True , seed = None , event_type_col = 'J' , duration_col = 'X' , pid_col = 'pid' , twostages_fit_kwargs = { 'nb_workers' : 2 }, metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) \u00a4 This method implements K-fold cross-validation using PenaltyGridSearch and full_df data. Parameters: Name Type Description Default full_df pd.DataFrame Data to cross validate. required l1_ratio float regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required penalizers list penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required n_splits int Number of folds, defaults to 5. 5 shuffle boolean Shuffle samples before splitting to folds. Defaults to True. True seed Optional[int] Pseudo-random seed to KFold instance. Defaults to None. None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in full_df). 'X' pid_col str Sample ID column name (must be a column in full_df). 'pid' twostages_fit_kwargs dict keyword arguments to pass to each TwoStagesFitter. {'nb_workers': 2} metrics str, list Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearchCV.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearchCV.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearchCV.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearchCV.global_bs). ['IBS', 'GBS', 'IAUC', 'GAUC'] Returns: Type Description gauc_output_df (pd.DataFrame) Global AUC k-fold mean and standard error for all possible combination of the penalizers. Source code in pydts/cross_validation.py def cross_validate ( self , full_df : pd . DataFrame , l1_ratio : float , penalizers : list , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = { 'nb_workers' : WORKERS }, metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) -> pd . DataFrame : \"\"\" This method implements K-fold cross-validation using PenaltyGridSearch and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). twostages_fit_kwargs (dict): keyword arguments to pass to each TwoStagesFitter. metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearchCV.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearchCV.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearchCV.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearchCV.global_bs). Returns: gauc_output_df (pd.DataFrame): Global AUC k-fold mean and standard error for all possible combination of the penalizers. \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . folds_grids = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): print ( f 'Starting fold { i_fold + 1 } / { n_splits } ' ) start = time () self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_pgs = PenaltyGridSearchExact () else : fold_pgs = PenaltyGridSearch () fold_pgs . evaluate ( train_df = train_df , test_df = test_df , l1_ratio = l1_ratio , penalizers = penalizers , metrics = metrics , seed = seed , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , twostages_fit_kwargs = twostages_fit_kwargs ) self . folds_grids [ i_fold ] = fold_pgs for metric in metrics : if metric == 'GAUC' : self . global_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_auc ) elif metric == 'IAUC' : self . integrated_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_auc ) elif metric == 'GBS' : self . global_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_bs ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_bs ) end = time () print ( f 'Finished fold { i_fold + 1 } / { n_splits } , { int ( end - start ) } seconds' ) if 'GAUC' in metrics : res = [ v for k , v in self . global_auc . items ()] gauc_output_df = pd . concat ([ pd . concat ( res , axis = 1 ) . mean ( axis = 1 ), pd . concat ( res , axis = 1 ) . std ( axis = 1 )], keys = [ 'Mean' , 'SE' ], axis = 1 ) else : gauc_output_df = pd . DataFrame () return gauc_output_df BaseTwoStagesCV \u00a4 This class implements K-fold cross-validation using TwoStagesFitters and TwoStagesFittersExact Source code in pydts/cross_validation.py class BaseTwoStagesCV ( object ): \"\"\" This class implements K-fold cross-validation using TwoStagesFitters and TwoStagesFittersExact \"\"\" def __init__ ( self ): self . models = {} self . test_pids = {} self . results = pd . DataFrame () self . global_auc = {} self . integrated_auc = {} self . global_bs = {} self . integrated_bs = {} self . TwoStagesFitter_type = 'CoxPHFitter' def cross_validate ( self , full_df : pd . DataFrame , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , fit_beta_kwargs : dict = {}, covariates = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , x0 : Union [ np . array , int ] = 0 , verbose : int = 2 , nb_workers : int = WORKERS , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]): \"\"\" This method implements K-fold cross-validation using TwoStagesFitters and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. covariates (list): list of covariates to be used in estimating the regression coefficients. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. metrics (str, list): Evaluation metrics. Available metrics: 'AUC': AUC at t (will be added to TwoStagesCV.results), 'IAUC': Integrated AUC (will be in TwoStagesCV.integrated_auc), 'GAUC': Global AUC (will be in TwoStagesCV.global_auc). 'BS': Brier score at t (will be added to TwoStagesCV.results), 'IBS': Integrated Brier Score (will be in TwoStagesCV.integrated_bs), 'GBS': Global Brier Score (will be in TwoStagesCV.global_bs). Returns: Results (pd.DataFrame): Cross validation metrics results \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . models = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_fitter = TwoStagesFitterExact () else : fold_fitter = TwoStagesFitter () print ( f 'Fitting fold { i_fold + 1 } / { n_splits } ' ) fold_fitter . fit ( df = train_df , covariates = covariates , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , x0 = x0 , fit_beta_kwargs = fit_beta_kwargs , verbose = verbose , nb_workers = nb_workers ) #self.models[i_fold] = deepcopy(fold_fitter) self . models [ i_fold ] = fold_fitter pred_df = self . models [ i_fold ] . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ i_fold ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ i_fold ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ i_fold ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'AUC' : tmp_res = events_auc_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) elif metric == 'BS' : tmp_res = events_brier_score_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) return self . results cross_validate ( self , full_df , n_splits = 5 , shuffle = True , seed = None , fit_beta_kwargs = {}, covariates = None , event_type_col = 'J' , duration_col = 'X' , pid_col = 'pid' , x0 = 0 , verbose = 2 , nb_workers = 2 , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]) \u00a4 This method implements K-fold cross-validation using TwoStagesFitters and full_df data. Parameters: Name Type Description Default full_df pd.DataFrame Data to cross validate. required n_splits int Number of folds, defaults to 5. 5 shuffle boolean Shuffle samples before splitting to folds. Defaults to True. True seed Optional[int] Pseudo-random seed to KFold instance. Defaults to None. None fit_beta_kwargs dict, Optional Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. {} covariates list list of covariates to be used in estimating the regression coefficients. None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in full_df). 'X' pid_col str Sample ID column name (must be a column in full_df). 'pid' x0 Union[numpy.array, int], Optional initial guess to pass to scipy.optimize.minimize function 0 verbose int, Optional The verbosity level of pandaallel 2 nb_workers int, Optional The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. 2 metrics str, list Evaluation metrics. Available metrics: 'AUC': AUC at t (will be added to TwoStagesCV.results), 'IAUC': Integrated AUC (will be in TwoStagesCV.integrated_auc), 'GAUC': Global AUC (will be in TwoStagesCV.global_auc). 'BS': Brier score at t (will be added to TwoStagesCV.results), 'IBS': Integrated Brier Score (will be in TwoStagesCV.integrated_bs), 'GBS': Global Brier Score (will be in TwoStagesCV.global_bs). ['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC'] Returns: Type Description Results (pd.DataFrame) Cross validation metrics results Source code in pydts/cross_validation.py def cross_validate ( self , full_df : pd . DataFrame , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , fit_beta_kwargs : dict = {}, covariates = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , x0 : Union [ np . array , int ] = 0 , verbose : int = 2 , nb_workers : int = WORKERS , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]): \"\"\" This method implements K-fold cross-validation using TwoStagesFitters and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. covariates (list): list of covariates to be used in estimating the regression coefficients. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. metrics (str, list): Evaluation metrics. Available metrics: 'AUC': AUC at t (will be added to TwoStagesCV.results), 'IAUC': Integrated AUC (will be in TwoStagesCV.integrated_auc), 'GAUC': Global AUC (will be in TwoStagesCV.global_auc). 'BS': Brier score at t (will be added to TwoStagesCV.results), 'IBS': Integrated Brier Score (will be in TwoStagesCV.integrated_bs), 'GBS': Global Brier Score (will be in TwoStagesCV.global_bs). Returns: Results (pd.DataFrame): Cross validation metrics results \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . models = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_fitter = TwoStagesFitterExact () else : fold_fitter = TwoStagesFitter () print ( f 'Fitting fold { i_fold + 1 } / { n_splits } ' ) fold_fitter . fit ( df = train_df , covariates = covariates , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , x0 = x0 , fit_beta_kwargs = fit_beta_kwargs , verbose = verbose , nb_workers = nb_workers ) #self.models[i_fold] = deepcopy(fold_fitter) self . models [ i_fold ] = fold_fitter pred_df = self . models [ i_fold ] . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ i_fold ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ i_fold ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ i_fold ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'AUC' : tmp_res = events_auc_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) elif metric == 'BS' : tmp_res = events_brier_score_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) return self . results","title":"Cross Validation"},{"location":"api/cross_validation/#pydts.cross_validation.BasePenaltyGridSearchCV","text":"This class implements K-fold cross-validation of the PenaltyGridSearch Source code in pydts/cross_validation.py class BasePenaltyGridSearchCV ( object ): \"\"\" This class implements K-fold cross-validation of the PenaltyGridSearch \"\"\" def __init__ ( self ): self . folds_grids = {} self . test_pids = {} self . global_auc = {} self . integrated_auc = {} self . global_bs = {} self . integrated_bs = {} self . TwoStagesFitter_type = 'CoxPHFitter' def cross_validate ( self , full_df : pd . DataFrame , l1_ratio : float , penalizers : list , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = { 'nb_workers' : WORKERS }, metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) -> pd . DataFrame : \"\"\" This method implements K-fold cross-validation using PenaltyGridSearch and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). twostages_fit_kwargs (dict): keyword arguments to pass to each TwoStagesFitter. metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearchCV.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearchCV.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearchCV.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearchCV.global_bs). Returns: gauc_output_df (pd.DataFrame): Global AUC k-fold mean and standard error for all possible combination of the penalizers. \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . folds_grids = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): print ( f 'Starting fold { i_fold + 1 } / { n_splits } ' ) start = time () self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_pgs = PenaltyGridSearchExact () else : fold_pgs = PenaltyGridSearch () fold_pgs . evaluate ( train_df = train_df , test_df = test_df , l1_ratio = l1_ratio , penalizers = penalizers , metrics = metrics , seed = seed , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , twostages_fit_kwargs = twostages_fit_kwargs ) self . folds_grids [ i_fold ] = fold_pgs for metric in metrics : if metric == 'GAUC' : self . global_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_auc ) elif metric == 'IAUC' : self . integrated_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_auc ) elif metric == 'GBS' : self . global_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_bs ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_bs ) end = time () print ( f 'Finished fold { i_fold + 1 } / { n_splits } , { int ( end - start ) } seconds' ) if 'GAUC' in metrics : res = [ v for k , v in self . global_auc . items ()] gauc_output_df = pd . concat ([ pd . concat ( res , axis = 1 ) . mean ( axis = 1 ), pd . concat ( res , axis = 1 ) . std ( axis = 1 )], keys = [ 'Mean' , 'SE' ], axis = 1 ) else : gauc_output_df = pd . DataFrame () return gauc_output_df","title":"BasePenaltyGridSearchCV"},{"location":"api/cross_validation/#pydts.cross_validation.BasePenaltyGridSearchCV.cross_validate","text":"This method implements K-fold cross-validation using PenaltyGridSearch and full_df data. Parameters: Name Type Description Default full_df pd.DataFrame Data to cross validate. required l1_ratio float regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required penalizers list penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required n_splits int Number of folds, defaults to 5. 5 shuffle boolean Shuffle samples before splitting to folds. Defaults to True. True seed Optional[int] Pseudo-random seed to KFold instance. Defaults to None. None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in full_df). 'X' pid_col str Sample ID column name (must be a column in full_df). 'pid' twostages_fit_kwargs dict keyword arguments to pass to each TwoStagesFitter. {'nb_workers': 2} metrics str, list Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearchCV.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearchCV.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearchCV.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearchCV.global_bs). ['IBS', 'GBS', 'IAUC', 'GAUC'] Returns: Type Description gauc_output_df (pd.DataFrame) Global AUC k-fold mean and standard error for all possible combination of the penalizers. Source code in pydts/cross_validation.py def cross_validate ( self , full_df : pd . DataFrame , l1_ratio : float , penalizers : list , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = { 'nb_workers' : WORKERS }, metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ]) -> pd . DataFrame : \"\"\" This method implements K-fold cross-validation using PenaltyGridSearch and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). twostages_fit_kwargs (dict): keyword arguments to pass to each TwoStagesFitter. metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearchCV.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearchCV.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearchCV.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearchCV.global_bs). Returns: gauc_output_df (pd.DataFrame): Global AUC k-fold mean and standard error for all possible combination of the penalizers. \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . folds_grids = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): print ( f 'Starting fold { i_fold + 1 } / { n_splits } ' ) start = time () self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_pgs = PenaltyGridSearchExact () else : fold_pgs = PenaltyGridSearch () fold_pgs . evaluate ( train_df = train_df , test_df = test_df , l1_ratio = l1_ratio , penalizers = penalizers , metrics = metrics , seed = seed , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , twostages_fit_kwargs = twostages_fit_kwargs ) self . folds_grids [ i_fold ] = fold_pgs for metric in metrics : if metric == 'GAUC' : self . global_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_auc ) elif metric == 'IAUC' : self . integrated_auc [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_auc ) elif metric == 'GBS' : self . global_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . global_bs ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = fold_pgs . convert_results_dict_to_df ( fold_pgs . integrated_bs ) end = time () print ( f 'Finished fold { i_fold + 1 } / { n_splits } , { int ( end - start ) } seconds' ) if 'GAUC' in metrics : res = [ v for k , v in self . global_auc . items ()] gauc_output_df = pd . concat ([ pd . concat ( res , axis = 1 ) . mean ( axis = 1 ), pd . concat ( res , axis = 1 ) . std ( axis = 1 )], keys = [ 'Mean' , 'SE' ], axis = 1 ) else : gauc_output_df = pd . DataFrame () return gauc_output_df","title":"cross_validate()"},{"location":"api/cross_validation/#pydts.cross_validation.BaseTwoStagesCV","text":"This class implements K-fold cross-validation using TwoStagesFitters and TwoStagesFittersExact Source code in pydts/cross_validation.py class BaseTwoStagesCV ( object ): \"\"\" This class implements K-fold cross-validation using TwoStagesFitters and TwoStagesFittersExact \"\"\" def __init__ ( self ): self . models = {} self . test_pids = {} self . results = pd . DataFrame () self . global_auc = {} self . integrated_auc = {} self . global_bs = {} self . integrated_bs = {} self . TwoStagesFitter_type = 'CoxPHFitter' def cross_validate ( self , full_df : pd . DataFrame , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , fit_beta_kwargs : dict = {}, covariates = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , x0 : Union [ np . array , int ] = 0 , verbose : int = 2 , nb_workers : int = WORKERS , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]): \"\"\" This method implements K-fold cross-validation using TwoStagesFitters and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. covariates (list): list of covariates to be used in estimating the regression coefficients. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. metrics (str, list): Evaluation metrics. Available metrics: 'AUC': AUC at t (will be added to TwoStagesCV.results), 'IAUC': Integrated AUC (will be in TwoStagesCV.integrated_auc), 'GAUC': Global AUC (will be in TwoStagesCV.global_auc). 'BS': Brier score at t (will be added to TwoStagesCV.results), 'IBS': Integrated Brier Score (will be in TwoStagesCV.integrated_bs), 'GBS': Global Brier Score (will be in TwoStagesCV.global_bs). Returns: Results (pd.DataFrame): Cross validation metrics results \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . models = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_fitter = TwoStagesFitterExact () else : fold_fitter = TwoStagesFitter () print ( f 'Fitting fold { i_fold + 1 } / { n_splits } ' ) fold_fitter . fit ( df = train_df , covariates = covariates , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , x0 = x0 , fit_beta_kwargs = fit_beta_kwargs , verbose = verbose , nb_workers = nb_workers ) #self.models[i_fold] = deepcopy(fold_fitter) self . models [ i_fold ] = fold_fitter pred_df = self . models [ i_fold ] . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ i_fold ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ i_fold ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ i_fold ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'AUC' : tmp_res = events_auc_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) elif metric == 'BS' : tmp_res = events_brier_score_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) return self . results","title":"BaseTwoStagesCV"},{"location":"api/cross_validation/#pydts.cross_validation.BaseTwoStagesCV.cross_validate","text":"This method implements K-fold cross-validation using TwoStagesFitters and full_df data. Parameters: Name Type Description Default full_df pd.DataFrame Data to cross validate. required n_splits int Number of folds, defaults to 5. 5 shuffle boolean Shuffle samples before splitting to folds. Defaults to True. True seed Optional[int] Pseudo-random seed to KFold instance. Defaults to None. None fit_beta_kwargs dict, Optional Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. {} covariates list list of covariates to be used in estimating the regression coefficients. None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in full_df). 'X' pid_col str Sample ID column name (must be a column in full_df). 'pid' x0 Union[numpy.array, int], Optional initial guess to pass to scipy.optimize.minimize function 0 verbose int, Optional The verbosity level of pandaallel 2 nb_workers int, Optional The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. 2 metrics str, list Evaluation metrics. Available metrics: 'AUC': AUC at t (will be added to TwoStagesCV.results), 'IAUC': Integrated AUC (will be in TwoStagesCV.integrated_auc), 'GAUC': Global AUC (will be in TwoStagesCV.global_auc). 'BS': Brier score at t (will be added to TwoStagesCV.results), 'IBS': Integrated Brier Score (will be in TwoStagesCV.integrated_bs), 'GBS': Global Brier Score (will be in TwoStagesCV.global_bs). ['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC'] Returns: Type Description Results (pd.DataFrame) Cross validation metrics results Source code in pydts/cross_validation.py def cross_validate ( self , full_df : pd . DataFrame , n_splits : int = 5 , shuffle : bool = True , seed : Union [ int , None ] = None , fit_beta_kwargs : dict = {}, covariates = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , x0 : Union [ np . array , int ] = 0 , verbose : int = 2 , nb_workers : int = WORKERS , metrics = [ 'BS' , 'IBS' , 'GBS' , 'AUC' , 'IAUC' , 'GAUC' ]): \"\"\" This method implements K-fold cross-validation using TwoStagesFitters and full_df data. Args: full_df (pd.DataFrame): Data to cross validate. n_splits (int): Number of folds, defaults to 5. shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True. seed: Pseudo-random seed to KFold instance. Defaults to None. fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. covariates (list): list of covariates to be used in estimating the regression coefficients. event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in full_df). pid_col (str): Sample ID column name (must be a column in full_df). x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. metrics (str, list): Evaluation metrics. Available metrics: 'AUC': AUC at t (will be added to TwoStagesCV.results), 'IAUC': Integrated AUC (will be in TwoStagesCV.integrated_auc), 'GAUC': Global AUC (will be in TwoStagesCV.global_auc). 'BS': Brier score at t (will be added to TwoStagesCV.results), 'IBS': Integrated Brier Score (will be in TwoStagesCV.integrated_bs), 'GBS': Global Brier Score (will be in TwoStagesCV.global_bs). Returns: Results (pd.DataFrame): Cross validation metrics results \"\"\" if isinstance ( metrics , str ): metrics = [ metrics ] self . models = {} self . kfold_cv = KFold ( n_splits = n_splits , shuffle = shuffle , random_state = seed ) if 'C' in full_df . columns : full_df = full_df . drop ([ 'C' ], axis = 1 ) if 'T' in full_df . columns : full_df = full_df . drop ([ 'T' ], axis = 1 ) for i_fold , ( train_index , test_index ) in enumerate ( self . kfold_cv . split ( full_df )): self . test_pids [ i_fold ] = full_df . iloc [ test_index ][ pid_col ] . values train_df , test_df = full_df . iloc [ train_index ], full_df . iloc [ test_index ] if self . TwoStagesFitter_type == 'Exact' : fold_fitter = TwoStagesFitterExact () else : fold_fitter = TwoStagesFitter () print ( f 'Fitting fold { i_fold + 1 } / { n_splits } ' ) fold_fitter . fit ( df = train_df , covariates = covariates , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col , x0 = x0 , fit_beta_kwargs = fit_beta_kwargs , verbose = verbose , nb_workers = nb_workers ) #self.models[i_fold] = deepcopy(fold_fitter) self . models [ i_fold ] = fold_fitter pred_df = self . models [ i_fold ] . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ i_fold ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ i_fold ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ i_fold ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ i_fold ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'AUC' : tmp_res = events_auc_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) elif metric == 'BS' : tmp_res = events_brier_score_at_t ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) tmp_res = pd . concat ([ tmp_res ], keys = [ i_fold ], names = [ 'fold' ]) tmp_res = pd . concat ([ tmp_res ], keys = [ metric ], names = [ 'metric' ]) self . results = pd . concat ([ self . results , tmp_res ], axis = 0 ) return self . results","title":"cross_validate()"},{"location":"api/data_expansion_fitter/","text":"This class implements the estimation procedure of Lee et al. (2018) [1]. See also the Example section. Examples: 1 2 3 4 from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter () fitter . fit ( df = train_df , event_type_col = 'J' , duration_col = 'X' ) fitter . print_summary () References [1] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881 Source code in pydts/fitters.py class DataExpansionFitter ( ExpansionBasedFitter ): \"\"\" This class implements the estimation procedure of Lee et al. (2018) [1]. See also the Example section. Example: ```py linenums=\"1\" from pydts.fitters import DataExpansionFitter fitter = DataExpansionFitter() fitter.fit(df=train_df, event_type_col='J', duration_col='X') fitter.print_summary() ``` References: [1] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881 \"\"\" def __init__ ( self ): super () . __init__ () self . models_kwargs = dict ( family = sm . families . Binomial ()) def _fit_event ( self , model_fit_kwargs = {}): \"\"\" This method fits a model for a GLM model for a specific event. Args: model_fit_kwargs (dict, Optional): Keyword arguments to pass to model.fit() method. Returns: fitted GLM model \"\"\" model = sm . GLM . from_formula ( formula = self . formula , data = self . expanded_df , ** self . models_kwargs ) return model . fit ( ** model_fit_kwargs ) def fit ( self , df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , skip_expansion : bool = False , covariates : Optional [ list ] = None , formula : Optional [ str ] = None , models_kwargs : Optional [ dict ] = None , model_fit_kwargs : Optional [ dict ] = {}) -> dict : \"\"\" This method fits a model to the discrete data. Args: df (pd.DataFrame): training data for fitting the model event_type_col (str): The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. covariates (list, Optional): A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col. formula (str, Optional): Model formula to be fitted. Patsy format string. models_kwargs (dict, Optional): Keyword arguments to pass to model instance initiation. model_fit_kwargs (dict, Optional): Keyword arguments to pass to model.fit() method. Returns: event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event. References: [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 \"\"\" if models_kwargs is not None : self . models_kwargs = models_kwargs if 'C' in df . columns : raise ValueError ( 'C is an invalid column name, to avoid errors with categorical symbol C() in formula' ) self . _validate_cols ( df , event_type_col , duration_col , pid_col ) if covariates is not None : cov_not_in_df = [ cov for cov in covariates if cov not in df . columns ] if len ( cov_not_in_df ) > 0 : raise ValueError ( f \"Error during fit - missing covariates from df: { cov_not_in_df } \" ) self . events = [ c for c in sorted ( df [ event_type_col ] . unique ()) if c != 0 ] self . covariates = [ col for col in df if col not in [ event_type_col , duration_col , pid_col ]] \\ if covariates is None else covariates self . times = sorted ( df [ duration_col ] . unique ()) if not skip_expansion : self . expanded_df = self . _expand_data ( df = df , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col ) else : print ( 'Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.' ) self . expanded_df = df for event in self . events : cov = ' + ' . join ( self . covariates ) _formula = f 'j_ { event } ~ { formula } ' if formula is not None else \\ f 'j_ { event } ~ { cov } + C( { duration_col } ) -1 ' self . formula = _formula self . event_models [ event ] = self . _fit_event ( model_fit_kwargs = model_fit_kwargs ) return self . event_models def print_summary ( self , summary_func : str = \"summary\" , summary_kwargs : dict = {}) -> None : \"\"\" This method prints the summary of the fitted models for all the events. Args: summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\"). summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function. Returns: None \"\"\" for event , model in self . event_models . items (): _summary_func = getattr ( model , summary_func , None ) if _summary_func is not None : print ( f ' \\n\\n Model summary for event: { event } ' ) print ( _summary_func ( ** summary_kwargs )) else : print ( f 'Not { summary_func } function in event { event } model' ) def predict_hazard_jt ( self , df : pd . DataFrame , event : Union [ str , int ], t : Union [ Iterable , int ], n_jobs : int = - 1 ) -> pd . DataFrame : \"\"\" This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Args: df (pd.DataFrame): samples to predict for event (Union[str, int]): event name t (np.array): times to calculate the hazard for n_jobs: number of CPUs to use, defualt to every available CPU Returns: df (pd.DataFrame): samples with the prediction columns \"\"\" t = self . _validate_t ( t , return_iter = True ) assert event in self . events , \\ f \"Cannot predict for event { event } - it was not included during .fit()\" self . _validate_covariates_in_df ( df . head ()) _t = np . array ([ t_i for t_i in t if ( f 'hazard_j { event } _t { t_i } ' not in df . columns )]) if len ( _t ) == 0 : return df temp_df = df . copy () model = self . event_models [ event ] res = Parallel ( n_jobs = n_jobs )( delayed ( model . predict )( df [ self . covariates ] . assign ( X = c )) for c in t ) temp_hazard_df = pd . concat ( res , axis = 1 ) temp_df [[ f 'hazard_j { event } _t { c_ } ' for c_ in t ]] = temp_hazard_df . values return temp_df def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : summary = self . event_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [ - len ( self . covariates ):]], axis = 1 ) return full_table def get_alpha_df ( self ): \"\"\" This function returns the Alpha coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Alpha coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : summary = self . event_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [: - len ( self . covariates ) - 1 ]], axis = 1 ) return full_table fit ( self , df , event_type_col = 'J' , duration_col = 'X' , pid_col = 'pid' , skip_expansion = False , covariates = None , formula = None , models_kwargs = None , model_fit_kwargs = {}) \u00a4 This method fits a model to the discrete data. Parameters: Name Type Description Default df pd.DataFrame training data for fitting the model required event_type_col str The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in df). 'X' pid_col str Sample ID column name (must be a column in df). 'pid' skip_expansion boolean Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. False covariates list, Optional A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col. None formula str, Optional Model formula to be fitted. Patsy format string. None models_kwargs dict, Optional Keyword arguments to pass to model instance initiation. None model_fit_kwargs dict, Optional Keyword arguments to pass to model.fit() method. {} Returns: Type Description event_models (dict) Fitted models dictionary. Keys - event names, Values - fitted models for the event. References [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 Source code in pydts/fitters.py def fit ( self , df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , skip_expansion : bool = False , covariates : Optional [ list ] = None , formula : Optional [ str ] = None , models_kwargs : Optional [ dict ] = None , model_fit_kwargs : Optional [ dict ] = {}) -> dict : \"\"\" This method fits a model to the discrete data. Args: df (pd.DataFrame): training data for fitting the model event_type_col (str): The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. covariates (list, Optional): A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col. formula (str, Optional): Model formula to be fitted. Patsy format string. models_kwargs (dict, Optional): Keyword arguments to pass to model instance initiation. model_fit_kwargs (dict, Optional): Keyword arguments to pass to model.fit() method. Returns: event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event. References: [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 \"\"\" if models_kwargs is not None : self . models_kwargs = models_kwargs if 'C' in df . columns : raise ValueError ( 'C is an invalid column name, to avoid errors with categorical symbol C() in formula' ) self . _validate_cols ( df , event_type_col , duration_col , pid_col ) if covariates is not None : cov_not_in_df = [ cov for cov in covariates if cov not in df . columns ] if len ( cov_not_in_df ) > 0 : raise ValueError ( f \"Error during fit - missing covariates from df: { cov_not_in_df } \" ) self . events = [ c for c in sorted ( df [ event_type_col ] . unique ()) if c != 0 ] self . covariates = [ col for col in df if col not in [ event_type_col , duration_col , pid_col ]] \\ if covariates is None else covariates self . times = sorted ( df [ duration_col ] . unique ()) if not skip_expansion : self . expanded_df = self . _expand_data ( df = df , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col ) else : print ( 'Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.' ) self . expanded_df = df for event in self . events : cov = ' + ' . join ( self . covariates ) _formula = f 'j_ { event } ~ { formula } ' if formula is not None else \\ f 'j_ { event } ~ { cov } + C( { duration_col } ) -1 ' self . formula = _formula self . event_models [ event ] = self . _fit_event ( model_fit_kwargs = model_fit_kwargs ) return self . event_models get_alpha_df ( self ) \u00a4 This function returns the Alpha coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Alpha coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_alpha_df ( self ): \"\"\" This function returns the Alpha coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Alpha coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : summary = self . event_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [: - len ( self . covariates ) - 1 ]], axis = 1 ) return full_table get_beta_SE ( self ) \u00a4 This function returns the Beta coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Beta coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : summary = self . event_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [ - len ( self . covariates ):]], axis = 1 ) return full_table predict_hazard_jt ( self , df , event , t , n_jobs =- 1 ) \u00a4 This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Parameters: Name Type Description Default df pd.DataFrame samples to predict for required event Union[str, int] event name required t np.array times to calculate the hazard for required n_jobs int number of CPUs to use, defualt to every available CPU -1 Returns: Type Description df (pd.DataFrame) samples with the prediction columns Source code in pydts/fitters.py def predict_hazard_jt ( self , df : pd . DataFrame , event : Union [ str , int ], t : Union [ Iterable , int ], n_jobs : int = - 1 ) -> pd . DataFrame : \"\"\" This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Args: df (pd.DataFrame): samples to predict for event (Union[str, int]): event name t (np.array): times to calculate the hazard for n_jobs: number of CPUs to use, defualt to every available CPU Returns: df (pd.DataFrame): samples with the prediction columns \"\"\" t = self . _validate_t ( t , return_iter = True ) assert event in self . events , \\ f \"Cannot predict for event { event } - it was not included during .fit()\" self . _validate_covariates_in_df ( df . head ()) _t = np . array ([ t_i for t_i in t if ( f 'hazard_j { event } _t { t_i } ' not in df . columns )]) if len ( _t ) == 0 : return df temp_df = df . copy () model = self . event_models [ event ] res = Parallel ( n_jobs = n_jobs )( delayed ( model . predict )( df [ self . covariates ] . assign ( X = c )) for c in t ) temp_hazard_df = pd . concat ( res , axis = 1 ) temp_df [[ f 'hazard_j { event } _t { c_ } ' for c_ in t ]] = temp_hazard_df . values return temp_df print_summary ( self , summary_func = 'summary' , summary_kwargs = {}) \u00a4 This method prints the summary of the fitted models for all the events. Parameters: Name Type Description Default summary_func str, Optional print summary method of the fitted model type (\"summary\", \"print_summary\"). 'summary' summary_kwargs dict, Optional Keyword arguments to pass to the model summary function. {} Returns: Type Description None None Source code in pydts/fitters.py def print_summary ( self , summary_func : str = \"summary\" , summary_kwargs : dict = {}) -> None : \"\"\" This method prints the summary of the fitted models for all the events. Args: summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\"). summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function. Returns: None \"\"\" for event , model in self . event_models . items (): _summary_func = getattr ( model , summary_func , None ) if _summary_func is not None : print ( f ' \\n\\n Model summary for event: { event } ' ) print ( _summary_func ( ** summary_kwargs )) else : print ( f 'Not { summary_func } function in event { event } model' )","title":"Data Expansion Procedure of Lee et al. (2018)"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.fit","text":"This method fits a model to the discrete data. Parameters: Name Type Description Default df pd.DataFrame training data for fitting the model required event_type_col str The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in df). 'X' pid_col str Sample ID column name (must be a column in df). 'pid' skip_expansion boolean Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. False covariates list, Optional A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col. None formula str, Optional Model formula to be fitted. Patsy format string. None models_kwargs dict, Optional Keyword arguments to pass to model instance initiation. None model_fit_kwargs dict, Optional Keyword arguments to pass to model.fit() method. {} Returns: Type Description event_models (dict) Fitted models dictionary. Keys - event names, Values - fitted models for the event. References [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 Source code in pydts/fitters.py def fit ( self , df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , skip_expansion : bool = False , covariates : Optional [ list ] = None , formula : Optional [ str ] = None , models_kwargs : Optional [ dict ] = None , model_fit_kwargs : Optional [ dict ] = {}) -> dict : \"\"\" This method fits a model to the discrete data. Args: df (pd.DataFrame): training data for fitting the model event_type_col (str): The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. covariates (list, Optional): A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col. formula (str, Optional): Model formula to be fitted. Patsy format string. models_kwargs (dict, Optional): Keyword arguments to pass to model instance initiation. model_fit_kwargs (dict, Optional): Keyword arguments to pass to model.fit() method. Returns: event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event. References: [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 \"\"\" if models_kwargs is not None : self . models_kwargs = models_kwargs if 'C' in df . columns : raise ValueError ( 'C is an invalid column name, to avoid errors with categorical symbol C() in formula' ) self . _validate_cols ( df , event_type_col , duration_col , pid_col ) if covariates is not None : cov_not_in_df = [ cov for cov in covariates if cov not in df . columns ] if len ( cov_not_in_df ) > 0 : raise ValueError ( f \"Error during fit - missing covariates from df: { cov_not_in_df } \" ) self . events = [ c for c in sorted ( df [ event_type_col ] . unique ()) if c != 0 ] self . covariates = [ col for col in df if col not in [ event_type_col , duration_col , pid_col ]] \\ if covariates is None else covariates self . times = sorted ( df [ duration_col ] . unique ()) if not skip_expansion : self . expanded_df = self . _expand_data ( df = df , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col ) else : print ( 'Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.' ) self . expanded_df = df for event in self . events : cov = ' + ' . join ( self . covariates ) _formula = f 'j_ { event } ~ { formula } ' if formula is not None else \\ f 'j_ { event } ~ { cov } + C( { duration_col } ) -1 ' self . formula = _formula self . event_models [ event ] = self . _fit_event ( model_fit_kwargs = model_fit_kwargs ) return self . event_models","title":"fit()"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.get_alpha_df","text":"This function returns the Alpha coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Alpha coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_alpha_df ( self ): \"\"\" This function returns the Alpha coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Alpha coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : summary = self . event_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [: - len ( self . covariates ) - 1 ]], axis = 1 ) return full_table","title":"get_alpha_df()"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.get_beta_SE","text":"This function returns the Beta coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Beta coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : summary = self . event_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [ - len ( self . covariates ):]], axis = 1 ) return full_table","title":"get_beta_SE()"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_hazard_jt","text":"This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Parameters: Name Type Description Default df pd.DataFrame samples to predict for required event Union[str, int] event name required t np.array times to calculate the hazard for required n_jobs int number of CPUs to use, defualt to every available CPU -1 Returns: Type Description df (pd.DataFrame) samples with the prediction columns Source code in pydts/fitters.py def predict_hazard_jt ( self , df : pd . DataFrame , event : Union [ str , int ], t : Union [ Iterable , int ], n_jobs : int = - 1 ) -> pd . DataFrame : \"\"\" This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Args: df (pd.DataFrame): samples to predict for event (Union[str, int]): event name t (np.array): times to calculate the hazard for n_jobs: number of CPUs to use, defualt to every available CPU Returns: df (pd.DataFrame): samples with the prediction columns \"\"\" t = self . _validate_t ( t , return_iter = True ) assert event in self . events , \\ f \"Cannot predict for event { event } - it was not included during .fit()\" self . _validate_covariates_in_df ( df . head ()) _t = np . array ([ t_i for t_i in t if ( f 'hazard_j { event } _t { t_i } ' not in df . columns )]) if len ( _t ) == 0 : return df temp_df = df . copy () model = self . event_models [ event ] res = Parallel ( n_jobs = n_jobs )( delayed ( model . predict )( df [ self . covariates ] . assign ( X = c )) for c in t ) temp_hazard_df = pd . concat ( res , axis = 1 ) temp_df [[ f 'hazard_j { event } _t { c_ } ' for c_ in t ]] = temp_hazard_df . values return temp_df","title":"predict_hazard_jt()"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.print_summary","text":"This method prints the summary of the fitted models for all the events. Parameters: Name Type Description Default summary_func str, Optional print summary method of the fitted model type (\"summary\", \"print_summary\"). 'summary' summary_kwargs dict, Optional Keyword arguments to pass to the model summary function. {} Returns: Type Description None None Source code in pydts/fitters.py def print_summary ( self , summary_func : str = \"summary\" , summary_kwargs : dict = {}) -> None : \"\"\" This method prints the summary of the fitted models for all the events. Args: summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\"). summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function. Returns: None \"\"\" for event , model in self . event_models . items (): _summary_func = getattr ( model , summary_func , None ) if _summary_func is not None : print ( f ' \\n\\n Model summary for event: { event } ' ) print ( _summary_func ( ** summary_kwargs )) else : print ( f 'Not { summary_func } function in event { event } model' )","title":"print_summary()"},{"location":"api/evaluation/","text":"event_specific_auc_at_t ( pred_df , event , t , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the event specific AUC at time t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the integrated AUC for. required t int time to calculate the AUC for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific AUC for all times included in duration_col of pred_df. Source code in pydts/evaluation.py def event_specific_auc_at_t ( pred_df : pd . DataFrame , event : int , t : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the event specific AUC at time t. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the integrated AUC for. t (int): time to calculate the AUC for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific AUC for all times included in duration_col of pred_df. \"\"\" event_observed_at_t_df = pred_df [( pred_df [ event_type_col ] == event ) & ( pred_df [ duration_col ] == t )] no_event_at_t_df = pred_df [ pred_df [ duration_col ] >= t ] no_event_at_t_df = no_event_at_t_df [ ~ (( no_event_at_t_df [ event_type_col ] == event ) & ( no_event_at_t_df [ duration_col ] == t ))] total_t = ( len ( event_observed_at_t_df ) * len ( no_event_at_t_df )) if total_t == 0 : print ( f 'AUC could not be calculated for event { event } at time { t } - no test pairs of with and without observed event { event } at time { t } ' ) return np . nan correct_order = 0 for i_idx , i_row in event_observed_at_t_df . iterrows (): pi_ij = i_row . loc [ f 'prob_j { event } _at_t { t } ' ] pi_mj = no_event_at_t_df . loc [:, f 'prob_j { event } _at_t { t } ' ] correct_order += (( pi_ij > pi_mj ) . sum () + 0.5 * ( pi_ij == pi_mj ) . sum ()) return correct_order / total_t event_specific_auc_at_t_all ( pred_df , event , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the event specific AUC at time t for all times included in duration_col of pred_df. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the AUC for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific AUC for all times included in duration_col of pred_df. Source code in pydts/evaluation.py def event_specific_auc_at_t_all ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . Series : \"\"\" This function implements the calculation of the event specific AUC at time t for all times included in duration_col of pred_df. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the AUC for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific AUC for all times included in duration_col of pred_df. \"\"\" res = {} for t in sorted ( pred_df [ duration_col ] . unique ())[: - 1 ]: res [ t ] = event_specific_auc_at_t ( pred_df = pred_df , event = event , t = t , event_type_col = event_type_col , duration_col = duration_col ) return pd . Series ( res , name = event ) event_specific_brier_score_at_t ( pred_df , event , t , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the event specific Brier Score at time t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier Score for. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the Brier Score for. required t int time to calculate the Brier Score for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific Brier Score at time t. Source code in pydts/evaluation.py def event_specific_brier_score_at_t ( pred_df : pd . DataFrame , event : int , t : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the event specific Brier Score at time t. Args: pred_df (pd.DataFrame): Data to calculate Brier Score for. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the Brier Score for. t (int): time to calculate the Brier Score for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific Brier Score at time t. \"\"\" pi_ij = pred_df . loc [:, f 'prob_j { event } _at_t { t } ' ] D_ij = (( pred_df . loc [:, event_type_col ] == event ) & ( pred_df . loc [:, duration_col ] == t )) . astype ( int ) censoring_kmf = KaplanMeierFitter () censoring_kmf . fit ( durations = pred_df [ duration_col ], event_observed = ( pred_df [ event_type_col ] == 0 )) in_risk_set_at_t = ( pred_df . loc [:, duration_col ] >= t ) . astype ( int ) W_ij = ( in_risk_set_at_t / censoring_kmf . predict ( times = t )) BS_jt = (( W_ij * (( D_ij - pi_ij ) ** 2 )) . sum () / in_risk_set_at_t . sum ()) return BS_jt event_specific_brier_score_at_t_all ( pred_df , event , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the event specific Brier Score at time t for all times included in duration_col of pred_df. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See c required event int Event-type to calculate the Brier Score for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific Brier Score for all times included in duration_col of pred_df. Source code in pydts/evaluation.py def event_specific_brier_score_at_t_all ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . Series : \"\"\" This function implements the calculation of the event specific Brier Score at time t for all times included in duration_col of pred_df. Args: pred_df (pd.DataFrame): Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See c event (int): Event-type to calculate the Brier Score for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific Brier Score for all times included in duration_col of pred_df. \"\"\" res = {} for t in sorted ( pred_df [ duration_col ] . unique ())[: - 1 ]: res [ t ] = event_specific_brier_score_at_t ( pred_df = pred_df , event = event , t = t , event_type_col = event_type_col , duration_col = duration_col ) return pd . Series ( res , name = event ) event_specific_integrated_auc ( pred_df , event , event_type_col = 'J' , duration_col = 'X' , weights = None ) \u00a4 This function implements the calculation of the event specific integrated auc. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the integrated AUC for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' weights pd.Series Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. None Returns: Type Description result (float) integrated AUC results. Source code in pydts/evaluation.py def event_specific_integrated_auc ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' , weights : Union [ pd . Series , None ] = None ) -> float : \"\"\" This function implements the calculation of the event specific integrated auc. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the integrated AUC for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. weights (pd.Series): Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. Returns: result (float): integrated AUC results. \"\"\" auc_at_t = event_specific_auc_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) if auc_at_t . isnull () . any (): print ( f 'There are NaN values in AUC(t) during Integrated AUC calculation for event { event } . Times: { auc_at_t [ auc_at_t . isnull ()] . index } \\n ' ) if weights is None : if auc_at_t . isnull () . any (): print ( f 'Please check there are events of type { event } at each time in pred_df or provide a weights vector with weight 0 for the problematic times.' ) return np . nan weights = event_specific_weights ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) result = auc_at_t . dot ( weights . sort_index ()) return result event_specific_integrated_brier_score ( pred_df , event , event_type_col = 'J' , duration_col = 'X' , weights = None ) \u00a4 This function implements the calculation of the event specific integrated Brier Score. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the integrated Brier Score for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' weights pd.Series Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. None Returns: Type Description result (float) integrated Brier Score results. Source code in pydts/evaluation.py def event_specific_integrated_brier_score ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' , weights : Union [ pd . Series , None ] = None ) -> float : \"\"\" This function implements the calculation of the event specific integrated Brier Score. Args: pred_df (pd.DataFrame): Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the integrated Brier Score for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. weights (pd.Series): Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. Returns: result (float): integrated Brier Score results. \"\"\" brier_score_at_t = event_specific_brier_score_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) if brier_score_at_t . isnull () . any (): print ( f 'There are NaN values in BS(t) during Integrated Brier Score calculation for event { event } . Times: { brier_score_at_t [ brier_score_at_t . isnull ()] . index } \\n ' ) if weights is None : weights = event_specific_weights ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) result = brier_score_at_t . dot ( weights . sort_index ()) return result event_specific_weights ( pred_df , event , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the event specific time-weights. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the weights for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific weights. Source code in pydts/evaluation.py def event_specific_weights ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . Series : \"\"\" This function implements the calculation of the event specific time-weights. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the weights for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific weights. \"\"\" event_df = pred_df [ pred_df [ event_type_col ] == event ] if len ( event_df ) == 0 : print ( f 'Could not calculate weights for event { event } - no test events' ) return np . nan weights = event_df . groupby ( duration_col ) . size () / event_df . groupby ( duration_col ) . size () . sum () times = sorted ( pred_df [ duration_col ] . unique ())[: - 1 ] return weights . reindex ( times ) . fillna ( 0 ) events_auc_at_t ( pred_df , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the events AUC at t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate AUC. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description event_auc_at_t_df (pd.DataFrame) events AUC at t results. Source code in pydts/evaluation.py def events_auc_at_t ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . DataFrame : \"\"\" This function implements the calculation of the events AUC at t. Args: pred_df (pd.DataFrame): Data to calculate AUC. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: event_auc_at_t_df (pd.DataFrame): events AUC at t results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] event_auc_at_t_df = pd . DataFrame () for event in sorted ( events ): event_auc_at_t_df = pd . concat ([ event_auc_at_t_df , event_specific_auc_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col )], axis = 1 ) return event_auc_at_t_df . T events_brier_score_at_t ( pred_df , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the events Brier score at t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description event_brier_score_at_t_df (pd.DataFrame) events Brier score at t results. Source code in pydts/evaluation.py def events_brier_score_at_t ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . DataFrame : \"\"\" This function implements the calculation of the events Brier score at t. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: event_brier_score_at_t_df (pd.DataFrame): events Brier score at t results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] event_brier_score_at_t_df = pd . DataFrame () for event in sorted ( events ): event_brier_score_at_t_df = pd . concat ([ event_brier_score_at_t_df , event_specific_brier_score_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col )], axis = 1 ) return event_brier_score_at_t_df . T events_integrated_auc ( pred_df , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the integrated AUC to all events. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description integrated_auc (dict) integrated AUC results. Source code in pydts/evaluation.py def events_integrated_auc ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> dict : \"\"\" This function implements the calculation of the integrated AUC to all events. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: integrated_auc (dict): integrated AUC results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] integrated_auc = {} for event in sorted ( events ): integrated_auc [ event ] = event_specific_integrated_auc ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return integrated_auc events_integrated_brier_score ( pred_df , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the integrated Brier Score to all events. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate integrated Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description integrated_brier_score (dict) integrated Brier Score results. Source code in pydts/evaluation.py def events_integrated_brier_score ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> dict : \"\"\" This function implements the calculation of the integrated Brier Score to all events. Args: pred_df (pd.DataFrame): Data to calculate integrated Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: integrated_brier_score (dict): integrated Brier Score results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] integrated_brier_score = {} for event in sorted ( events ): integrated_brier_score [ event ] = event_specific_integrated_brier_score ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return integrated_brier_score global_auc ( pred_df , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the global AUC. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description global_auc (float) global AUC results. Source code in pydts/evaluation.py def global_auc ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the global AUC. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: global_auc (float): global AUC results. \"\"\" e_j_ser = pred_df [ pred_df [ event_type_col ] != 0 ] . groupby ( 'J' ) . size () . sort_index () total_e = e_j_ser . sum () global_auc = 0 for event , e_j in e_j_ser . iteritems (): global_auc += ( e_j / total_e ) * event_specific_integrated_auc ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return global_auc global_brier_score ( pred_df , event_type_col = 'J' , duration_col = 'X' ) \u00a4 This function implements the calculation of the global Brier Score. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description global_auc (float) global Brier Score results. Source code in pydts/evaluation.py def global_brier_score ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the global Brier Score. Args: pred_df (pd.DataFrame): Data to calculate Brier score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: global_auc (float): global Brier Score results. \"\"\" e_j_ser = pred_df [ pred_df [ event_type_col ] != 0 ] . groupby ( 'J' ) . size () . sort_index () total_e = e_j_ser . sum () global_bs = 0 for event , e_j in e_j_ser . iteritems (): global_bs += ( e_j / total_e ) * event_specific_integrated_brier_score ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return global_bs","title":"Evaluation"},{"location":"api/evaluation/#pydts.evaluation.event_specific_auc_at_t","text":"This function implements the calculation of the event specific AUC at time t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the integrated AUC for. required t int time to calculate the AUC for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific AUC for all times included in duration_col of pred_df. Source code in pydts/evaluation.py def event_specific_auc_at_t ( pred_df : pd . DataFrame , event : int , t : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the event specific AUC at time t. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the integrated AUC for. t (int): time to calculate the AUC for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific AUC for all times included in duration_col of pred_df. \"\"\" event_observed_at_t_df = pred_df [( pred_df [ event_type_col ] == event ) & ( pred_df [ duration_col ] == t )] no_event_at_t_df = pred_df [ pred_df [ duration_col ] >= t ] no_event_at_t_df = no_event_at_t_df [ ~ (( no_event_at_t_df [ event_type_col ] == event ) & ( no_event_at_t_df [ duration_col ] == t ))] total_t = ( len ( event_observed_at_t_df ) * len ( no_event_at_t_df )) if total_t == 0 : print ( f 'AUC could not be calculated for event { event } at time { t } - no test pairs of with and without observed event { event } at time { t } ' ) return np . nan correct_order = 0 for i_idx , i_row in event_observed_at_t_df . iterrows (): pi_ij = i_row . loc [ f 'prob_j { event } _at_t { t } ' ] pi_mj = no_event_at_t_df . loc [:, f 'prob_j { event } _at_t { t } ' ] correct_order += (( pi_ij > pi_mj ) . sum () + 0.5 * ( pi_ij == pi_mj ) . sum ()) return correct_order / total_t","title":"event_specific_auc_at_t()"},{"location":"api/evaluation/#pydts.evaluation.event_specific_auc_at_t_all","text":"This function implements the calculation of the event specific AUC at time t for all times included in duration_col of pred_df. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the AUC for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific AUC for all times included in duration_col of pred_df. Source code in pydts/evaluation.py def event_specific_auc_at_t_all ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . Series : \"\"\" This function implements the calculation of the event specific AUC at time t for all times included in duration_col of pred_df. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the AUC for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific AUC for all times included in duration_col of pred_df. \"\"\" res = {} for t in sorted ( pred_df [ duration_col ] . unique ())[: - 1 ]: res [ t ] = event_specific_auc_at_t ( pred_df = pred_df , event = event , t = t , event_type_col = event_type_col , duration_col = duration_col ) return pd . Series ( res , name = event )","title":"event_specific_auc_at_t_all()"},{"location":"api/evaluation/#pydts.evaluation.event_specific_brier_score_at_t","text":"This function implements the calculation of the event specific Brier Score at time t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier Score for. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the Brier Score for. required t int time to calculate the Brier Score for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific Brier Score at time t. Source code in pydts/evaluation.py def event_specific_brier_score_at_t ( pred_df : pd . DataFrame , event : int , t : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the event specific Brier Score at time t. Args: pred_df (pd.DataFrame): Data to calculate Brier Score for. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the Brier Score for. t (int): time to calculate the Brier Score for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific Brier Score at time t. \"\"\" pi_ij = pred_df . loc [:, f 'prob_j { event } _at_t { t } ' ] D_ij = (( pred_df . loc [:, event_type_col ] == event ) & ( pred_df . loc [:, duration_col ] == t )) . astype ( int ) censoring_kmf = KaplanMeierFitter () censoring_kmf . fit ( durations = pred_df [ duration_col ], event_observed = ( pred_df [ event_type_col ] == 0 )) in_risk_set_at_t = ( pred_df . loc [:, duration_col ] >= t ) . astype ( int ) W_ij = ( in_risk_set_at_t / censoring_kmf . predict ( times = t )) BS_jt = (( W_ij * (( D_ij - pi_ij ) ** 2 )) . sum () / in_risk_set_at_t . sum ()) return BS_jt","title":"event_specific_brier_score_at_t()"},{"location":"api/evaluation/#pydts.evaluation.event_specific_brier_score_at_t_all","text":"This function implements the calculation of the event specific Brier Score at time t for all times included in duration_col of pred_df. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See c required event int Event-type to calculate the Brier Score for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific Brier Score for all times included in duration_col of pred_df. Source code in pydts/evaluation.py def event_specific_brier_score_at_t_all ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . Series : \"\"\" This function implements the calculation of the event specific Brier Score at time t for all times included in duration_col of pred_df. Args: pred_df (pd.DataFrame): Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See c event (int): Event-type to calculate the Brier Score for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific Brier Score for all times included in duration_col of pred_df. \"\"\" res = {} for t in sorted ( pred_df [ duration_col ] . unique ())[: - 1 ]: res [ t ] = event_specific_brier_score_at_t ( pred_df = pred_df , event = event , t = t , event_type_col = event_type_col , duration_col = duration_col ) return pd . Series ( res , name = event )","title":"event_specific_brier_score_at_t_all()"},{"location":"api/evaluation/#pydts.evaluation.event_specific_integrated_auc","text":"This function implements the calculation of the event specific integrated auc. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the integrated AUC for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' weights pd.Series Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. None Returns: Type Description result (float) integrated AUC results. Source code in pydts/evaluation.py def event_specific_integrated_auc ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' , weights : Union [ pd . Series , None ] = None ) -> float : \"\"\" This function implements the calculation of the event specific integrated auc. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the integrated AUC for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. weights (pd.Series): Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. Returns: result (float): integrated AUC results. \"\"\" auc_at_t = event_specific_auc_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) if auc_at_t . isnull () . any (): print ( f 'There are NaN values in AUC(t) during Integrated AUC calculation for event { event } . Times: { auc_at_t [ auc_at_t . isnull ()] . index } \\n ' ) if weights is None : if auc_at_t . isnull () . any (): print ( f 'Please check there are events of type { event } at each time in pred_df or provide a weights vector with weight 0 for the problematic times.' ) return np . nan weights = event_specific_weights ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) result = auc_at_t . dot ( weights . sort_index ()) return result","title":"event_specific_integrated_auc()"},{"location":"api/evaluation/#pydts.evaluation.event_specific_integrated_brier_score","text":"This function implements the calculation of the event specific integrated Brier Score. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the integrated Brier Score for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' weights pd.Series Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. None Returns: Type Description result (float) integrated Brier Score results. Source code in pydts/evaluation.py def event_specific_integrated_brier_score ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' , weights : Union [ pd . Series , None ] = None ) -> float : \"\"\" This function implements the calculation of the event specific integrated Brier Score. Args: pred_df (pd.DataFrame): Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the integrated Brier Score for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. weights (pd.Series): Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times. Returns: result (float): integrated Brier Score results. \"\"\" brier_score_at_t = event_specific_brier_score_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) if brier_score_at_t . isnull () . any (): print ( f 'There are NaN values in BS(t) during Integrated Brier Score calculation for event { event } . Times: { brier_score_at_t [ brier_score_at_t . isnull ()] . index } \\n ' ) if weights is None : weights = event_specific_weights ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) result = brier_score_at_t . dot ( weights . sort_index ()) return result","title":"event_specific_integrated_brier_score()"},{"location":"api/evaluation/#pydts.evaluation.event_specific_weights","text":"This function implements the calculation of the event specific time-weights. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() required event int Event-type to calculate the weights for. required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description result (pd.Series) event specific weights. Source code in pydts/evaluation.py def event_specific_weights ( pred_df : pd . DataFrame , event : int , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . Series : \"\"\" This function implements the calculation of the event specific time-weights. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events() event (int): Event-type to calculate the weights for. duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: result (pd.Series): event specific weights. \"\"\" event_df = pred_df [ pred_df [ event_type_col ] == event ] if len ( event_df ) == 0 : print ( f 'Could not calculate weights for event { event } - no test events' ) return np . nan weights = event_df . groupby ( duration_col ) . size () / event_df . groupby ( duration_col ) . size () . sum () times = sorted ( pred_df [ duration_col ] . unique ())[: - 1 ] return weights . reindex ( times ) . fillna ( 0 )","title":"event_specific_weights()"},{"location":"api/evaluation/#pydts.evaluation.events_auc_at_t","text":"This function implements the calculation of the events AUC at t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate AUC. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description event_auc_at_t_df (pd.DataFrame) events AUC at t results. Source code in pydts/evaluation.py def events_auc_at_t ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . DataFrame : \"\"\" This function implements the calculation of the events AUC at t. Args: pred_df (pd.DataFrame): Data to calculate AUC. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: event_auc_at_t_df (pd.DataFrame): events AUC at t results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] event_auc_at_t_df = pd . DataFrame () for event in sorted ( events ): event_auc_at_t_df = pd . concat ([ event_auc_at_t_df , event_specific_auc_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col )], axis = 1 ) return event_auc_at_t_df . T","title":"events_auc_at_t()"},{"location":"api/evaluation/#pydts.evaluation.events_brier_score_at_t","text":"This function implements the calculation of the events Brier score at t. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description event_brier_score_at_t_df (pd.DataFrame) events Brier score at t results. Source code in pydts/evaluation.py def events_brier_score_at_t ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> pd . DataFrame : \"\"\" This function implements the calculation of the events Brier score at t. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: event_brier_score_at_t_df (pd.DataFrame): events Brier score at t results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] event_brier_score_at_t_df = pd . DataFrame () for event in sorted ( events ): event_brier_score_at_t_df = pd . concat ([ event_brier_score_at_t_df , event_specific_brier_score_at_t_all ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col )], axis = 1 ) return event_brier_score_at_t_df . T","title":"events_brier_score_at_t()"},{"location":"api/evaluation/#pydts.evaluation.events_integrated_auc","text":"This function implements the calculation of the integrated AUC to all events. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description integrated_auc (dict) integrated AUC results. Source code in pydts/evaluation.py def events_integrated_auc ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> dict : \"\"\" This function implements the calculation of the integrated AUC to all events. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: integrated_auc (dict): integrated AUC results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] integrated_auc = {} for event in sorted ( events ): integrated_auc [ event ] = event_specific_integrated_auc ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return integrated_auc","title":"events_integrated_auc()"},{"location":"api/evaluation/#pydts.evaluation.events_integrated_brier_score","text":"This function implements the calculation of the integrated Brier Score to all events. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate integrated Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description integrated_brier_score (dict) integrated Brier Score results. Source code in pydts/evaluation.py def events_integrated_brier_score ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> dict : \"\"\" This function implements the calculation of the integrated Brier Score to all events. Args: pred_df (pd.DataFrame): Data to calculate integrated Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: integrated_brier_score (dict): integrated Brier Score results. \"\"\" events = [ e for e in pred_df [ event_type_col ] . unique () if e != 0 ] integrated_brier_score = {} for event in sorted ( events ): integrated_brier_score [ event ] = event_specific_integrated_brier_score ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return integrated_brier_score","title":"events_integrated_brier_score()"},{"location":"api/evaluation/#pydts.evaluation.global_auc","text":"This function implements the calculation of the global AUC. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description global_auc (float) global AUC results. Source code in pydts/evaluation.py def global_auc ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the global AUC. Args: pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: global_auc (float): global AUC results. \"\"\" e_j_ser = pred_df [ pred_df [ event_type_col ] != 0 ] . groupby ( 'J' ) . size () . sort_index () total_e = e_j_ser . sum () global_auc = 0 for event , e_j in e_j_ser . iteritems (): global_auc += ( e_j / total_e ) * event_specific_integrated_auc ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return global_auc","title":"global_auc()"},{"location":"api/evaluation/#pydts.evaluation.global_brier_score","text":"This function implements the calculation of the global Brier Score. Parameters: Name Type Description Default pred_df pd.DataFrame Data to calculate Brier score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() required duration_col str Last follow up time column name (must be a column in pred_df). 'X' event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' Returns: Type Description global_auc (float) global Brier Score results. Source code in pydts/evaluation.py def global_brier_score ( pred_df : pd . DataFrame , event_type_col : str = 'J' , duration_col : str = 'X' ) -> float : \"\"\" This function implements the calculation of the global Brier Score. Args: pred_df (pd.DataFrame): Data to calculate Brier score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events() duration_col (str): Last follow up time column name (must be a column in pred_df). event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. Returns: global_auc (float): global Brier Score results. \"\"\" e_j_ser = pred_df [ pred_df [ event_type_col ] != 0 ] . groupby ( 'J' ) . size () . sort_index () total_e = e_j_ser . sum () global_bs = 0 for event , e_j in e_j_ser . iteritems (): global_bs += ( e_j / total_e ) * event_specific_integrated_brier_score ( pred_df = pred_df , event = event , event_type_col = event_type_col , duration_col = duration_col ) return global_bs","title":"global_brier_score()"},{"location":"api/event_times_sampler/","text":"Source code in pydts/data_generation.py class EventTimesSampler ( object ): def __init__ ( self , d_times : int , j_event_types : int ): \"\"\" This class implements sampling procedure for discrete event times and censoring times for given observations. Args: d_times (int): number of possible event times j_event_types (int): number of possible event types \"\"\" self . d_times = d_times self . times = range ( 1 , self . d_times + 2 ) # d + 1 for administrative censoring self . j_event_types = j_event_types self . events = range ( 1 , self . j_event_types + 1 ) def _validate_prob_dfs_list ( self , dfs_list : list , numerical_error_tolerance : float = 0.001 ) -> list : for df in dfs_list : if ((( df < ( 0 - numerical_error_tolerance )) . any () . any ()) or (( df > ( 1 + numerical_error_tolerance )) . any () . any ())): raise ValueError ( \"The chosen sampling parameters result in invalid probabilities for event j at time t\" ) # Only fixes numerical errors smaller than the tolerance size df . clip ( 0 , 1 , inplace = True ) return dfs_list def calculate_hazards ( self , observations_df : pd . DataFrame , hazard_coefs : dict , events : list = None ) -> list : \"\"\" Calculates the hazard function for the observations given the hazard coefficients. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. coefficients (dict): time coefficients and covariates coefficients for each event type. Returns: hazards_dfs (list): A list of dataframes, one for each event type, with the hazard function at time t to each of the observations. \"\"\" events = events if events is not None else self . events a_t = {} for event in events : if callable ( hazard_coefs [ 'alpha' ][ event ]): a_t [ event ] = { t : hazard_coefs [ 'alpha' ][ event ]( t ) for t in range ( 1 , self . d_times + 1 )} else : a_t [ event ] = { t : hazard_coefs [ 'alpha' ][ event ][ t - 1 ] for t in range ( 1 , self . d_times + 1 )} b = pd . concat ([ observations_df . dot ( hazard_coefs [ 'beta' ][ j ]) for j in events ], axis = 1 , keys = events ) hazards_dfs = [ pd . concat ([ expit (( a_t [ j ][ t ] + b [ j ]) . astype ( float )) for t in range ( 1 , self . d_times + 1 )], axis = 1 , keys = ( range ( 1 , self . d_times + 1 ))) for j in events ] return hazards_dfs def calculate_overall_survival ( self , hazards : list , numerical_error_tolerance : float = 0.001 ) -> pd . DataFrame : \"\"\" Calculates the overall survival function given the hazards Args: hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: overall_survival (pd.Dataframe): The overall survival functions \"\"\" if ((( sum ( hazards )) > ( 1 + numerical_error_tolerance )) . sum () . sum () > 0 ): raise ValueError ( \"The chosen sampling parameters result in negative values of the overall survival function\" ) sum_hazards = sum ( hazards ) . clip ( 0 , 1 ) overall_survival = pd . concat ([ pd . Series ( 1 , index = hazards [ 0 ] . index ), ( 1 - sum_hazards ) . cumprod ( axis = 1 ) . iloc [:, : - 1 ]], axis = 1 ) overall_survival . columns += 1 return overall_survival def calculate_prob_event_at_t ( self , hazards : list , overall_survival : pd . DataFrame , numerical_error_tolerance : float = 0.001 ) -> list : \"\"\" Calculates the probability for event j at time t. Args: hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) overall_survival (pd.Dataframe): The overall survival functions numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: prob_event_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. \"\"\" prob_event_at_t = [ hazard * overall_survival for hazard in hazards ] prob_event_at_t = self . _validate_prob_dfs_list ( prob_event_at_t , numerical_error_tolerance ) return prob_event_at_t def calculate_prob_event_j ( self , prob_j_at_t : list , numerical_error_tolerance : float = 0.001 ) -> list : \"\"\" Calculates the total probability for event j. Args: prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. \"\"\" total_prob_j = [ prob . sum ( axis = 1 ) for prob in prob_j_at_t ] total_prob_j = self . _validate_prob_dfs_list ( total_prob_j , numerical_error_tolerance ) return total_prob_j def calc_prob_t_given_j ( self , prob_j_at_t , total_prob_j , numerical_error_tolerance = 0.001 ): \"\"\" Calculates the conditional probability for event occurrance at time t given J_i=j Args: prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: conditional_prob (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. \"\"\" conditional_prob = [ prob . div ( sumj , axis = 0 ) for prob , sumj in zip ( prob_j_at_t , total_prob_j )] conditional_prob = self . _validate_prob_dfs_list ( conditional_prob , numerical_error_tolerance ) return conditional_prob def sample_event_times ( self , observations_df : pd . DataFrame , hazard_coefs : dict , covariates : Union [ list , None ] = None , events : Union [ list , None ] = None , seed : Union [ int , None ] = None ) -> pd . DataFrame : \"\"\" Sample event type and event occurance times Args: observations_df (pd.DataFrame): Dataframe with observations covariates. covariates (list): list of covariates name, must be a subset of observations_df.columns coefficients (dict): time coefficients and covariates coefficients for each event type. seed (int, None): numpy seed number for pseudo random sampling. Returns: observations_df (pd.DataFrame): Dataframe with additional columns for sampled event time (T) and event type (J). \"\"\" np . random . seed ( seed ) if covariates is None : covariates = [ c for c in observations_df . columns if c not in [ 'X' , 'T' , 'C' , 'J' ]] events = events if events is not None else self . events cov_df = observations_df [ covariates ] hazards = self . calculate_hazards ( cov_df , hazard_coefs , events = events ) overall_survival = self . calculate_overall_survival ( hazards ) probs_j_at_t = self . calculate_prob_event_at_t ( hazards , overall_survival ) total_prob_j = self . calculate_prob_event_j ( probs_j_at_t ) probs_t_given_j = self . calc_prob_t_given_j ( probs_j_at_t , total_prob_j ) sampled_jt = self . sample_jt ( total_prob_j , probs_t_given_j ) if 'J' in observations_df . columns : observations_df . drop ( 'J' , axis = 1 , inplace = True ) if 'T' in observations_df . columns : observations_df . drop ( 'T' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_jt ], axis = 1 ) return observations_df def sample_jt ( self , total_prob_j : list , probs_t_given_j : list , numerical_error_tolerance : float = 0.001 ) -> pd . DataFrame : \"\"\" Sample event type and event time for each observation Args: total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. probs_t_given_j (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. Returns: sampled_df (pd.DataFrame): A dataframe with sampled event time and event type for each observation. \"\"\" total_prob_j = self . _validate_prob_dfs_list ( total_prob_j , numerical_error_tolerance ) probs_t_given_j = self . _validate_prob_dfs_list ( probs_t_given_j , numerical_error_tolerance ) # Add administrative censoring (no event occured until Tmax) probability as J=0 temp_sums = pd . concat ([ 1 - sum ( total_prob_j ), * total_prob_j ], axis = 1 , keys = [ 0 , * self . events ]) if ((( temp_sums < ( 0 - numerical_error_tolerance )) . any () . any ()) or \\ (( temp_sums > ( 1 + numerical_error_tolerance )) . any () . any ())): raise ValueError ( \"The chosen sampling parameters result in invalid probabilities\" ) # Only fixes numerical errors smaller than the tolerance size temp_sums . clip ( 0 , 1 , inplace = True ) # Efficient way to sample j for each observation with different event probabilities sampled_df = ( temp_sums . cumsum ( 1 ) > np . random . rand ( temp_sums . shape [ 0 ])[:, None ]) . idxmax ( axis = 1 ) . to_frame ( 'J' ) temp_ts = [] for j in self . events : # Get the index of the observations with J_i = j rel_j = sampled_df . query ( \"J==@j\" ) . index # Get probs dataframe from the dfs list prob_df = probs_t_given_j [ j - 1 ] # the prob j to sample from # Sample time of occurrence given J_i = j temp_ts . append (( prob_df . loc [ rel_j ] . cumsum ( 1 ) >= np . random . rand ( rel_j . shape [ 0 ])[:, None ]) . idxmax ( axis = 1 )) # Add Tmax+1 for observations with J_i = 0 temp_ts . append ( pd . Series ( self . d_times + 1 , index = sampled_df . query ( 'J==0' ) . index )) sampled_df [ \"T\" ] = pd . concat ( temp_ts ) . sort_index () return sampled_df def sample_independent_lof_censoring ( self , observations_df : pd . DataFrame , prob_lof_at_t : np . array , seed : Union [ int , None ] = None ) -> pd . DataFrame : \"\"\" Samples loss of follow-up censoring time from probabilities independent of covariates. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. prob_lof_at_t (np.array): Array of probabilities for sampling each of the possible times. seed (int): pseudo random seed number for numpy.random.seed() Returns: observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time. \"\"\" np . random . seed ( seed ) administrative_censoring_prob = ( 1 - sum ( prob_lof_at_t )) assert ( administrative_censoring_prob >= 0 ), \"Check the sum of prob_lof_at_t argument.\" assert ( administrative_censoring_prob <= 1 ), \"Check the sum of prob_lof_at_t argument.\" prob_lof_at_t = np . append ( prob_lof_at_t , administrative_censoring_prob ) sampled_df = pd . DataFrame ( np . random . choice ( a = self . times , size = len ( observations_df ), p = prob_lof_at_t ), index = observations_df . index , columns = [ 'C' ]) # No follow-up censoring, C=d+2 such that T wins when building X column: #sampled_df.loc[sampled_df['C'] == self.times[-1], 'C'] = self.d_times + 2 if 'C' in observations_df . columns : observations_df . drop ( 'C' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_df ], axis = 1 ) return observations_df def sample_hazard_lof_censoring ( self , observations_df : pd . DataFrame , censoring_hazard_coefs : dict , seed : Union [ int , None ] = None , covariates : Union [ list , None ] = None ) -> pd . DataFrame : \"\"\" Samples loss of follow-up censoring time from hazard coefficients. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. censoring_hazard_coefs (dict): time coefficients and covariates coefficients for the censoring hazard. seed (int): pseudo random seed number for numpy.random.seed() covariates (list): list of covariates names, must be a subset of observations_df.columns Returns: observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time. \"\"\" if covariates is None : covariates = [ c for c in observations_df . columns if c not in [ 'X' , 'T' , 'C' , 'J' ]] cov_df = observations_df [ covariates ] tmp_ets = EventTimesSampler ( d_times = self . d_times , j_event_types = 1 ) sampled_df = tmp_ets . sample_event_times ( cov_df , censoring_hazard_coefs , seed = seed , covariates = covariates , events = [ 0 ]) # No follow-up censoring, C=d+2 such that T wins when building X column: #sampled_df.loc[sampled_df['J'] == 0, 'T'] = self.d_times + 2 sampled_df = sampled_df [[ 'T' ]] sampled_df . columns = [ 'C' ] if 'C' in observations_df . columns : observations_df . drop ( 'C' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_df ], axis = 1 ) return observations_df def update_event_or_lof ( self , observations_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'. Event type 'J' will be changed to 0 for observation with 'C' < 'T'. Args: observations_df (pd.DataFrame): Dataframe with observations after sampling event times 'T' and censoring time 'C'. Returns: observations_df (pd.DataFrame): Dataframe with updated time column 'X' and event type column 'J' \"\"\" assert ( 'T' in observations_df . columns ), \"Trying to update event or censoring before sampling event times\" assert ( 'C' in observations_df . columns ), \"Trying to update event or censoring before sampling censoring time\" observations_df [ 'X' ] = observations_df [[ 'T' , 'C' ]] . min ( axis = 1 ) observations_df . loc [ observations_df . loc [( observations_df [ 'C' ] < observations_df [ 'T' ])] . index , 'J' ] = 0 return observations_df __init__ ( self , d_times , j_event_types ) special \u00a4 This class implements sampling procedure for discrete event times and censoring times for given observations. Parameters: Name Type Description Default d_times int number of possible event times required j_event_types int number of possible event types required Source code in pydts/data_generation.py def __init__ ( self , d_times : int , j_event_types : int ): \"\"\" This class implements sampling procedure for discrete event times and censoring times for given observations. Args: d_times (int): number of possible event times j_event_types (int): number of possible event types \"\"\" self . d_times = d_times self . times = range ( 1 , self . d_times + 2 ) # d + 1 for administrative censoring self . j_event_types = j_event_types self . events = range ( 1 , self . j_event_types + 1 ) calc_prob_t_given_j ( self , prob_j_at_t , total_prob_j , numerical_error_tolerance = 0.001 ) \u00a4 Calculates the conditional probability for event occurrance at time t given J_i=j Parameters: Name Type Description Default prob_j_at_t list A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. required total_prob_j list A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description conditional_prob (list) A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. Source code in pydts/data_generation.py def calc_prob_t_given_j ( self , prob_j_at_t , total_prob_j , numerical_error_tolerance = 0.001 ): \"\"\" Calculates the conditional probability for event occurrance at time t given J_i=j Args: prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: conditional_prob (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. \"\"\" conditional_prob = [ prob . div ( sumj , axis = 0 ) for prob , sumj in zip ( prob_j_at_t , total_prob_j )] conditional_prob = self . _validate_prob_dfs_list ( conditional_prob , numerical_error_tolerance ) return conditional_prob calculate_hazards ( self , observations_df , hazard_coefs , events = None ) \u00a4 Calculates the hazard function for the observations given the hazard coefficients. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required coefficients dict time coefficients and covariates coefficients for each event type. required Returns: Type Description hazards_dfs (list) A list of dataframes, one for each event type, with the hazard function at time t to each of the observations. Source code in pydts/data_generation.py def calculate_hazards ( self , observations_df : pd . DataFrame , hazard_coefs : dict , events : list = None ) -> list : \"\"\" Calculates the hazard function for the observations given the hazard coefficients. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. coefficients (dict): time coefficients and covariates coefficients for each event type. Returns: hazards_dfs (list): A list of dataframes, one for each event type, with the hazard function at time t to each of the observations. \"\"\" events = events if events is not None else self . events a_t = {} for event in events : if callable ( hazard_coefs [ 'alpha' ][ event ]): a_t [ event ] = { t : hazard_coefs [ 'alpha' ][ event ]( t ) for t in range ( 1 , self . d_times + 1 )} else : a_t [ event ] = { t : hazard_coefs [ 'alpha' ][ event ][ t - 1 ] for t in range ( 1 , self . d_times + 1 )} b = pd . concat ([ observations_df . dot ( hazard_coefs [ 'beta' ][ j ]) for j in events ], axis = 1 , keys = events ) hazards_dfs = [ pd . concat ([ expit (( a_t [ j ][ t ] + b [ j ]) . astype ( float )) for t in range ( 1 , self . d_times + 1 )], axis = 1 , keys = ( range ( 1 , self . d_times + 1 ))) for j in events ] return hazards_dfs calculate_overall_survival ( self , hazards , numerical_error_tolerance = 0.001 ) \u00a4 Calculates the overall survival function given the hazards Parameters: Name Type Description Default hazards list A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description overall_survival (pd.Dataframe) The overall survival functions Source code in pydts/data_generation.py def calculate_overall_survival ( self , hazards : list , numerical_error_tolerance : float = 0.001 ) -> pd . DataFrame : \"\"\" Calculates the overall survival function given the hazards Args: hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: overall_survival (pd.Dataframe): The overall survival functions \"\"\" if ((( sum ( hazards )) > ( 1 + numerical_error_tolerance )) . sum () . sum () > 0 ): raise ValueError ( \"The chosen sampling parameters result in negative values of the overall survival function\" ) sum_hazards = sum ( hazards ) . clip ( 0 , 1 ) overall_survival = pd . concat ([ pd . Series ( 1 , index = hazards [ 0 ] . index ), ( 1 - sum_hazards ) . cumprod ( axis = 1 ) . iloc [:, : - 1 ]], axis = 1 ) overall_survival . columns += 1 return overall_survival calculate_prob_event_at_t ( self , hazards , overall_survival , numerical_error_tolerance = 0.001 ) \u00a4 Calculates the probability for event j at time t. Parameters: Name Type Description Default hazards list A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) required overall_survival pd.Dataframe The overall survival functions required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description prob_event_at_t (list) A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. Source code in pydts/data_generation.py def calculate_prob_event_at_t ( self , hazards : list , overall_survival : pd . DataFrame , numerical_error_tolerance : float = 0.001 ) -> list : \"\"\" Calculates the probability for event j at time t. Args: hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) overall_survival (pd.Dataframe): The overall survival functions numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: prob_event_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. \"\"\" prob_event_at_t = [ hazard * overall_survival for hazard in hazards ] prob_event_at_t = self . _validate_prob_dfs_list ( prob_event_at_t , numerical_error_tolerance ) return prob_event_at_t calculate_prob_event_j ( self , prob_j_at_t , numerical_error_tolerance = 0.001 ) \u00a4 Calculates the total probability for event j. Parameters: Name Type Description Default prob_j_at_t list A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description total_prob_j (list) A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. Source code in pydts/data_generation.py def calculate_prob_event_j ( self , prob_j_at_t : list , numerical_error_tolerance : float = 0.001 ) -> list : \"\"\" Calculates the total probability for event j. Args: prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. \"\"\" total_prob_j = [ prob . sum ( axis = 1 ) for prob in prob_j_at_t ] total_prob_j = self . _validate_prob_dfs_list ( total_prob_j , numerical_error_tolerance ) return total_prob_j sample_event_times ( self , observations_df , hazard_coefs , covariates = None , events = None , seed = None ) \u00a4 Sample event type and event occurance times Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required covariates list list of covariates name, must be a subset of observations_df.columns None coefficients dict time coefficients and covariates coefficients for each event type. required seed int, None numpy seed number for pseudo random sampling. None Returns: Type Description observations_df (pd.DataFrame) Dataframe with additional columns for sampled event time (T) and event type (J). Source code in pydts/data_generation.py def sample_event_times ( self , observations_df : pd . DataFrame , hazard_coefs : dict , covariates : Union [ list , None ] = None , events : Union [ list , None ] = None , seed : Union [ int , None ] = None ) -> pd . DataFrame : \"\"\" Sample event type and event occurance times Args: observations_df (pd.DataFrame): Dataframe with observations covariates. covariates (list): list of covariates name, must be a subset of observations_df.columns coefficients (dict): time coefficients and covariates coefficients for each event type. seed (int, None): numpy seed number for pseudo random sampling. Returns: observations_df (pd.DataFrame): Dataframe with additional columns for sampled event time (T) and event type (J). \"\"\" np . random . seed ( seed ) if covariates is None : covariates = [ c for c in observations_df . columns if c not in [ 'X' , 'T' , 'C' , 'J' ]] events = events if events is not None else self . events cov_df = observations_df [ covariates ] hazards = self . calculate_hazards ( cov_df , hazard_coefs , events = events ) overall_survival = self . calculate_overall_survival ( hazards ) probs_j_at_t = self . calculate_prob_event_at_t ( hazards , overall_survival ) total_prob_j = self . calculate_prob_event_j ( probs_j_at_t ) probs_t_given_j = self . calc_prob_t_given_j ( probs_j_at_t , total_prob_j ) sampled_jt = self . sample_jt ( total_prob_j , probs_t_given_j ) if 'J' in observations_df . columns : observations_df . drop ( 'J' , axis = 1 , inplace = True ) if 'T' in observations_df . columns : observations_df . drop ( 'T' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_jt ], axis = 1 ) return observations_df sample_hazard_lof_censoring ( self , observations_df , censoring_hazard_coefs , seed = None , covariates = None ) \u00a4 Samples loss of follow-up censoring time from hazard coefficients. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required censoring_hazard_coefs dict time coefficients and covariates coefficients for the censoring hazard. required seed int pseudo random seed number for numpy.random.seed() None covariates list list of covariates names, must be a subset of observations_df.columns None Returns: Type Description observations_df (pd.DataFrame) Upadted dataframe including sampled censoring time. Source code in pydts/data_generation.py def sample_hazard_lof_censoring ( self , observations_df : pd . DataFrame , censoring_hazard_coefs : dict , seed : Union [ int , None ] = None , covariates : Union [ list , None ] = None ) -> pd . DataFrame : \"\"\" Samples loss of follow-up censoring time from hazard coefficients. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. censoring_hazard_coefs (dict): time coefficients and covariates coefficients for the censoring hazard. seed (int): pseudo random seed number for numpy.random.seed() covariates (list): list of covariates names, must be a subset of observations_df.columns Returns: observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time. \"\"\" if covariates is None : covariates = [ c for c in observations_df . columns if c not in [ 'X' , 'T' , 'C' , 'J' ]] cov_df = observations_df [ covariates ] tmp_ets = EventTimesSampler ( d_times = self . d_times , j_event_types = 1 ) sampled_df = tmp_ets . sample_event_times ( cov_df , censoring_hazard_coefs , seed = seed , covariates = covariates , events = [ 0 ]) # No follow-up censoring, C=d+2 such that T wins when building X column: #sampled_df.loc[sampled_df['J'] == 0, 'T'] = self.d_times + 2 sampled_df = sampled_df [[ 'T' ]] sampled_df . columns = [ 'C' ] if 'C' in observations_df . columns : observations_df . drop ( 'C' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_df ], axis = 1 ) return observations_df sample_independent_lof_censoring ( self , observations_df , prob_lof_at_t , seed = None ) \u00a4 Samples loss of follow-up censoring time from probabilities independent of covariates. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required prob_lof_at_t np.array Array of probabilities for sampling each of the possible times. required seed int pseudo random seed number for numpy.random.seed() None Returns: Type Description observations_df (pd.DataFrame) Upadted dataframe including sampled censoring time. Source code in pydts/data_generation.py def sample_independent_lof_censoring ( self , observations_df : pd . DataFrame , prob_lof_at_t : np . array , seed : Union [ int , None ] = None ) -> pd . DataFrame : \"\"\" Samples loss of follow-up censoring time from probabilities independent of covariates. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. prob_lof_at_t (np.array): Array of probabilities for sampling each of the possible times. seed (int): pseudo random seed number for numpy.random.seed() Returns: observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time. \"\"\" np . random . seed ( seed ) administrative_censoring_prob = ( 1 - sum ( prob_lof_at_t )) assert ( administrative_censoring_prob >= 0 ), \"Check the sum of prob_lof_at_t argument.\" assert ( administrative_censoring_prob <= 1 ), \"Check the sum of prob_lof_at_t argument.\" prob_lof_at_t = np . append ( prob_lof_at_t , administrative_censoring_prob ) sampled_df = pd . DataFrame ( np . random . choice ( a = self . times , size = len ( observations_df ), p = prob_lof_at_t ), index = observations_df . index , columns = [ 'C' ]) # No follow-up censoring, C=d+2 such that T wins when building X column: #sampled_df.loc[sampled_df['C'] == self.times[-1], 'C'] = self.d_times + 2 if 'C' in observations_df . columns : observations_df . drop ( 'C' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_df ], axis = 1 ) return observations_df sample_jt ( self , total_prob_j , probs_t_given_j , numerical_error_tolerance = 0.001 ) \u00a4 Sample event type and event time for each observation Parameters: Name Type Description Default total_prob_j list A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. required probs_t_given_j list A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. required Returns: Type Description sampled_df (pd.DataFrame) A dataframe with sampled event time and event type for each observation. Source code in pydts/data_generation.py def sample_jt ( self , total_prob_j : list , probs_t_given_j : list , numerical_error_tolerance : float = 0.001 ) -> pd . DataFrame : \"\"\" Sample event type and event time for each observation Args: total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. probs_t_given_j (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. Returns: sampled_df (pd.DataFrame): A dataframe with sampled event time and event type for each observation. \"\"\" total_prob_j = self . _validate_prob_dfs_list ( total_prob_j , numerical_error_tolerance ) probs_t_given_j = self . _validate_prob_dfs_list ( probs_t_given_j , numerical_error_tolerance ) # Add administrative censoring (no event occured until Tmax) probability as J=0 temp_sums = pd . concat ([ 1 - sum ( total_prob_j ), * total_prob_j ], axis = 1 , keys = [ 0 , * self . events ]) if ((( temp_sums < ( 0 - numerical_error_tolerance )) . any () . any ()) or \\ (( temp_sums > ( 1 + numerical_error_tolerance )) . any () . any ())): raise ValueError ( \"The chosen sampling parameters result in invalid probabilities\" ) # Only fixes numerical errors smaller than the tolerance size temp_sums . clip ( 0 , 1 , inplace = True ) # Efficient way to sample j for each observation with different event probabilities sampled_df = ( temp_sums . cumsum ( 1 ) > np . random . rand ( temp_sums . shape [ 0 ])[:, None ]) . idxmax ( axis = 1 ) . to_frame ( 'J' ) temp_ts = [] for j in self . events : # Get the index of the observations with J_i = j rel_j = sampled_df . query ( \"J==@j\" ) . index # Get probs dataframe from the dfs list prob_df = probs_t_given_j [ j - 1 ] # the prob j to sample from # Sample time of occurrence given J_i = j temp_ts . append (( prob_df . loc [ rel_j ] . cumsum ( 1 ) >= np . random . rand ( rel_j . shape [ 0 ])[:, None ]) . idxmax ( axis = 1 )) # Add Tmax+1 for observations with J_i = 0 temp_ts . append ( pd . Series ( self . d_times + 1 , index = sampled_df . query ( 'J==0' ) . index )) sampled_df [ \"T\" ] = pd . concat ( temp_ts ) . sort_index () return sampled_df update_event_or_lof ( self , observations_df ) \u00a4 Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'. Event type 'J' will be changed to 0 for observation with 'C' < 'T'. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations after sampling event times 'T' and censoring time 'C'. required Returns: Type Description observations_df (pd.DataFrame) Dataframe with updated time column 'X' and event type column 'J' Source code in pydts/data_generation.py def update_event_or_lof ( self , observations_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'. Event type 'J' will be changed to 0 for observation with 'C' < 'T'. Args: observations_df (pd.DataFrame): Dataframe with observations after sampling event times 'T' and censoring time 'C'. Returns: observations_df (pd.DataFrame): Dataframe with updated time column 'X' and event type column 'J' \"\"\" assert ( 'T' in observations_df . columns ), \"Trying to update event or censoring before sampling event times\" assert ( 'C' in observations_df . columns ), \"Trying to update event or censoring before sampling censoring time\" observations_df [ 'X' ] = observations_df [[ 'T' , 'C' ]] . min ( axis = 1 ) observations_df . loc [ observations_df . loc [( observations_df [ 'C' ] < observations_df [ 'T' ])] . index , 'J' ] = 0 return observations_df","title":"Event Times Sampler"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.__init__","text":"This class implements sampling procedure for discrete event times and censoring times for given observations. Parameters: Name Type Description Default d_times int number of possible event times required j_event_types int number of possible event types required Source code in pydts/data_generation.py def __init__ ( self , d_times : int , j_event_types : int ): \"\"\" This class implements sampling procedure for discrete event times and censoring times for given observations. Args: d_times (int): number of possible event times j_event_types (int): number of possible event types \"\"\" self . d_times = d_times self . times = range ( 1 , self . d_times + 2 ) # d + 1 for administrative censoring self . j_event_types = j_event_types self . events = range ( 1 , self . j_event_types + 1 )","title":"__init__()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calc_prob_t_given_j","text":"Calculates the conditional probability for event occurrance at time t given J_i=j Parameters: Name Type Description Default prob_j_at_t list A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. required total_prob_j list A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description conditional_prob (list) A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. Source code in pydts/data_generation.py def calc_prob_t_given_j ( self , prob_j_at_t , total_prob_j , numerical_error_tolerance = 0.001 ): \"\"\" Calculates the conditional probability for event occurrance at time t given J_i=j Args: prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: conditional_prob (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. \"\"\" conditional_prob = [ prob . div ( sumj , axis = 0 ) for prob , sumj in zip ( prob_j_at_t , total_prob_j )] conditional_prob = self . _validate_prob_dfs_list ( conditional_prob , numerical_error_tolerance ) return conditional_prob","title":"calc_prob_t_given_j()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_hazards","text":"Calculates the hazard function for the observations given the hazard coefficients. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required coefficients dict time coefficients and covariates coefficients for each event type. required Returns: Type Description hazards_dfs (list) A list of dataframes, one for each event type, with the hazard function at time t to each of the observations. Source code in pydts/data_generation.py def calculate_hazards ( self , observations_df : pd . DataFrame , hazard_coefs : dict , events : list = None ) -> list : \"\"\" Calculates the hazard function for the observations given the hazard coefficients. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. coefficients (dict): time coefficients and covariates coefficients for each event type. Returns: hazards_dfs (list): A list of dataframes, one for each event type, with the hazard function at time t to each of the observations. \"\"\" events = events if events is not None else self . events a_t = {} for event in events : if callable ( hazard_coefs [ 'alpha' ][ event ]): a_t [ event ] = { t : hazard_coefs [ 'alpha' ][ event ]( t ) for t in range ( 1 , self . d_times + 1 )} else : a_t [ event ] = { t : hazard_coefs [ 'alpha' ][ event ][ t - 1 ] for t in range ( 1 , self . d_times + 1 )} b = pd . concat ([ observations_df . dot ( hazard_coefs [ 'beta' ][ j ]) for j in events ], axis = 1 , keys = events ) hazards_dfs = [ pd . concat ([ expit (( a_t [ j ][ t ] + b [ j ]) . astype ( float )) for t in range ( 1 , self . d_times + 1 )], axis = 1 , keys = ( range ( 1 , self . d_times + 1 ))) for j in events ] return hazards_dfs","title":"calculate_hazards()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_overall_survival","text":"Calculates the overall survival function given the hazards Parameters: Name Type Description Default hazards list A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description overall_survival (pd.Dataframe) The overall survival functions Source code in pydts/data_generation.py def calculate_overall_survival ( self , hazards : list , numerical_error_tolerance : float = 0.001 ) -> pd . DataFrame : \"\"\" Calculates the overall survival function given the hazards Args: hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: overall_survival (pd.Dataframe): The overall survival functions \"\"\" if ((( sum ( hazards )) > ( 1 + numerical_error_tolerance )) . sum () . sum () > 0 ): raise ValueError ( \"The chosen sampling parameters result in negative values of the overall survival function\" ) sum_hazards = sum ( hazards ) . clip ( 0 , 1 ) overall_survival = pd . concat ([ pd . Series ( 1 , index = hazards [ 0 ] . index ), ( 1 - sum_hazards ) . cumprod ( axis = 1 ) . iloc [:, : - 1 ]], axis = 1 ) overall_survival . columns += 1 return overall_survival","title":"calculate_overall_survival()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_prob_event_at_t","text":"Calculates the probability for event j at time t. Parameters: Name Type Description Default hazards list A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) required overall_survival pd.Dataframe The overall survival functions required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description prob_event_at_t (list) A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. Source code in pydts/data_generation.py def calculate_prob_event_at_t ( self , hazards : list , overall_survival : pd . DataFrame , numerical_error_tolerance : float = 0.001 ) -> list : \"\"\" Calculates the probability for event j at time t. Args: hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function) overall_survival (pd.Dataframe): The overall survival functions numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: prob_event_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. \"\"\" prob_event_at_t = [ hazard * overall_survival for hazard in hazards ] prob_event_at_t = self . _validate_prob_dfs_list ( prob_event_at_t , numerical_error_tolerance ) return prob_event_at_t","title":"calculate_prob_event_at_t()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_prob_event_j","text":"Calculates the total probability for event j. Parameters: Name Type Description Default prob_j_at_t list A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. required numerical_error_tolerance float Tolerate numerical errors of probabilities up to this value. 0.001 Returns: Type Description total_prob_j (list) A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. Source code in pydts/data_generation.py def calculate_prob_event_j ( self , prob_j_at_t : list , numerical_error_tolerance : float = 0.001 ) -> list : \"\"\" Calculates the total probability for event j. Args: prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations. numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value. Returns: total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. \"\"\" total_prob_j = [ prob . sum ( axis = 1 ) for prob in prob_j_at_t ] total_prob_j = self . _validate_prob_dfs_list ( total_prob_j , numerical_error_tolerance ) return total_prob_j","title":"calculate_prob_event_j()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_event_times","text":"Sample event type and event occurance times Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required covariates list list of covariates name, must be a subset of observations_df.columns None coefficients dict time coefficients and covariates coefficients for each event type. required seed int, None numpy seed number for pseudo random sampling. None Returns: Type Description observations_df (pd.DataFrame) Dataframe with additional columns for sampled event time (T) and event type (J). Source code in pydts/data_generation.py def sample_event_times ( self , observations_df : pd . DataFrame , hazard_coefs : dict , covariates : Union [ list , None ] = None , events : Union [ list , None ] = None , seed : Union [ int , None ] = None ) -> pd . DataFrame : \"\"\" Sample event type and event occurance times Args: observations_df (pd.DataFrame): Dataframe with observations covariates. covariates (list): list of covariates name, must be a subset of observations_df.columns coefficients (dict): time coefficients and covariates coefficients for each event type. seed (int, None): numpy seed number for pseudo random sampling. Returns: observations_df (pd.DataFrame): Dataframe with additional columns for sampled event time (T) and event type (J). \"\"\" np . random . seed ( seed ) if covariates is None : covariates = [ c for c in observations_df . columns if c not in [ 'X' , 'T' , 'C' , 'J' ]] events = events if events is not None else self . events cov_df = observations_df [ covariates ] hazards = self . calculate_hazards ( cov_df , hazard_coefs , events = events ) overall_survival = self . calculate_overall_survival ( hazards ) probs_j_at_t = self . calculate_prob_event_at_t ( hazards , overall_survival ) total_prob_j = self . calculate_prob_event_j ( probs_j_at_t ) probs_t_given_j = self . calc_prob_t_given_j ( probs_j_at_t , total_prob_j ) sampled_jt = self . sample_jt ( total_prob_j , probs_t_given_j ) if 'J' in observations_df . columns : observations_df . drop ( 'J' , axis = 1 , inplace = True ) if 'T' in observations_df . columns : observations_df . drop ( 'T' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_jt ], axis = 1 ) return observations_df","title":"sample_event_times()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_hazard_lof_censoring","text":"Samples loss of follow-up censoring time from hazard coefficients. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required censoring_hazard_coefs dict time coefficients and covariates coefficients for the censoring hazard. required seed int pseudo random seed number for numpy.random.seed() None covariates list list of covariates names, must be a subset of observations_df.columns None Returns: Type Description observations_df (pd.DataFrame) Upadted dataframe including sampled censoring time. Source code in pydts/data_generation.py def sample_hazard_lof_censoring ( self , observations_df : pd . DataFrame , censoring_hazard_coefs : dict , seed : Union [ int , None ] = None , covariates : Union [ list , None ] = None ) -> pd . DataFrame : \"\"\" Samples loss of follow-up censoring time from hazard coefficients. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. censoring_hazard_coefs (dict): time coefficients and covariates coefficients for the censoring hazard. seed (int): pseudo random seed number for numpy.random.seed() covariates (list): list of covariates names, must be a subset of observations_df.columns Returns: observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time. \"\"\" if covariates is None : covariates = [ c for c in observations_df . columns if c not in [ 'X' , 'T' , 'C' , 'J' ]] cov_df = observations_df [ covariates ] tmp_ets = EventTimesSampler ( d_times = self . d_times , j_event_types = 1 ) sampled_df = tmp_ets . sample_event_times ( cov_df , censoring_hazard_coefs , seed = seed , covariates = covariates , events = [ 0 ]) # No follow-up censoring, C=d+2 such that T wins when building X column: #sampled_df.loc[sampled_df['J'] == 0, 'T'] = self.d_times + 2 sampled_df = sampled_df [[ 'T' ]] sampled_df . columns = [ 'C' ] if 'C' in observations_df . columns : observations_df . drop ( 'C' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_df ], axis = 1 ) return observations_df","title":"sample_hazard_lof_censoring()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_independent_lof_censoring","text":"Samples loss of follow-up censoring time from probabilities independent of covariates. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations covariates. required prob_lof_at_t np.array Array of probabilities for sampling each of the possible times. required seed int pseudo random seed number for numpy.random.seed() None Returns: Type Description observations_df (pd.DataFrame) Upadted dataframe including sampled censoring time. Source code in pydts/data_generation.py def sample_independent_lof_censoring ( self , observations_df : pd . DataFrame , prob_lof_at_t : np . array , seed : Union [ int , None ] = None ) -> pd . DataFrame : \"\"\" Samples loss of follow-up censoring time from probabilities independent of covariates. Args: observations_df (pd.DataFrame): Dataframe with observations covariates. prob_lof_at_t (np.array): Array of probabilities for sampling each of the possible times. seed (int): pseudo random seed number for numpy.random.seed() Returns: observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time. \"\"\" np . random . seed ( seed ) administrative_censoring_prob = ( 1 - sum ( prob_lof_at_t )) assert ( administrative_censoring_prob >= 0 ), \"Check the sum of prob_lof_at_t argument.\" assert ( administrative_censoring_prob <= 1 ), \"Check the sum of prob_lof_at_t argument.\" prob_lof_at_t = np . append ( prob_lof_at_t , administrative_censoring_prob ) sampled_df = pd . DataFrame ( np . random . choice ( a = self . times , size = len ( observations_df ), p = prob_lof_at_t ), index = observations_df . index , columns = [ 'C' ]) # No follow-up censoring, C=d+2 such that T wins when building X column: #sampled_df.loc[sampled_df['C'] == self.times[-1], 'C'] = self.d_times + 2 if 'C' in observations_df . columns : observations_df . drop ( 'C' , axis = 1 , inplace = True ) observations_df = pd . concat ([ observations_df , sampled_df ], axis = 1 ) return observations_df","title":"sample_independent_lof_censoring()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_jt","text":"Sample event type and event time for each observation Parameters: Name Type Description Default total_prob_j list A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. required probs_t_given_j list A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. required Returns: Type Description sampled_df (pd.DataFrame) A dataframe with sampled event time and event type for each observation. Source code in pydts/data_generation.py def sample_jt ( self , total_prob_j : list , probs_t_given_j : list , numerical_error_tolerance : float = 0.001 ) -> pd . DataFrame : \"\"\" Sample event type and event time for each observation Args: total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations. probs_t_given_j (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations. Returns: sampled_df (pd.DataFrame): A dataframe with sampled event time and event type for each observation. \"\"\" total_prob_j = self . _validate_prob_dfs_list ( total_prob_j , numerical_error_tolerance ) probs_t_given_j = self . _validate_prob_dfs_list ( probs_t_given_j , numerical_error_tolerance ) # Add administrative censoring (no event occured until Tmax) probability as J=0 temp_sums = pd . concat ([ 1 - sum ( total_prob_j ), * total_prob_j ], axis = 1 , keys = [ 0 , * self . events ]) if ((( temp_sums < ( 0 - numerical_error_tolerance )) . any () . any ()) or \\ (( temp_sums > ( 1 + numerical_error_tolerance )) . any () . any ())): raise ValueError ( \"The chosen sampling parameters result in invalid probabilities\" ) # Only fixes numerical errors smaller than the tolerance size temp_sums . clip ( 0 , 1 , inplace = True ) # Efficient way to sample j for each observation with different event probabilities sampled_df = ( temp_sums . cumsum ( 1 ) > np . random . rand ( temp_sums . shape [ 0 ])[:, None ]) . idxmax ( axis = 1 ) . to_frame ( 'J' ) temp_ts = [] for j in self . events : # Get the index of the observations with J_i = j rel_j = sampled_df . query ( \"J==@j\" ) . index # Get probs dataframe from the dfs list prob_df = probs_t_given_j [ j - 1 ] # the prob j to sample from # Sample time of occurrence given J_i = j temp_ts . append (( prob_df . loc [ rel_j ] . cumsum ( 1 ) >= np . random . rand ( rel_j . shape [ 0 ])[:, None ]) . idxmax ( axis = 1 )) # Add Tmax+1 for observations with J_i = 0 temp_ts . append ( pd . Series ( self . d_times + 1 , index = sampled_df . query ( 'J==0' ) . index )) sampled_df [ \"T\" ] = pd . concat ( temp_ts ) . sort_index () return sampled_df","title":"sample_jt()"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.update_event_or_lof","text":"Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'. Event type 'J' will be changed to 0 for observation with 'C' < 'T'. Parameters: Name Type Description Default observations_df pd.DataFrame Dataframe with observations after sampling event times 'T' and censoring time 'C'. required Returns: Type Description observations_df (pd.DataFrame) Dataframe with updated time column 'X' and event type column 'J' Source code in pydts/data_generation.py def update_event_or_lof ( self , observations_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'. Event type 'J' will be changed to 0 for observation with 'C' < 'T'. Args: observations_df (pd.DataFrame): Dataframe with observations after sampling event times 'T' and censoring time 'C'. Returns: observations_df (pd.DataFrame): Dataframe with updated time column 'X' and event type column 'J' \"\"\" assert ( 'T' in observations_df . columns ), \"Trying to update event or censoring before sampling event times\" assert ( 'C' in observations_df . columns ), \"Trying to update event or censoring before sampling censoring time\" observations_df [ 'X' ] = observations_df [[ 'T' , 'C' ]] . min ( axis = 1 ) observations_df . loc [ observations_df . loc [( observations_df [ 'C' ] < observations_df [ 'T' ])] . index , 'J' ] = 0 return observations_df","title":"update_event_or_lof()"},{"location":"api/model_selection/","text":"BasePenaltyGridSearch \u00a4 This class implements the penalty parameter grid search. Source code in pydts/model_selection.py class BasePenaltyGridSearch ( object ): \"\"\" This class implements the penalty parameter grid search. \"\"\" def __init__ ( self ): self . l1_ratio = None self . penalizers = [] self . seed = None self . meta_models = {} self . train_df = None self . test_df = None self . global_auc = {} self . integrated_auc = {} self . global_bs = {} self . integrated_bs = {} self . TwoStagesFitter_type = 'CoxPHFitter' def evaluate ( self , train_df : pd . DataFrame , test_df : pd . DataFrame , l1_ratio : float , penalizers : list , metrics : Union [ list , str ] = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ], seed : Union [ None , int ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = {}) -> tuple : \"\"\" This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers. Args: train_df (pd.DataFrame): training data for fitting the model. test_df (pd.DataFrame): testing data for evaluating the estimated model's performance. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearch.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearch.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearch.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearch.global_bs). seed (int): pseudo random seed number for numpy.random.seed() event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). twostages_fit_kwargs (dict): keyword arguments to pass to the TwoStagesFitter. Returns: output (Tuple): Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics. \"\"\" self . l1_ratio = l1_ratio self . penalizers = penalizers self . seed = seed np . random . seed ( seed ) for idp , penalizer in enumerate ( penalizers ): fit_beta_kwargs = self . _get_model_fit_kwargs ( penalizer , l1_ratio ) if self . TwoStagesFitter_type == 'Exact' : self . meta_models [ penalizer ] = TwoStagesFitterExact () else : self . meta_models [ penalizer ] = TwoStagesFitter () print ( f \"Started estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } )\" ) start = time () self . meta_models [ penalizer ] . fit ( df = train_df , fit_beta_kwargs = fit_beta_kwargs , pid_col = pid_col , event_type_col = event_type_col , duration_col = duration_col , ** twostages_fit_kwargs ) end = time () print ( f \"Finished estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } ), { int ( end - start ) } seconds\" ) events = [ j for j in sorted ( train_df [ event_type_col ] . unique ()) if j != 0 ] grid = [ penalizers for e in events ] penalizers_combinations = list ( product ( * grid )) for idc , combination in enumerate ( penalizers_combinations ): mixed_two_stages = self . get_mixed_two_stages_fitter ( combination ) pred_df = mixed_two_stages . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ combination ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ combination ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ combination ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ combination ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) output = self . convert_results_dict_to_df ( self . global_auc ) . idxmax () . values [ 0 ] if 'GAUC' in metrics else [] return output def convert_results_dict_to_df ( self , results_dict ): \"\"\" This function converts a results dictionary to a pd.DataFrame format. Args: results_dict: one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs. Returns: df (pd.DataFrame): Results in a pd.DataFrame format. \"\"\" df = pd . DataFrame ( results_dict . values (), index = pd . MultiIndex . from_tuples ( results_dict . keys ())) return df def get_mixed_two_stages_fitter ( self , penalizers_combination : list ) -> TwoStagesFitter : \"\"\" This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination. Args: penalizers_combination (list): List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method. Returns: mixed_two_stages (pydts.fitters.TwoStagesFitter): TwoStagesFitter for the required penalty combination. \"\"\" _validate_estimated_value = [ p for p in penalizers_combination if p not in list ( self . meta_models . keys ())] assert len ( _validate_estimated_value ) == 0 , \\ f \"Values { _validate_estimated_value } were note estimated. All the penalizers in penalizers_combination must be estimated using evaluate() before a mixed combination can be generated.\" events = self . meta_models [ penalizers_combination [ 0 ]] . events event_type_col = self . meta_models [ penalizers_combination [ 0 ]] . event_type_col if self . TwoStagesFitter_type == 'Exact' : mixed_two_stages = TwoStagesFitterExact () else : mixed_two_stages = TwoStagesFitter () for ide , event in enumerate ( sorted ( events )): if ide == 0 : mixed_two_stages . covariates = self . meta_models [ penalizers_combination [ ide ]] . covariates mixed_two_stages . duration_col = self . meta_models [ penalizers_combination [ ide ]] . duration_col mixed_two_stages . event_type_col = self . meta_models [ penalizers_combination [ ide ]] . event_type_col mixed_two_stages . events = self . meta_models [ penalizers_combination [ ide ]] . events mixed_two_stages . pid_col = self . meta_models [ penalizers_combination [ ide ]] . pid_col mixed_two_stages . times = self . meta_models [ penalizers_combination [ ide ]] . times mixed_two_stages . beta_models [ event ] = self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ] mixed_two_stages . event_models [ event ] = [] mixed_two_stages . event_models [ event ] . append ( self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ]) event_alpha = self . meta_models [ penalizers_combination [ ide ]] . alpha_df . copy () event_alpha = event_alpha [ event_alpha [ event_type_col ] == event ] mixed_two_stages . alpha_df = pd . concat ([ mixed_two_stages . alpha_df , event_alpha ]) mixed_two_stages . event_models [ event ] . append ( event_alpha ) return mixed_two_stages def _get_model_fit_kwargs ( self , penalizer , l1_ratio ): if self . TwoStagesFitter_type == 'Exact' : fit_beta_kwargs = { 'model_fit_kwargs' : { 'alpha' : penalizer , 'L1_wt' : l1_ratio } } else : fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : penalizer , 'l1_ratio' : l1_ratio }, } return fit_beta_kwargs convert_results_dict_to_df ( self , results_dict ) \u00a4 This function converts a results dictionary to a pd.DataFrame format. Parameters: Name Type Description Default results_dict one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs. required Returns: Type Description df (pd.DataFrame) Results in a pd.DataFrame format. Source code in pydts/model_selection.py def convert_results_dict_to_df ( self , results_dict ): \"\"\" This function converts a results dictionary to a pd.DataFrame format. Args: results_dict: one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs. Returns: df (pd.DataFrame): Results in a pd.DataFrame format. \"\"\" df = pd . DataFrame ( results_dict . values (), index = pd . MultiIndex . from_tuples ( results_dict . keys ())) return df evaluate ( self , train_df , test_df , l1_ratio , penalizers , metrics = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ], seed = None , event_type_col = 'J' , duration_col = 'X' , pid_col = 'pid' , twostages_fit_kwargs = {}) \u00a4 This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers. Parameters: Name Type Description Default train_df pd.DataFrame training data for fitting the model. required test_df pd.DataFrame testing data for evaluating the estimated model's performance. required l1_ratio float regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required penalizers list penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required metrics str, list Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearch.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearch.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearch.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearch.global_bs). ['IBS', 'GBS', 'IAUC', 'GAUC'] seed int pseudo random seed number for numpy.random.seed() None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in df). 'X' pid_col str Sample ID column name (must be a column in df). 'pid' twostages_fit_kwargs dict keyword arguments to pass to the TwoStagesFitter. {} Returns: Type Description output (Tuple) Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics. Source code in pydts/model_selection.py def evaluate ( self , train_df : pd . DataFrame , test_df : pd . DataFrame , l1_ratio : float , penalizers : list , metrics : Union [ list , str ] = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ], seed : Union [ None , int ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = {}) -> tuple : \"\"\" This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers. Args: train_df (pd.DataFrame): training data for fitting the model. test_df (pd.DataFrame): testing data for evaluating the estimated model's performance. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearch.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearch.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearch.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearch.global_bs). seed (int): pseudo random seed number for numpy.random.seed() event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). twostages_fit_kwargs (dict): keyword arguments to pass to the TwoStagesFitter. Returns: output (Tuple): Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics. \"\"\" self . l1_ratio = l1_ratio self . penalizers = penalizers self . seed = seed np . random . seed ( seed ) for idp , penalizer in enumerate ( penalizers ): fit_beta_kwargs = self . _get_model_fit_kwargs ( penalizer , l1_ratio ) if self . TwoStagesFitter_type == 'Exact' : self . meta_models [ penalizer ] = TwoStagesFitterExact () else : self . meta_models [ penalizer ] = TwoStagesFitter () print ( f \"Started estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } )\" ) start = time () self . meta_models [ penalizer ] . fit ( df = train_df , fit_beta_kwargs = fit_beta_kwargs , pid_col = pid_col , event_type_col = event_type_col , duration_col = duration_col , ** twostages_fit_kwargs ) end = time () print ( f \"Finished estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } ), { int ( end - start ) } seconds\" ) events = [ j for j in sorted ( train_df [ event_type_col ] . unique ()) if j != 0 ] grid = [ penalizers for e in events ] penalizers_combinations = list ( product ( * grid )) for idc , combination in enumerate ( penalizers_combinations ): mixed_two_stages = self . get_mixed_two_stages_fitter ( combination ) pred_df = mixed_two_stages . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ combination ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ combination ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ combination ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ combination ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) output = self . convert_results_dict_to_df ( self . global_auc ) . idxmax () . values [ 0 ] if 'GAUC' in metrics else [] return output get_mixed_two_stages_fitter ( self , penalizers_combination ) \u00a4 This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination. Parameters: Name Type Description Default penalizers_combination list List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method. required Returns: Type Description mixed_two_stages (pydts.fitters.TwoStagesFitter) TwoStagesFitter for the required penalty combination. Source code in pydts/model_selection.py def get_mixed_two_stages_fitter ( self , penalizers_combination : list ) -> TwoStagesFitter : \"\"\" This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination. Args: penalizers_combination (list): List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method. Returns: mixed_two_stages (pydts.fitters.TwoStagesFitter): TwoStagesFitter for the required penalty combination. \"\"\" _validate_estimated_value = [ p for p in penalizers_combination if p not in list ( self . meta_models . keys ())] assert len ( _validate_estimated_value ) == 0 , \\ f \"Values { _validate_estimated_value } were note estimated. All the penalizers in penalizers_combination must be estimated using evaluate() before a mixed combination can be generated.\" events = self . meta_models [ penalizers_combination [ 0 ]] . events event_type_col = self . meta_models [ penalizers_combination [ 0 ]] . event_type_col if self . TwoStagesFitter_type == 'Exact' : mixed_two_stages = TwoStagesFitterExact () else : mixed_two_stages = TwoStagesFitter () for ide , event in enumerate ( sorted ( events )): if ide == 0 : mixed_two_stages . covariates = self . meta_models [ penalizers_combination [ ide ]] . covariates mixed_two_stages . duration_col = self . meta_models [ penalizers_combination [ ide ]] . duration_col mixed_two_stages . event_type_col = self . meta_models [ penalizers_combination [ ide ]] . event_type_col mixed_two_stages . events = self . meta_models [ penalizers_combination [ ide ]] . events mixed_two_stages . pid_col = self . meta_models [ penalizers_combination [ ide ]] . pid_col mixed_two_stages . times = self . meta_models [ penalizers_combination [ ide ]] . times mixed_two_stages . beta_models [ event ] = self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ] mixed_two_stages . event_models [ event ] = [] mixed_two_stages . event_models [ event ] . append ( self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ]) event_alpha = self . meta_models [ penalizers_combination [ ide ]] . alpha_df . copy () event_alpha = event_alpha [ event_alpha [ event_type_col ] == event ] mixed_two_stages . alpha_df = pd . concat ([ mixed_two_stages . alpha_df , event_alpha ]) mixed_two_stages . event_models [ event ] . append ( event_alpha ) return mixed_two_stages","title":"Model Selection"},{"location":"api/model_selection/#pydts.model_selection.BasePenaltyGridSearch","text":"This class implements the penalty parameter grid search. Source code in pydts/model_selection.py class BasePenaltyGridSearch ( object ): \"\"\" This class implements the penalty parameter grid search. \"\"\" def __init__ ( self ): self . l1_ratio = None self . penalizers = [] self . seed = None self . meta_models = {} self . train_df = None self . test_df = None self . global_auc = {} self . integrated_auc = {} self . global_bs = {} self . integrated_bs = {} self . TwoStagesFitter_type = 'CoxPHFitter' def evaluate ( self , train_df : pd . DataFrame , test_df : pd . DataFrame , l1_ratio : float , penalizers : list , metrics : Union [ list , str ] = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ], seed : Union [ None , int ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = {}) -> tuple : \"\"\" This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers. Args: train_df (pd.DataFrame): training data for fitting the model. test_df (pd.DataFrame): testing data for evaluating the estimated model's performance. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearch.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearch.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearch.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearch.global_bs). seed (int): pseudo random seed number for numpy.random.seed() event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). twostages_fit_kwargs (dict): keyword arguments to pass to the TwoStagesFitter. Returns: output (Tuple): Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics. \"\"\" self . l1_ratio = l1_ratio self . penalizers = penalizers self . seed = seed np . random . seed ( seed ) for idp , penalizer in enumerate ( penalizers ): fit_beta_kwargs = self . _get_model_fit_kwargs ( penalizer , l1_ratio ) if self . TwoStagesFitter_type == 'Exact' : self . meta_models [ penalizer ] = TwoStagesFitterExact () else : self . meta_models [ penalizer ] = TwoStagesFitter () print ( f \"Started estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } )\" ) start = time () self . meta_models [ penalizer ] . fit ( df = train_df , fit_beta_kwargs = fit_beta_kwargs , pid_col = pid_col , event_type_col = event_type_col , duration_col = duration_col , ** twostages_fit_kwargs ) end = time () print ( f \"Finished estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } ), { int ( end - start ) } seconds\" ) events = [ j for j in sorted ( train_df [ event_type_col ] . unique ()) if j != 0 ] grid = [ penalizers for e in events ] penalizers_combinations = list ( product ( * grid )) for idc , combination in enumerate ( penalizers_combinations ): mixed_two_stages = self . get_mixed_two_stages_fitter ( combination ) pred_df = mixed_two_stages . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ combination ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ combination ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ combination ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ combination ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) output = self . convert_results_dict_to_df ( self . global_auc ) . idxmax () . values [ 0 ] if 'GAUC' in metrics else [] return output def convert_results_dict_to_df ( self , results_dict ): \"\"\" This function converts a results dictionary to a pd.DataFrame format. Args: results_dict: one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs. Returns: df (pd.DataFrame): Results in a pd.DataFrame format. \"\"\" df = pd . DataFrame ( results_dict . values (), index = pd . MultiIndex . from_tuples ( results_dict . keys ())) return df def get_mixed_two_stages_fitter ( self , penalizers_combination : list ) -> TwoStagesFitter : \"\"\" This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination. Args: penalizers_combination (list): List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method. Returns: mixed_two_stages (pydts.fitters.TwoStagesFitter): TwoStagesFitter for the required penalty combination. \"\"\" _validate_estimated_value = [ p for p in penalizers_combination if p not in list ( self . meta_models . keys ())] assert len ( _validate_estimated_value ) == 0 , \\ f \"Values { _validate_estimated_value } were note estimated. All the penalizers in penalizers_combination must be estimated using evaluate() before a mixed combination can be generated.\" events = self . meta_models [ penalizers_combination [ 0 ]] . events event_type_col = self . meta_models [ penalizers_combination [ 0 ]] . event_type_col if self . TwoStagesFitter_type == 'Exact' : mixed_two_stages = TwoStagesFitterExact () else : mixed_two_stages = TwoStagesFitter () for ide , event in enumerate ( sorted ( events )): if ide == 0 : mixed_two_stages . covariates = self . meta_models [ penalizers_combination [ ide ]] . covariates mixed_two_stages . duration_col = self . meta_models [ penalizers_combination [ ide ]] . duration_col mixed_two_stages . event_type_col = self . meta_models [ penalizers_combination [ ide ]] . event_type_col mixed_two_stages . events = self . meta_models [ penalizers_combination [ ide ]] . events mixed_two_stages . pid_col = self . meta_models [ penalizers_combination [ ide ]] . pid_col mixed_two_stages . times = self . meta_models [ penalizers_combination [ ide ]] . times mixed_two_stages . beta_models [ event ] = self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ] mixed_two_stages . event_models [ event ] = [] mixed_two_stages . event_models [ event ] . append ( self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ]) event_alpha = self . meta_models [ penalizers_combination [ ide ]] . alpha_df . copy () event_alpha = event_alpha [ event_alpha [ event_type_col ] == event ] mixed_two_stages . alpha_df = pd . concat ([ mixed_two_stages . alpha_df , event_alpha ]) mixed_two_stages . event_models [ event ] . append ( event_alpha ) return mixed_two_stages def _get_model_fit_kwargs ( self , penalizer , l1_ratio ): if self . TwoStagesFitter_type == 'Exact' : fit_beta_kwargs = { 'model_fit_kwargs' : { 'alpha' : penalizer , 'L1_wt' : l1_ratio } } else : fit_beta_kwargs = { 'model_kwargs' : { 'penalizer' : penalizer , 'l1_ratio' : l1_ratio }, } return fit_beta_kwargs","title":"BasePenaltyGridSearch"},{"location":"api/model_selection/#pydts.model_selection.BasePenaltyGridSearch.convert_results_dict_to_df","text":"This function converts a results dictionary to a pd.DataFrame format. Parameters: Name Type Description Default results_dict one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs. required Returns: Type Description df (pd.DataFrame) Results in a pd.DataFrame format. Source code in pydts/model_selection.py def convert_results_dict_to_df ( self , results_dict ): \"\"\" This function converts a results dictionary to a pd.DataFrame format. Args: results_dict: one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs. Returns: df (pd.DataFrame): Results in a pd.DataFrame format. \"\"\" df = pd . DataFrame ( results_dict . values (), index = pd . MultiIndex . from_tuples ( results_dict . keys ())) return df","title":"convert_results_dict_to_df()"},{"location":"api/model_selection/#pydts.model_selection.BasePenaltyGridSearch.evaluate","text":"This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers. Parameters: Name Type Description Default train_df pd.DataFrame training data for fitting the model. required test_df pd.DataFrame testing data for evaluating the estimated model's performance. required l1_ratio float regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required penalizers list penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). required metrics str, list Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearch.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearch.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearch.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearch.global_bs). ['IBS', 'GBS', 'IAUC', 'GAUC'] seed int pseudo random seed number for numpy.random.seed() None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in df). 'X' pid_col str Sample ID column name (must be a column in df). 'pid' twostages_fit_kwargs dict keyword arguments to pass to the TwoStagesFitter. {} Returns: Type Description output (Tuple) Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics. Source code in pydts/model_selection.py def evaluate ( self , train_df : pd . DataFrame , test_df : pd . DataFrame , l1_ratio : float , penalizers : list , metrics : Union [ list , str ] = [ 'IBS' , 'GBS' , 'IAUC' , 'GAUC' ], seed : Union [ None , int ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , twostages_fit_kwargs : dict = {}) -> tuple : \"\"\" This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers. Args: train_df (pd.DataFrame): training data for fitting the model. test_df (pd.DataFrame): testing data for evaluating the estimated model's performance. l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation). metrics (str, list): Evaluation metrics. Available metrics: 'IAUC': Integrated AUC (will be in PenaltyGridSearch.integrated_auc), 'GAUC': Global AUC (will be in PenaltyGridSearch.global_auc). 'IBS': Integrated Brier Score (will be in PenaltyGridSearch.integrated_bs), 'GBS': Global Brier Score (will be in PenaltyGridSearch.global_bs). seed (int): pseudo random seed number for numpy.random.seed() event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). twostages_fit_kwargs (dict): keyword arguments to pass to the TwoStagesFitter. Returns: output (Tuple): Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics. \"\"\" self . l1_ratio = l1_ratio self . penalizers = penalizers self . seed = seed np . random . seed ( seed ) for idp , penalizer in enumerate ( penalizers ): fit_beta_kwargs = self . _get_model_fit_kwargs ( penalizer , l1_ratio ) if self . TwoStagesFitter_type == 'Exact' : self . meta_models [ penalizer ] = TwoStagesFitterExact () else : self . meta_models [ penalizer ] = TwoStagesFitter () print ( f \"Started estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } )\" ) start = time () self . meta_models [ penalizer ] . fit ( df = train_df , fit_beta_kwargs = fit_beta_kwargs , pid_col = pid_col , event_type_col = event_type_col , duration_col = duration_col , ** twostages_fit_kwargs ) end = time () print ( f \"Finished estimating the coefficients for penalizer { penalizer } ( { idp + 1 } / { len ( penalizers ) } ), { int ( end - start ) } seconds\" ) events = [ j for j in sorted ( train_df [ event_type_col ] . unique ()) if j != 0 ] grid = [ penalizers for e in events ] penalizers_combinations = list ( product ( * grid )) for idc , combination in enumerate ( penalizers_combinations ): mixed_two_stages = self . get_mixed_two_stages_fitter ( combination ) pred_df = mixed_two_stages . predict_prob_events ( test_df ) for metric in metrics : if metric == 'IAUC' : self . integrated_auc [ combination ] = events_integrated_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GAUC' : self . global_auc [ combination ] = global_auc ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'IBS' : self . integrated_bs [ combination ] = events_integrated_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) elif metric == 'GBS' : self . global_bs [ combination ] = global_brier_score ( pred_df , event_type_col = event_type_col , duration_col = duration_col ) output = self . convert_results_dict_to_df ( self . global_auc ) . idxmax () . values [ 0 ] if 'GAUC' in metrics else [] return output","title":"evaluate()"},{"location":"api/model_selection/#pydts.model_selection.BasePenaltyGridSearch.get_mixed_two_stages_fitter","text":"This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination. Parameters: Name Type Description Default penalizers_combination list List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method. required Returns: Type Description mixed_two_stages (pydts.fitters.TwoStagesFitter) TwoStagesFitter for the required penalty combination. Source code in pydts/model_selection.py def get_mixed_two_stages_fitter ( self , penalizers_combination : list ) -> TwoStagesFitter : \"\"\" This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination. Args: penalizers_combination (list): List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method. Returns: mixed_two_stages (pydts.fitters.TwoStagesFitter): TwoStagesFitter for the required penalty combination. \"\"\" _validate_estimated_value = [ p for p in penalizers_combination if p not in list ( self . meta_models . keys ())] assert len ( _validate_estimated_value ) == 0 , \\ f \"Values { _validate_estimated_value } were note estimated. All the penalizers in penalizers_combination must be estimated using evaluate() before a mixed combination can be generated.\" events = self . meta_models [ penalizers_combination [ 0 ]] . events event_type_col = self . meta_models [ penalizers_combination [ 0 ]] . event_type_col if self . TwoStagesFitter_type == 'Exact' : mixed_two_stages = TwoStagesFitterExact () else : mixed_two_stages = TwoStagesFitter () for ide , event in enumerate ( sorted ( events )): if ide == 0 : mixed_two_stages . covariates = self . meta_models [ penalizers_combination [ ide ]] . covariates mixed_two_stages . duration_col = self . meta_models [ penalizers_combination [ ide ]] . duration_col mixed_two_stages . event_type_col = self . meta_models [ penalizers_combination [ ide ]] . event_type_col mixed_two_stages . events = self . meta_models [ penalizers_combination [ ide ]] . events mixed_two_stages . pid_col = self . meta_models [ penalizers_combination [ ide ]] . pid_col mixed_two_stages . times = self . meta_models [ penalizers_combination [ ide ]] . times mixed_two_stages . beta_models [ event ] = self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ] mixed_two_stages . event_models [ event ] = [] mixed_two_stages . event_models [ event ] . append ( self . meta_models [ penalizers_combination [ ide ]] . beta_models [ event ]) event_alpha = self . meta_models [ penalizers_combination [ ide ]] . alpha_df . copy () event_alpha = event_alpha [ event_alpha [ event_type_col ] == event ] mixed_two_stages . alpha_df = pd . concat ([ mixed_two_stages . alpha_df , event_alpha ]) mixed_two_stages . event_models [ event ] . append ( event_alpha ) return mixed_two_stages","title":"get_mixed_two_stages_fitter()"},{"location":"api/two_stages_fitter/","text":"This class implements the approach of Meir et al. (2022): Examples: 1 2 3 4 from pydts.fitters import TwoStagesFitter fitter = TwoStagesFitter () fitter . fit ( df = train_df , event_type_col = 'J' , duration_col = 'X' ) fitter . print_summary () References [1] Meir, Tomer*, Gutman, Rom*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) Source code in pydts/fitters.py class TwoStagesFitter ( ExpansionBasedFitter ): \"\"\" This class implements the approach of Meir et al. (2022): Example: ```py linenums=\"1\" from pydts.fitters import TwoStagesFitter fitter = TwoStagesFitter() fitter.fit(df=train_df, event_type_col='J', duration_col='X') fitter.print_summary() ``` References: [1] Meir, Tomer\\*, Gutman, Rom\\*, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks\" (2022) \"\"\" def __init__ ( self ): super () . __init__ () self . alpha_df = pd . DataFrame () self . beta_models = {} self . beta_models_params_attr = 'params_' def _alpha_jt ( self , x , df , y_t , beta_j , n_jt , t , event ): # Alpha_jt optimization objective partial_df = df [ df [ self . duration_col ] >= t ] if isinstance ( self . covariates , list ): expit_add = np . dot ( partial_df [ self . covariates ], beta_j ) elif isinstance ( self . covariates , dict ): expit_add = np . dot ( partial_df [ self . covariates [ event ]], beta_j ) else : raise ValueError return (( 1 / y_t ) * np . sum ( expit ( x + expit_add )) - ( n_jt / y_t )) ** 2 def _fit_event_beta ( self , expanded_df , event , model = CoxPHFitter , model_kwargs = {}, model_fit_kwargs = {}): # Model fitting for conditional estimation of Beta_j for specific event if isinstance ( self . covariates , list ): strata_df = expanded_df [ self . covariates + [ f 'j_ { event } ' , self . duration_col ]] . copy () elif isinstance ( self . covariates , dict ): strata_df = expanded_df [ self . covariates [ event ] + [ f 'j_ { event } ' , self . duration_col ]] . copy () else : raise TypeError strata_df . loc [:, f ' { self . duration_col } _copy' ] = np . ones_like ( expanded_df [ self . duration_col ]) beta_j_model = model ( ** model_kwargs ) if isinstance ( self . covariates , list ): beta_j_model . fit ( df = strata_df [ self . covariates + [ f ' { self . duration_col } ' , f ' { self . duration_col } _copy' , f 'j_ { event } ' ]], duration_col = f ' { self . duration_col } _copy' , event_col = f 'j_ { event } ' , strata = self . duration_col , ** model_fit_kwargs , batch_mode = False ) elif isinstance ( self . covariates , dict ): beta_j_model . fit ( df = strata_df [ self . covariates [ event ] + [ f ' { self . duration_col } ' , f ' { self . duration_col } _copy' , f 'j_ { event } ' ]], duration_col = f ' { self . duration_col } _copy' , event_col = f 'j_ { event } ' , strata = self . duration_col , ** model_fit_kwargs , batch_mode = False ) return beta_j_model def _fit_beta ( self , expanded_df , events , model = CoxPHFitter , model_kwargs = {}, model_fit_kwargs = {}): # Model fitting for conditional estimation of Beta_j for all events _model_kwargs_per_event = np . any ([ event in model_kwargs . keys () for event in events ]) _model_fit_kwargs_per_event = np . any ([ event in model_fit_kwargs . keys () for event in events ]) beta_models = {} for event in events : _model_kwargs = model_kwargs [ event ] if _model_kwargs_per_event else model_kwargs _model_fit_kwargs = model_fit_kwargs [ event ] if _model_fit_kwargs_per_event else model_fit_kwargs beta_models [ event ] = self . _fit_event_beta ( expanded_df = expanded_df , event = event , model = model , model_kwargs = _model_kwargs , model_fit_kwargs = _model_fit_kwargs ) return beta_models def fit ( self , df : pd . DataFrame , covariates : Union [ list , dict ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , skip_expansion : bool = False , x0 : Union [ np . array , int ] = 0 , fit_beta_kwargs : dict = {}, verbose : int = 2 , nb_workers : int = WORKERS ) -> dict : \"\"\" This method fits a model to the discrete data. Args: df (pd.DataFrame): training data for fitting the model covariates (list): list of covariates to be used in estimating the regression coefficients event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. For example: fit_beta_kwargs={ model=CoxPHFitter, # model object model_kwargs={}, # keywords arguments to pass on to the model instance initiation model_fit_kwargs={} # keywords arguments to pass on to model.fit() method } verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. Returns: event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event. References: [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 \"\"\" self . _validate_cols ( df , event_type_col , duration_col , pid_col ) self . events = [ c for c in sorted ( df [ event_type_col ] . unique ()) if c != 0 ] if ( covariates is not None ): cov_not_in_df = [] if isinstance ( covariates , list ): cov_not_in_df = [ cov for cov in covariates if cov not in df . columns ] elif isinstance ( covariates , dict ): for event in self . events : event_cov_not_in_df = [ cov for cov in covariates [ event ] if cov not in df . columns ] cov_not_in_df . extend ( event_cov_not_in_df ) if len ( cov_not_in_df ) > 0 : raise ValueError ( f \"Error during fit - missing covariates from df: { cov_not_in_df } \" ) #pandarallel.initialize(verbose=verbose, nb_workers=nb_workers) if covariates is None : covariates = [ col for col in df if col not in [ event_type_col , duration_col , pid_col ]] self . covariates = covariates self . event_type_col = event_type_col self . duration_col = duration_col self . pid_col = pid_col self . times = sorted ( df [ duration_col ] . unique ()) if not skip_expansion : expanded_df = self . _expand_data ( df = df , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col ) else : print ( 'Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.' ) expanded_df = df self . beta_models = self . _fit_beta ( expanded_df , self . events , ** fit_beta_kwargs ) y_t = ( df [ duration_col ] . value_counts () . sort_index ( ascending = False ) # each event count for its occurring time and the times before . cumsum () . sort_index () ) n_jt = df . groupby ([ event_type_col , duration_col ]) . size () . to_frame () . reset_index () n_jt . columns = [ event_type_col , duration_col , 'n_jt' ] for event in self . events : n_et = n_jt [ n_jt [ event_type_col ] == event ] . copy () if isinstance ( self . beta_models [ event ], CoxPHFitter ): self . beta_models_params_attr = 'params_' _res = Parallel ( n_jobs = nb_workers )( delayed ( minimize )( self . _alpha_jt , x0 = x0 , args = ( df , y_t . loc [ row [ duration_col ]], getattr ( self . beta_models [ event ], self . beta_models_params_attr ), row [ 'n_jt' ], row [ duration_col ], event ), method = 'BFGS' , options = { 'gtol' : 1e-7 , 'eps' : 1.5e-08 , 'maxiter' : 200 }) for _ , row in n_et . iterrows ()) n_et [ 'success' ] = Parallel ( n_jobs = nb_workers )( delayed ( lambda row : row . success )( val ) for val in _res ) n_et [ 'alpha_jt' ] = Parallel ( n_jobs = nb_workers )( delayed ( lambda row : row . x [ 0 ])( val ) for val in _res ) elif isinstance ( self . beta_models [ event ], ConditionalResultsWrapper ) or \\ isinstance ( self . beta_models [ event ], RegularizedResultsWrapper ): self . beta_models_params_attr = 'params' for idx , row in n_et . iterrows (): _res = minimize ( self . _alpha_jt , x0 = x0 , args = ( df , y_t . loc [ row [ duration_col ]], getattr ( self . beta_models [ event ], self . beta_models_params_attr ), row [ 'n_jt' ], row [ duration_col ], event ), method = 'BFGS' , options = { 'gtol' : 1e-7 , 'eps' : 1.5e-08 , 'maxiter' : 200 }) n_et . loc [ idx , 'success' ] = _res . success n_et . loc [ idx , 'alpha_jt' ] = _res . x [ 0 ] else : raise ValueError # n_et['opt_res'] = n_et.parallel_apply(lambda row: minimize(self._alpha_jt, x0=x0, # args=(df, y_t.loc[row[duration_col]], event_beta_params, row['n_jt'], # row[duration_col], event), method='BFGS', # options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200}), axis=1) # n_et['success'] = n_et['opt_res'].parallel_apply(lambda val: val.success) # n_et['alpha_jt'] = n_et['opt_res'].parallel_apply(lambda val: val.x[0]) assert_fit ( n_et , self . times [: - 1 ], event_type_col = event_type_col , duration_col = duration_col ) # todo move basic input validation before any optimization self . event_models [ event ] = [ self . beta_models [ event ], n_et ] self . alpha_df = pd . concat ([ self . alpha_df , n_et ], ignore_index = True ) return self . event_models def print_summary ( self , summary_func : str = \"print_summary\" , summary_kwargs : dict = {}) -> None : \"\"\" This method prints the summary of the fitted models for all the events. Args: summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\"). summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function. Returns: None \"\"\" from IPython.display import display display ( self . get_beta_SE ()) for event , model in self . event_models . items (): print ( f ' \\n\\n Model summary for event: { event } ' ) display ( model [ 1 ] . set_index ([ self . event_type_col , self . duration_col ])) def plot_event_alpha ( self , event : Union [ str , int ], ax : plt . Axes = None , scatter_kwargs : dict = {}, show = True , title = None , xlabel = 't' , ylabel = r '$\\alpha_ {jt} $' , fontsize = 18 , color : str = None , label : str = None , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event. Args: event (Union[str, int]): event name ax (matplotlib.pyplot.Axes, Optional): ax to use scatter_kwargs (dict, Optional): keywords to pass to the scatter function show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize color (str, Optional): color name to use label (str, Optional): label name Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" assert event in self . events , f \"Cannot plot event { event } alpha - it was not included during .fit()\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) title = r '$\\alpha_ {jt} $' + f ' for event { event } ' if title is None else title label = f ' { event } ' if label is None else label color = 'tab:blue' if color is None else color alpha_df = self . event_models [ event ][ 1 ] ax . scatter ( alpha_df [ self . duration_col ] . values , alpha_df [ 'alpha_jt' ] . values , label = label , color = color , ** scatter_kwargs ) ax . set_title ( title , fontsize = fontsize ) ax . set_xlabel ( xlabel , fontsize = fontsize ) ax . set_ylabel ( ylabel , fontsize = fontsize ) if show : plt . show () return ax def plot_all_events_alpha ( self , ax : plt . Axes = None , scatter_kwargs : dict = {}, colors : list = COLORS , show : bool = True , title : Union [ str , None ] = None , xlabel : str = 't' , ylabel : str = r '$\\alpha_ {jt} $' , fontsize : int = 18 , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events. Args: ax (matplotlib.pyplot.Axes, Optional): ax to use scatter_kwargs (dict, Optional): keywords to pass to the scatter function colors (list, Optional): colors names show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) title = r '$\\alpha_ {jt} $' + f ' for all events' if title is None else title for idx , ( event , model ) in enumerate ( self . event_models . items ()): label = f ' { event } ' color = colors [ idx % len ( colors )] self . plot_event_alpha ( event = event , ax = ax , scatter_kwargs = scatter_kwargs , show = False , title = title , ylabel = ylabel , xlabel = xlabel , fontsize = fontsize , label = label , color = color , ticklabelsize = ticklabelsize ) ax . legend () if show : plt . show () return ax def predict_hazard_jt ( self , df : pd . DataFrame , event : Union [ str , int ], t : Union [ Iterable , int ]) -> pd . DataFrame : \"\"\" This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Args: df (pd.DataFrame): samples to predict for event (Union[str, int]): event name t (Union[Iterable, int]): times to calculate the hazard for Returns: df (pd.DataFrame): samples with the prediction columns \"\"\" self . _validate_covariates_in_df ( df . head ()) t = self . _validate_t ( t , return_iter = True ) assert event in self . events , \\ f \"Cannot predict for event { event } - it was not included during .fit()\" model = self . event_models [ event ] alpha_df = model [ 1 ] . set_index ( self . duration_col )[ 'alpha_jt' ] . copy () _t = np . array ([ t_i for t_i in t if ( f 'hazard_j { event } _t { t_i } ' not in df . columns )]) if len ( _t ) == 0 : return df temp_df = df . copy () if isinstance ( self . covariates , list ): beta_j_x = temp_df [ self . covariates ] . dot ( getattr ( model [ 0 ], self . beta_models_params_attr )) elif isinstance ( self . covariates , dict ): beta_j_x = temp_df [ self . covariates [ event ]] . dot ( getattr ( model [ 0 ], self . beta_models_params_attr )) temp_df [[ f 'hazard_j { event } _t { c } ' for c in _t ]] = pd . concat ( [ self . _hazard_inverse_transformation ( alpha_df [ c ] + beta_j_x ) for c in _t ], axis = 1 ) . values return temp_df def _hazard_transformation ( self , a : Union [ int , np . array , pd . Series , pd . DataFrame ]) -> \\ Union [ int , np . array , pd . Series , pd . DataFrame ]: \"\"\" This function defines the transformation of the hazard function such that $ h ( \\lambda_j (t | Z) ) = \\alpha_{jt} + Z^{T} \\beta_{j} $ Args: a (Union[int, np.array, pd.Series, pd.DataFrame]): Returns: i (Union[int, np.array, pd.Series, pd.DataFrame]): the inverse function applied on a. $ h^{-1} (a)$ \"\"\" i = logit ( a ) return i def _hazard_inverse_transformation ( self , a : Union [ int , np . array , pd . Series , pd . DataFrame ]) -> \\ Union [ int , np . array , pd . Series , pd . DataFrame ]: \"\"\" This function defines the inverse transformation of the hazard function such that $\\lambda_j (t | Z) = h^{-1} ( \\alpha_{jt} + Z^{T} \\beta_{j} )$ Args: a (Union[int, np.array, pd.Series, pd.DataFrame]): Returns: i (Union[int, np.array, pd.Series, pd.DataFrame]): the inverse function applied on a. $ h^{-1} (a) $ \"\"\" i = expit ( a ) return i def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" se_df = pd . DataFrame () for event , model in self . beta_models . items (): mdf = pd . concat ([ model . params_ , model . standard_errors_ ], axis = 1 ) mdf . columns = [ f 'j { event } _params' , f 'j { event } _SE' ] se_df = pd . concat ([ se_df , mdf ], axis = 1 ) return se_df def get_alpha_df ( self ): \"\"\" This function returns the Alpha coefficients for all the events. Returns: alpha_df (pandas.DataFrame): Alpha coefficients Dataframe \"\"\" alpha_df = pd . DataFrame () for event , model in self . event_models . items (): model_alpha_df = model [ 1 ] . set_index ([ self . event_type_col , self . duration_col ]) model_alpha_df . columns = pd . MultiIndex . from_product ([[ event ], model_alpha_df . columns ]) alpha_df = pd . concat ([ alpha_df , model_alpha_df ], axis = 1 ) return alpha_df def plot_all_events_beta ( self , ax : plt . Axes = None , colors : list = COLORS , show : bool = True , title : Union [ str , None ] = None , xlabel : str = 'Value' , ylabel : str = r '$\\beta_ {j} $' , fontsize : int = 18 , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots the $ beta_{j} $ coefficients and standard errors of all the events. Args: ax (matplotlib.pyplot.Axes, Optional): ax to use colors (list, Optional): colors names show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize ticklabelsize (int, Optional): axes xticklabels, yticklabels fontsize Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) title = r '$\\beta_ {j} $' + f ' for all events' if title is None else title ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) se_df = self . get_beta_SE () for idx , col in enumerate ( se_df . columns ): if idx % 2 == 1 : continue y = np . arange (( idx // 2 ) * len ( se_df ), ( 1 + ( idx // 2 )) * len ( se_df )) ax . errorbar ( x = se_df . iloc [:, idx ] . values , y = y , color = colors [ idx % len ( colors )], xerr = se_df . iloc [:, idx + 1 ] . values , label = f ' { col } ' , markersize = 6 , ls = '' , marker = 'o' ) yt = list ( se_df . index ) * ( len ( se_df . columns ) // 2 ) ax . set_yticks ( np . arange ( 0 , len ( yt ))) ax . set_yticklabels ( yt ) ax . set_title ( title , fontsize = fontsize ) ax . set_xlabel ( xlabel , fontsize = fontsize ) ax . set_ylabel ( ylabel , fontsize = fontsize ) ax . grid () plt . gca () . invert_yaxis () ax . legend () if show : plt . show () return ax fit ( self , df , covariates = None , event_type_col = 'J' , duration_col = 'X' , pid_col = 'pid' , skip_expansion = False , x0 = 0 , fit_beta_kwargs = {}, verbose = 2 , nb_workers = 2 ) \u00a4 This method fits a model to the discrete data. Parameters: Name Type Description Default df pd.DataFrame training data for fitting the model required covariates list list of covariates to be used in estimating the regression coefficients None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in df). 'X' pid_col str Sample ID column name (must be a column in df). 'pid' skip_expansion boolean Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. False x0 Union[numpy.array, int], Optional initial guess to pass to scipy.optimize.minimize function 0 fit_beta_kwargs dict, Optional Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. For example: fit_beta_kwargs={ model=CoxPHFitter, # model object model_kwargs={}, # keywords arguments to pass on to the model instance initiation model_fit_kwargs={} # keywords arguments to pass on to model.fit() method } {} verbose int, Optional The verbosity level of pandaallel 2 nb_workers int, Optional The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. 2 Returns: Type Description event_models (dict) Fitted models dictionary. Keys - event names, Values - fitted models for the event. References [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 Source code in pydts/fitters.py def fit ( self , df : pd . DataFrame , covariates : Union [ list , dict ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , skip_expansion : bool = False , x0 : Union [ np . array , int ] = 0 , fit_beta_kwargs : dict = {}, verbose : int = 2 , nb_workers : int = WORKERS ) -> dict : \"\"\" This method fits a model to the discrete data. Args: df (pd.DataFrame): training data for fitting the model covariates (list): list of covariates to be used in estimating the regression coefficients event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. For example: fit_beta_kwargs={ model=CoxPHFitter, # model object model_kwargs={}, # keywords arguments to pass on to the model instance initiation model_fit_kwargs={} # keywords arguments to pass on to model.fit() method } verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. Returns: event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event. References: [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 \"\"\" self . _validate_cols ( df , event_type_col , duration_col , pid_col ) self . events = [ c for c in sorted ( df [ event_type_col ] . unique ()) if c != 0 ] if ( covariates is not None ): cov_not_in_df = [] if isinstance ( covariates , list ): cov_not_in_df = [ cov for cov in covariates if cov not in df . columns ] elif isinstance ( covariates , dict ): for event in self . events : event_cov_not_in_df = [ cov for cov in covariates [ event ] if cov not in df . columns ] cov_not_in_df . extend ( event_cov_not_in_df ) if len ( cov_not_in_df ) > 0 : raise ValueError ( f \"Error during fit - missing covariates from df: { cov_not_in_df } \" ) #pandarallel.initialize(verbose=verbose, nb_workers=nb_workers) if covariates is None : covariates = [ col for col in df if col not in [ event_type_col , duration_col , pid_col ]] self . covariates = covariates self . event_type_col = event_type_col self . duration_col = duration_col self . pid_col = pid_col self . times = sorted ( df [ duration_col ] . unique ()) if not skip_expansion : expanded_df = self . _expand_data ( df = df , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col ) else : print ( 'Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.' ) expanded_df = df self . beta_models = self . _fit_beta ( expanded_df , self . events , ** fit_beta_kwargs ) y_t = ( df [ duration_col ] . value_counts () . sort_index ( ascending = False ) # each event count for its occurring time and the times before . cumsum () . sort_index () ) n_jt = df . groupby ([ event_type_col , duration_col ]) . size () . to_frame () . reset_index () n_jt . columns = [ event_type_col , duration_col , 'n_jt' ] for event in self . events : n_et = n_jt [ n_jt [ event_type_col ] == event ] . copy () if isinstance ( self . beta_models [ event ], CoxPHFitter ): self . beta_models_params_attr = 'params_' _res = Parallel ( n_jobs = nb_workers )( delayed ( minimize )( self . _alpha_jt , x0 = x0 , args = ( df , y_t . loc [ row [ duration_col ]], getattr ( self . beta_models [ event ], self . beta_models_params_attr ), row [ 'n_jt' ], row [ duration_col ], event ), method = 'BFGS' , options = { 'gtol' : 1e-7 , 'eps' : 1.5e-08 , 'maxiter' : 200 }) for _ , row in n_et . iterrows ()) n_et [ 'success' ] = Parallel ( n_jobs = nb_workers )( delayed ( lambda row : row . success )( val ) for val in _res ) n_et [ 'alpha_jt' ] = Parallel ( n_jobs = nb_workers )( delayed ( lambda row : row . x [ 0 ])( val ) for val in _res ) elif isinstance ( self . beta_models [ event ], ConditionalResultsWrapper ) or \\ isinstance ( self . beta_models [ event ], RegularizedResultsWrapper ): self . beta_models_params_attr = 'params' for idx , row in n_et . iterrows (): _res = minimize ( self . _alpha_jt , x0 = x0 , args = ( df , y_t . loc [ row [ duration_col ]], getattr ( self . beta_models [ event ], self . beta_models_params_attr ), row [ 'n_jt' ], row [ duration_col ], event ), method = 'BFGS' , options = { 'gtol' : 1e-7 , 'eps' : 1.5e-08 , 'maxiter' : 200 }) n_et . loc [ idx , 'success' ] = _res . success n_et . loc [ idx , 'alpha_jt' ] = _res . x [ 0 ] else : raise ValueError # n_et['opt_res'] = n_et.parallel_apply(lambda row: minimize(self._alpha_jt, x0=x0, # args=(df, y_t.loc[row[duration_col]], event_beta_params, row['n_jt'], # row[duration_col], event), method='BFGS', # options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200}), axis=1) # n_et['success'] = n_et['opt_res'].parallel_apply(lambda val: val.success) # n_et['alpha_jt'] = n_et['opt_res'].parallel_apply(lambda val: val.x[0]) assert_fit ( n_et , self . times [: - 1 ], event_type_col = event_type_col , duration_col = duration_col ) # todo move basic input validation before any optimization self . event_models [ event ] = [ self . beta_models [ event ], n_et ] self . alpha_df = pd . concat ([ self . alpha_df , n_et ], ignore_index = True ) return self . event_models get_alpha_df ( self ) \u00a4 This function returns the Alpha coefficients for all the events. Returns: Type Description alpha_df (pandas.DataFrame) Alpha coefficients Dataframe Source code in pydts/fitters.py def get_alpha_df ( self ): \"\"\" This function returns the Alpha coefficients for all the events. Returns: alpha_df (pandas.DataFrame): Alpha coefficients Dataframe \"\"\" alpha_df = pd . DataFrame () for event , model in self . event_models . items (): model_alpha_df = model [ 1 ] . set_index ([ self . event_type_col , self . duration_col ]) model_alpha_df . columns = pd . MultiIndex . from_product ([[ event ], model_alpha_df . columns ]) alpha_df = pd . concat ([ alpha_df , model_alpha_df ], axis = 1 ) return alpha_df get_beta_SE ( self ) \u00a4 This function returns the Beta coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Beta coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" se_df = pd . DataFrame () for event , model in self . beta_models . items (): mdf = pd . concat ([ model . params_ , model . standard_errors_ ], axis = 1 ) mdf . columns = [ f 'j { event } _params' , f 'j { event } _SE' ] se_df = pd . concat ([ se_df , mdf ], axis = 1 ) return se_df plot_all_events_alpha ( self , ax = None , scatter_kwargs = {}, colors = [ 'tab:blue' , 'tab:orange' , 'tab:green' , 'tab:red' , 'tab:purple' , 'tab:brown' , 'tab:pink' , 'tab:gray' , 'tab:olive' , 'tab:cyan' ], show = True , title = None , xlabel = 't' , ylabel = '$ \\\\ alpha_ {jt} $' , fontsize = 18 , ticklabelsize = 15 ) \u00a4 This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events. Parameters: Name Type Description Default ax matplotlib.pyplot.Axes, Optional ax to use None scatter_kwargs dict, Optional keywords to pass to the scatter function {} colors list, Optional colors names ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'] show bool, Optional if to use plt.show() True title str, Optional axes title None xlabel str, Optional axes xlabel 't' ylabel str, Optional axes ylabel '$\\\\alpha_{jt}$' fontsize int, Optional axes title, xlabel, ylabel fontsize 18 Returns: Type Description ax (matplotlib.pyplot.Axes) output figure Source code in pydts/fitters.py def plot_all_events_alpha ( self , ax : plt . Axes = None , scatter_kwargs : dict = {}, colors : list = COLORS , show : bool = True , title : Union [ str , None ] = None , xlabel : str = 't' , ylabel : str = r '$\\alpha_ {jt} $' , fontsize : int = 18 , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events. Args: ax (matplotlib.pyplot.Axes, Optional): ax to use scatter_kwargs (dict, Optional): keywords to pass to the scatter function colors (list, Optional): colors names show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) title = r '$\\alpha_ {jt} $' + f ' for all events' if title is None else title for idx , ( event , model ) in enumerate ( self . event_models . items ()): label = f ' { event } ' color = colors [ idx % len ( colors )] self . plot_event_alpha ( event = event , ax = ax , scatter_kwargs = scatter_kwargs , show = False , title = title , ylabel = ylabel , xlabel = xlabel , fontsize = fontsize , label = label , color = color , ticklabelsize = ticklabelsize ) ax . legend () if show : plt . show () return ax plot_all_events_beta ( self , ax = None , colors = [ 'tab:blue' , 'tab:orange' , 'tab:green' , 'tab:red' , 'tab:purple' , 'tab:brown' , 'tab:pink' , 'tab:gray' , 'tab:olive' , 'tab:cyan' ], show = True , title = None , xlabel = 'Value' , ylabel = '$ \\\\ beta_ {j} $' , fontsize = 18 , ticklabelsize = 15 ) \u00a4 This function plots the $ beta_{j} $ coefficients and standard errors of all the events. Parameters: Name Type Description Default ax matplotlib.pyplot.Axes, Optional ax to use None colors list, Optional colors names ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'] show bool, Optional if to use plt.show() True title str, Optional axes title None xlabel str, Optional axes xlabel 'Value' ylabel str, Optional axes ylabel '$\\\\beta_{j}$' fontsize int, Optional axes title, xlabel, ylabel fontsize 18 ticklabelsize int, Optional axes xticklabels, yticklabels fontsize 15 Returns: Type Description ax (matplotlib.pyplot.Axes) output figure Source code in pydts/fitters.py def plot_all_events_beta ( self , ax : plt . Axes = None , colors : list = COLORS , show : bool = True , title : Union [ str , None ] = None , xlabel : str = 'Value' , ylabel : str = r '$\\beta_ {j} $' , fontsize : int = 18 , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots the $ beta_{j} $ coefficients and standard errors of all the events. Args: ax (matplotlib.pyplot.Axes, Optional): ax to use colors (list, Optional): colors names show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize ticklabelsize (int, Optional): axes xticklabels, yticklabels fontsize Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) title = r '$\\beta_ {j} $' + f ' for all events' if title is None else title ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) se_df = self . get_beta_SE () for idx , col in enumerate ( se_df . columns ): if idx % 2 == 1 : continue y = np . arange (( idx // 2 ) * len ( se_df ), ( 1 + ( idx // 2 )) * len ( se_df )) ax . errorbar ( x = se_df . iloc [:, idx ] . values , y = y , color = colors [ idx % len ( colors )], xerr = se_df . iloc [:, idx + 1 ] . values , label = f ' { col } ' , markersize = 6 , ls = '' , marker = 'o' ) yt = list ( se_df . index ) * ( len ( se_df . columns ) // 2 ) ax . set_yticks ( np . arange ( 0 , len ( yt ))) ax . set_yticklabels ( yt ) ax . set_title ( title , fontsize = fontsize ) ax . set_xlabel ( xlabel , fontsize = fontsize ) ax . set_ylabel ( ylabel , fontsize = fontsize ) ax . grid () plt . gca () . invert_yaxis () ax . legend () if show : plt . show () return ax plot_event_alpha ( self , event , ax = None , scatter_kwargs = {}, show = True , title = None , xlabel = 't' , ylabel = '$ \\\\ alpha_ {jt} $' , fontsize = 18 , color = None , label = None , ticklabelsize = 15 ) \u00a4 This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event. Parameters: Name Type Description Default event Union[str, int] event name required ax matplotlib.pyplot.Axes, Optional ax to use None scatter_kwargs dict, Optional keywords to pass to the scatter function {} show bool, Optional if to use plt.show() True title str, Optional axes title None xlabel str, Optional axes xlabel 't' ylabel str, Optional axes ylabel '$\\\\alpha_{jt}$' fontsize int, Optional axes title, xlabel, ylabel fontsize 18 color str, Optional color name to use None label str, Optional label name None Returns: Type Description ax (matplotlib.pyplot.Axes) output figure Source code in pydts/fitters.py def plot_event_alpha ( self , event : Union [ str , int ], ax : plt . Axes = None , scatter_kwargs : dict = {}, show = True , title = None , xlabel = 't' , ylabel = r '$\\alpha_ {jt} $' , fontsize = 18 , color : str = None , label : str = None , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event. Args: event (Union[str, int]): event name ax (matplotlib.pyplot.Axes, Optional): ax to use scatter_kwargs (dict, Optional): keywords to pass to the scatter function show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize color (str, Optional): color name to use label (str, Optional): label name Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" assert event in self . events , f \"Cannot plot event { event } alpha - it was not included during .fit()\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) title = r '$\\alpha_ {jt} $' + f ' for event { event } ' if title is None else title label = f ' { event } ' if label is None else label color = 'tab:blue' if color is None else color alpha_df = self . event_models [ event ][ 1 ] ax . scatter ( alpha_df [ self . duration_col ] . values , alpha_df [ 'alpha_jt' ] . values , label = label , color = color , ** scatter_kwargs ) ax . set_title ( title , fontsize = fontsize ) ax . set_xlabel ( xlabel , fontsize = fontsize ) ax . set_ylabel ( ylabel , fontsize = fontsize ) if show : plt . show () return ax predict_hazard_jt ( self , df , event , t ) \u00a4 This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Parameters: Name Type Description Default df pd.DataFrame samples to predict for required event Union[str, int] event name required t Union[Iterable, int] times to calculate the hazard for required Returns: Type Description df (pd.DataFrame) samples with the prediction columns Source code in pydts/fitters.py def predict_hazard_jt ( self , df : pd . DataFrame , event : Union [ str , int ], t : Union [ Iterable , int ]) -> pd . DataFrame : \"\"\" This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Args: df (pd.DataFrame): samples to predict for event (Union[str, int]): event name t (Union[Iterable, int]): times to calculate the hazard for Returns: df (pd.DataFrame): samples with the prediction columns \"\"\" self . _validate_covariates_in_df ( df . head ()) t = self . _validate_t ( t , return_iter = True ) assert event in self . events , \\ f \"Cannot predict for event { event } - it was not included during .fit()\" model = self . event_models [ event ] alpha_df = model [ 1 ] . set_index ( self . duration_col )[ 'alpha_jt' ] . copy () _t = np . array ([ t_i for t_i in t if ( f 'hazard_j { event } _t { t_i } ' not in df . columns )]) if len ( _t ) == 0 : return df temp_df = df . copy () if isinstance ( self . covariates , list ): beta_j_x = temp_df [ self . covariates ] . dot ( getattr ( model [ 0 ], self . beta_models_params_attr )) elif isinstance ( self . covariates , dict ): beta_j_x = temp_df [ self . covariates [ event ]] . dot ( getattr ( model [ 0 ], self . beta_models_params_attr )) temp_df [[ f 'hazard_j { event } _t { c } ' for c in _t ]] = pd . concat ( [ self . _hazard_inverse_transformation ( alpha_df [ c ] + beta_j_x ) for c in _t ], axis = 1 ) . values return temp_df print_summary ( self , summary_func = 'print_summary' , summary_kwargs = {}) \u00a4 This method prints the summary of the fitted models for all the events. Parameters: Name Type Description Default summary_func str, Optional print summary method of the fitted model type (\"summary\", \"print_summary\"). 'print_summary' summary_kwargs dict, Optional Keyword arguments to pass to the model summary function. {} Returns: Type Description None None Source code in pydts/fitters.py def print_summary ( self , summary_func : str = \"print_summary\" , summary_kwargs : dict = {}) -> None : \"\"\" This method prints the summary of the fitted models for all the events. Args: summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\"). summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function. Returns: None \"\"\" from IPython.display import display display ( self . get_beta_SE ()) for event , model in self . event_models . items (): print ( f ' \\n\\n Model summary for event: { event } ' ) display ( model [ 1 ] . set_index ([ self . event_type_col , self . duration_col ]))","title":"The Two Stages Procedure of Meir and Gorfine (2023) - Efron"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.fit","text":"This method fits a model to the discrete data. Parameters: Name Type Description Default df pd.DataFrame training data for fitting the model required covariates list list of covariates to be used in estimating the regression coefficients None event_type_col str The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. 'J' duration_col str Last follow up time column name (must be a column in df). 'X' pid_col str Sample ID column name (must be a column in df). 'pid' skip_expansion boolean Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. False x0 Union[numpy.array, int], Optional initial guess to pass to scipy.optimize.minimize function 0 fit_beta_kwargs dict, Optional Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. For example: fit_beta_kwargs={ model=CoxPHFitter, # model object model_kwargs={}, # keywords arguments to pass on to the model instance initiation model_fit_kwargs={} # keywords arguments to pass on to model.fit() method } {} verbose int, Optional The verbosity level of pandaallel 2 nb_workers int, Optional The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. 2 Returns: Type Description event_models (dict) Fitted models dictionary. Keys - event names, Values - fitted models for the event. References [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 Source code in pydts/fitters.py def fit ( self , df : pd . DataFrame , covariates : Union [ list , dict ] = None , event_type_col : str = 'J' , duration_col : str = 'X' , pid_col : str = 'pid' , skip_expansion : bool = False , x0 : Union [ np . array , int ] = 0 , fit_beta_kwargs : dict = {}, verbose : int = 2 , nb_workers : int = WORKERS ) -> dict : \"\"\" This method fits a model to the discrete data. Args: df (pd.DataFrame): training data for fitting the model covariates (list): list of covariates to be used in estimating the regression coefficients event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0. duration_col (str): Last follow up time column name (must be a column in df). pid_col (str): Sample ID column name (must be a column in df). skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded (see [1]). When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data. x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here. For example: fit_beta_kwargs={ model=CoxPHFitter, # model object model_kwargs={}, # keywords arguments to pass on to the model instance initiation model_fit_kwargs={} # keywords arguments to pass on to model.fit() method } verbose (int, Optional): The verbosity level of pandaallel nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available. Returns: event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event. References: [1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", https://arxiv.org/abs/2303.01186 \"\"\" self . _validate_cols ( df , event_type_col , duration_col , pid_col ) self . events = [ c for c in sorted ( df [ event_type_col ] . unique ()) if c != 0 ] if ( covariates is not None ): cov_not_in_df = [] if isinstance ( covariates , list ): cov_not_in_df = [ cov for cov in covariates if cov not in df . columns ] elif isinstance ( covariates , dict ): for event in self . events : event_cov_not_in_df = [ cov for cov in covariates [ event ] if cov not in df . columns ] cov_not_in_df . extend ( event_cov_not_in_df ) if len ( cov_not_in_df ) > 0 : raise ValueError ( f \"Error during fit - missing covariates from df: { cov_not_in_df } \" ) #pandarallel.initialize(verbose=verbose, nb_workers=nb_workers) if covariates is None : covariates = [ col for col in df if col not in [ event_type_col , duration_col , pid_col ]] self . covariates = covariates self . event_type_col = event_type_col self . duration_col = duration_col self . pid_col = pid_col self . times = sorted ( df [ duration_col ] . unique ()) if not skip_expansion : expanded_df = self . _expand_data ( df = df , event_type_col = event_type_col , duration_col = duration_col , pid_col = pid_col ) else : print ( 'Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.' ) expanded_df = df self . beta_models = self . _fit_beta ( expanded_df , self . events , ** fit_beta_kwargs ) y_t = ( df [ duration_col ] . value_counts () . sort_index ( ascending = False ) # each event count for its occurring time and the times before . cumsum () . sort_index () ) n_jt = df . groupby ([ event_type_col , duration_col ]) . size () . to_frame () . reset_index () n_jt . columns = [ event_type_col , duration_col , 'n_jt' ] for event in self . events : n_et = n_jt [ n_jt [ event_type_col ] == event ] . copy () if isinstance ( self . beta_models [ event ], CoxPHFitter ): self . beta_models_params_attr = 'params_' _res = Parallel ( n_jobs = nb_workers )( delayed ( minimize )( self . _alpha_jt , x0 = x0 , args = ( df , y_t . loc [ row [ duration_col ]], getattr ( self . beta_models [ event ], self . beta_models_params_attr ), row [ 'n_jt' ], row [ duration_col ], event ), method = 'BFGS' , options = { 'gtol' : 1e-7 , 'eps' : 1.5e-08 , 'maxiter' : 200 }) for _ , row in n_et . iterrows ()) n_et [ 'success' ] = Parallel ( n_jobs = nb_workers )( delayed ( lambda row : row . success )( val ) for val in _res ) n_et [ 'alpha_jt' ] = Parallel ( n_jobs = nb_workers )( delayed ( lambda row : row . x [ 0 ])( val ) for val in _res ) elif isinstance ( self . beta_models [ event ], ConditionalResultsWrapper ) or \\ isinstance ( self . beta_models [ event ], RegularizedResultsWrapper ): self . beta_models_params_attr = 'params' for idx , row in n_et . iterrows (): _res = minimize ( self . _alpha_jt , x0 = x0 , args = ( df , y_t . loc [ row [ duration_col ]], getattr ( self . beta_models [ event ], self . beta_models_params_attr ), row [ 'n_jt' ], row [ duration_col ], event ), method = 'BFGS' , options = { 'gtol' : 1e-7 , 'eps' : 1.5e-08 , 'maxiter' : 200 }) n_et . loc [ idx , 'success' ] = _res . success n_et . loc [ idx , 'alpha_jt' ] = _res . x [ 0 ] else : raise ValueError # n_et['opt_res'] = n_et.parallel_apply(lambda row: minimize(self._alpha_jt, x0=x0, # args=(df, y_t.loc[row[duration_col]], event_beta_params, row['n_jt'], # row[duration_col], event), method='BFGS', # options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200}), axis=1) # n_et['success'] = n_et['opt_res'].parallel_apply(lambda val: val.success) # n_et['alpha_jt'] = n_et['opt_res'].parallel_apply(lambda val: val.x[0]) assert_fit ( n_et , self . times [: - 1 ], event_type_col = event_type_col , duration_col = duration_col ) # todo move basic input validation before any optimization self . event_models [ event ] = [ self . beta_models [ event ], n_et ] self . alpha_df = pd . concat ([ self . alpha_df , n_et ], ignore_index = True ) return self . event_models","title":"fit()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.get_alpha_df","text":"This function returns the Alpha coefficients for all the events. Returns: Type Description alpha_df (pandas.DataFrame) Alpha coefficients Dataframe Source code in pydts/fitters.py def get_alpha_df ( self ): \"\"\" This function returns the Alpha coefficients for all the events. Returns: alpha_df (pandas.DataFrame): Alpha coefficients Dataframe \"\"\" alpha_df = pd . DataFrame () for event , model in self . event_models . items (): model_alpha_df = model [ 1 ] . set_index ([ self . event_type_col , self . duration_col ]) model_alpha_df . columns = pd . MultiIndex . from_product ([[ event ], model_alpha_df . columns ]) alpha_df = pd . concat ([ alpha_df , model_alpha_df ], axis = 1 ) return alpha_df","title":"get_alpha_df()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.get_beta_SE","text":"This function returns the Beta coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Beta coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" se_df = pd . DataFrame () for event , model in self . beta_models . items (): mdf = pd . concat ([ model . params_ , model . standard_errors_ ], axis = 1 ) mdf . columns = [ f 'j { event } _params' , f 'j { event } _SE' ] se_df = pd . concat ([ se_df , mdf ], axis = 1 ) return se_df","title":"get_beta_SE()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.plot_all_events_alpha","text":"This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events. Parameters: Name Type Description Default ax matplotlib.pyplot.Axes, Optional ax to use None scatter_kwargs dict, Optional keywords to pass to the scatter function {} colors list, Optional colors names ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'] show bool, Optional if to use plt.show() True title str, Optional axes title None xlabel str, Optional axes xlabel 't' ylabel str, Optional axes ylabel '$\\\\alpha_{jt}$' fontsize int, Optional axes title, xlabel, ylabel fontsize 18 Returns: Type Description ax (matplotlib.pyplot.Axes) output figure Source code in pydts/fitters.py def plot_all_events_alpha ( self , ax : plt . Axes = None , scatter_kwargs : dict = {}, colors : list = COLORS , show : bool = True , title : Union [ str , None ] = None , xlabel : str = 't' , ylabel : str = r '$\\alpha_ {jt} $' , fontsize : int = 18 , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events. Args: ax (matplotlib.pyplot.Axes, Optional): ax to use scatter_kwargs (dict, Optional): keywords to pass to the scatter function colors (list, Optional): colors names show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) title = r '$\\alpha_ {jt} $' + f ' for all events' if title is None else title for idx , ( event , model ) in enumerate ( self . event_models . items ()): label = f ' { event } ' color = colors [ idx % len ( colors )] self . plot_event_alpha ( event = event , ax = ax , scatter_kwargs = scatter_kwargs , show = False , title = title , ylabel = ylabel , xlabel = xlabel , fontsize = fontsize , label = label , color = color , ticklabelsize = ticklabelsize ) ax . legend () if show : plt . show () return ax","title":"plot_all_events_alpha()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.plot_all_events_beta","text":"This function plots the $ beta_{j} $ coefficients and standard errors of all the events. Parameters: Name Type Description Default ax matplotlib.pyplot.Axes, Optional ax to use None colors list, Optional colors names ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'] show bool, Optional if to use plt.show() True title str, Optional axes title None xlabel str, Optional axes xlabel 'Value' ylabel str, Optional axes ylabel '$\\\\beta_{j}$' fontsize int, Optional axes title, xlabel, ylabel fontsize 18 ticklabelsize int, Optional axes xticklabels, yticklabels fontsize 15 Returns: Type Description ax (matplotlib.pyplot.Axes) output figure Source code in pydts/fitters.py def plot_all_events_beta ( self , ax : plt . Axes = None , colors : list = COLORS , show : bool = True , title : Union [ str , None ] = None , xlabel : str = 'Value' , ylabel : str = r '$\\beta_ {j} $' , fontsize : int = 18 , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots the $ beta_{j} $ coefficients and standard errors of all the events. Args: ax (matplotlib.pyplot.Axes, Optional): ax to use colors (list, Optional): colors names show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize ticklabelsize (int, Optional): axes xticklabels, yticklabels fontsize Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) title = r '$\\beta_ {j} $' + f ' for all events' if title is None else title ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) se_df = self . get_beta_SE () for idx , col in enumerate ( se_df . columns ): if idx % 2 == 1 : continue y = np . arange (( idx // 2 ) * len ( se_df ), ( 1 + ( idx // 2 )) * len ( se_df )) ax . errorbar ( x = se_df . iloc [:, idx ] . values , y = y , color = colors [ idx % len ( colors )], xerr = se_df . iloc [:, idx + 1 ] . values , label = f ' { col } ' , markersize = 6 , ls = '' , marker = 'o' ) yt = list ( se_df . index ) * ( len ( se_df . columns ) // 2 ) ax . set_yticks ( np . arange ( 0 , len ( yt ))) ax . set_yticklabels ( yt ) ax . set_title ( title , fontsize = fontsize ) ax . set_xlabel ( xlabel , fontsize = fontsize ) ax . set_ylabel ( ylabel , fontsize = fontsize ) ax . grid () plt . gca () . invert_yaxis () ax . legend () if show : plt . show () return ax","title":"plot_all_events_beta()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.plot_event_alpha","text":"This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event. Parameters: Name Type Description Default event Union[str, int] event name required ax matplotlib.pyplot.Axes, Optional ax to use None scatter_kwargs dict, Optional keywords to pass to the scatter function {} show bool, Optional if to use plt.show() True title str, Optional axes title None xlabel str, Optional axes xlabel 't' ylabel str, Optional axes ylabel '$\\\\alpha_{jt}$' fontsize int, Optional axes title, xlabel, ylabel fontsize 18 color str, Optional color name to use None label str, Optional label name None Returns: Type Description ax (matplotlib.pyplot.Axes) output figure Source code in pydts/fitters.py def plot_event_alpha ( self , event : Union [ str , int ], ax : plt . Axes = None , scatter_kwargs : dict = {}, show = True , title = None , xlabel = 't' , ylabel = r '$\\alpha_ {jt} $' , fontsize = 18 , color : str = None , label : str = None , ticklabelsize : int = 15 ) -> plt . Axes : \"\"\" This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event. Args: event (Union[str, int]): event name ax (matplotlib.pyplot.Axes, Optional): ax to use scatter_kwargs (dict, Optional): keywords to pass to the scatter function show (bool, Optional): if to use plt.show() title (str, Optional): axes title xlabel (str, Optional): axes xlabel ylabel (str, Optional): axes ylabel fontsize (int, Optional): axes title, xlabel, ylabel fontsize color (str, Optional): color name to use label (str, Optional): label name Returns: ax (matplotlib.pyplot.Axes): output figure \"\"\" assert event in self . events , f \"Cannot plot event { event } alpha - it was not included during .fit()\" if ax is None : fig , ax = plt . subplots ( 1 , 1 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = ticklabelsize ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = ticklabelsize ) title = r '$\\alpha_ {jt} $' + f ' for event { event } ' if title is None else title label = f ' { event } ' if label is None else label color = 'tab:blue' if color is None else color alpha_df = self . event_models [ event ][ 1 ] ax . scatter ( alpha_df [ self . duration_col ] . values , alpha_df [ 'alpha_jt' ] . values , label = label , color = color , ** scatter_kwargs ) ax . set_title ( title , fontsize = fontsize ) ax . set_xlabel ( xlabel , fontsize = fontsize ) ax . set_ylabel ( ylabel , fontsize = fontsize ) if show : plt . show () return ax","title":"plot_event_alpha()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_hazard_jt","text":"This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Parameters: Name Type Description Default df pd.DataFrame samples to predict for required event Union[str, int] event name required t Union[Iterable, int] times to calculate the hazard for required Returns: Type Description df (pd.DataFrame) samples with the prediction columns Source code in pydts/fitters.py def predict_hazard_jt ( self , df : pd . DataFrame , event : Union [ str , int ], t : Union [ Iterable , int ]) -> pd . DataFrame : \"\"\" This method calculates the hazard for the given event at the given time values if they were included in the training set of the event. Args: df (pd.DataFrame): samples to predict for event (Union[str, int]): event name t (Union[Iterable, int]): times to calculate the hazard for Returns: df (pd.DataFrame): samples with the prediction columns \"\"\" self . _validate_covariates_in_df ( df . head ()) t = self . _validate_t ( t , return_iter = True ) assert event in self . events , \\ f \"Cannot predict for event { event } - it was not included during .fit()\" model = self . event_models [ event ] alpha_df = model [ 1 ] . set_index ( self . duration_col )[ 'alpha_jt' ] . copy () _t = np . array ([ t_i for t_i in t if ( f 'hazard_j { event } _t { t_i } ' not in df . columns )]) if len ( _t ) == 0 : return df temp_df = df . copy () if isinstance ( self . covariates , list ): beta_j_x = temp_df [ self . covariates ] . dot ( getattr ( model [ 0 ], self . beta_models_params_attr )) elif isinstance ( self . covariates , dict ): beta_j_x = temp_df [ self . covariates [ event ]] . dot ( getattr ( model [ 0 ], self . beta_models_params_attr )) temp_df [[ f 'hazard_j { event } _t { c } ' for c in _t ]] = pd . concat ( [ self . _hazard_inverse_transformation ( alpha_df [ c ] + beta_j_x ) for c in _t ], axis = 1 ) . values return temp_df","title":"predict_hazard_jt()"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.print_summary","text":"This method prints the summary of the fitted models for all the events. Parameters: Name Type Description Default summary_func str, Optional print summary method of the fitted model type (\"summary\", \"print_summary\"). 'print_summary' summary_kwargs dict, Optional Keyword arguments to pass to the model summary function. {} Returns: Type Description None None Source code in pydts/fitters.py def print_summary ( self , summary_func : str = \"print_summary\" , summary_kwargs : dict = {}) -> None : \"\"\" This method prints the summary of the fitted models for all the events. Args: summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\"). summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function. Returns: None \"\"\" from IPython.display import display display ( self . get_beta_SE ()) for event , model in self . event_models . items (): print ( f ' \\n\\n Model summary for event: { event } ' ) display ( model [ 1 ] . set_index ([ self . event_type_col , self . duration_col ]))","title":"print_summary()"},{"location":"api/two_stages_fitter_exact/","text":"Source code in pydts/fitters.py class TwoStagesFitterExact ( TwoStagesFitter ): def __init__ ( self ): super () . __init__ () self . beta_models_params_attr = 'params' def _fit_event_beta ( self , expanded_df , event , model = ConditionalLogit , model_kwargs = {}, model_fit_kwargs = {}): # Model fitting for conditional estimation of Beta_j for specific event if isinstance ( self . covariates , dict ): _covs = self . covariates [ event ] else : _covs = self . covariates beta_j_model = ConditionalLogit ( endog = expanded_df [ f 'j_ { event } ' ], exog = expanded_df [ _covs ], groups = expanded_df [ self . duration_col ], ** model_kwargs ) if ( 'alpha' in model_fit_kwargs . keys ()): # Use 0 <= L1_wt <= 1 parameter to switch between L2 (L1_wt = 0) and L1 (L1_wt = 1) or elastic net. # alpha is the the penalty weight. beta_j_model = beta_j_model . fit_regularized ( ** model_fit_kwargs ) else : beta_j_model = beta_j_model . fit ( ** model_fit_kwargs ) return beta_j_model def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : if isinstance ( self . beta_models [ event ], RegularizedResultsWrapper ): _p = self . beta_models [ event ] . params . copy () _p . name = 'coef' full_table = pd . concat ([ full_table , pd . concat ([ _p ], keys = [ event ], axis = 1 )], axis = 1 ) else : summary = self . beta_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [ - len ( self . covariates ):]], axis = 1 ) return full_table get_beta_SE ( self ) \u00a4 This function returns the Beta coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Beta coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : if isinstance ( self . beta_models [ event ], RegularizedResultsWrapper ): _p = self . beta_models [ event ] . params . copy () _p . name = 'coef' full_table = pd . concat ([ full_table , pd . concat ([ _p ], keys = [ event ], axis = 1 )], axis = 1 ) else : summary = self . beta_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [ - len ( self . covariates ):]], axis = 1 ) return full_table","title":"The Two Stages Procedure of Meir and Gorfine (2023) - Exact"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.get_beta_SE","text":"This function returns the Beta coefficients and their Standard Errors for all the events. Returns: Type Description se_df (pandas.DataFrame) Beta coefficients and Standard Errors Dataframe Source code in pydts/fitters.py def get_beta_SE ( self ): \"\"\" This function returns the Beta coefficients and their Standard Errors for all the events. Returns: se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe \"\"\" full_table = pd . DataFrame () for event in self . events : if isinstance ( self . beta_models [ event ], RegularizedResultsWrapper ): _p = self . beta_models [ event ] . params . copy () _p . name = 'coef' full_table = pd . concat ([ full_table , pd . concat ([ _p ], keys = [ event ], axis = 1 )], axis = 1 ) else : summary = self . beta_models [ event ] . summary () summary_df = pd . DataFrame ([ x . split ( ',' ) for x in summary . tables [ 1 ] . as_csv () . split ( ' \\n ' )]) summary_df . columns = summary_df . iloc [ 0 ] summary_df = summary_df . iloc [ 1 :] . set_index ( summary_df . columns [ 0 ]) summary_df . columns = pd . MultiIndex . from_product ([[ event ], summary_df . columns ]) full_table = pd . concat ([ full_table , summary_df . iloc [ - len ( self . covariates ):]], axis = 1 ) return full_table","title":"get_beta_SE()"}]}